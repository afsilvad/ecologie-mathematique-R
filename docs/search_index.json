[
["index.html", "Analyse et modélisation d’agroécosystèmes 1 Introduction 1.1 Définitions 1.2 À qui s’adresse ce manuel? 1.3 Les logiciels libres 1.4 Langage de programmation 1.5 Contenu du manuel 1.6 Objectifs généraux 1.7 Lectures complémentaires 1.8 Besoin d’aide? 1.9 À propos de l’auteur 1.10 Un cours complémentaire à d’autres cours 1.11 Contribuer au manuel", " Analyse et modélisation d’agroécosystèmes Serge-Étienne Parent 2019-03-21 1 Introduction En développant son jeu de la vie (game of life) en 1970, John Horton Connway a présenté un exemple percutant que des règles simples peuvent mener à des résultats inattendus. Le jeu consiste à placer des jetons sur les cases d’un plateau de jeu consistant en une simple grille orthogonale. Le jeu évolue en fonction du nombre de jetons présents parmi les huit cases du voisinage des jetons ou des cases vides. Les jetons ayant 0 ou 1 voisin sont retirés. Les jetons ayant 2 ou 3 voisins restent intacts Les jetons ayant plus de 3 voisins sont retirés Un jeton est posé sur les cases ayant exactement 3 voisins C’est tout. Selon la manière dont les jetons sont placés au départ, il se peut que la grille se vide de ses jetons, ou que les jetons y prennent beaucoup de place. Il arrive aussi que des cycles réguliers se dégagent ou que l’on se retrouve avec des formes régulières. Vous aurez peut-être compris à ce stade pourquoi le jeu est appelé “jeu de la vie”. La première règle est une situation localisée de sous-population, condition dans laquelle la reproduction est difficile. La deuxième règle est une situation localisée stable. La troisième est une situation de surpopulation, où des individus meurent par un environnement rendu inadéquat par insuffisance de ressource ou surplus de toxicité. Enfin, la quatrième indique une situation favorable à la reproduction. Une grille vidée correspond à une extinction et une grille remplie correspond à une explosion de population. Une oscillation est un “climax”, un état stable en écologie. Un léger changement initial dans la disposition initiale des jetons peut mener à des solutions différentes. Le jeu, qui en fait est une application de la technique des automates cellulaires, se complexifie à mesure que le nombre de jetons grandit. Un humain passera des heures à calculer une itération à 50 jetons, commettra probablement quelques erreurs et demandera quelques cafés. Un processeur pourra gérer des centaines d’itérations sur des grilles de centaines de jetons en quelques secondes. En établissant des règles correspondant aux mécanismes de l’objet étudié, il devient possible de modéliser l’évolution des systèmes vivants, comme l’émergence d’espèces invasives. Simulation avec automates cellulaires. Source: Anonyme, publié sur Giphy. 1.1 Définitions Les mathématiques confèrent aux humains une capacité d’abstraction suffisamment complexe pour leur permettre de toucher les étoiles et les atomes, d’assembler la pensée pour mieux apprécier l’histoire et de prédire le futur, de toucher l’infini et de goûter à l’éternité. À partir des maths, on a pu créer des outils de calcul qui permettent de projeter des images de l’univers, bien au-delà de la Voie lactée. Mais appréhender le vivant demeure néanmoins une tâche complexe. Carte des domaines de l’écologie mathématique L’écologie mathématique couvre un large spectre de domaines, mais peut être divisée en deux branches: l’écologie théorique et l’écologie quantitative (Legendre et Legendre, 2012). Alors que l’écologie théorique s’intéresse à l’expression mathématique des mécanismes écologiques, l’écologie quantitative, plus empirique, en étudie principalement les phénomènes. La modélisation écologique vise à prévoir une situation selon des conditions données. Faisant partie à la fois de l’écologie théorique et de l’écologie quantitative, elle superpose souvent des mécanismes de l’écologie théorique et des phénomènes empiriques de l’écologie quantitative. L’écologie numérique comprend la branche descriptive de l’écologie quantitative, c’est-à-dire qu’elle s’intéresse à évaluer des effets à partir de données empiriques. L’exploration des données dans le but d’y découvrir des structures passe souvent par des techniques multivariées comme la classification hiérarchique ou la réduction d’axe (par exemple, l’analyse en composantes principales), qui sont davantage heuristiques (dans notre cas, bioheuristique) que statistiques. Les tests d’hypothèses et l’analyse des probabilités, quant à eux, relèvent de la biostatistique. Le génie écologique, une discipline intimement liée à l’écologie mathématique, est voué à l’analyse, la modélisation, la conception et la construction de systèmes vivants dans le but de résoudre de manière efficace des problèmes liés à l’écologie et à une panoplie de domaines qui lui sont raccordés. L’agriculture est l’un de ces domaines. C’est d’emblée la discipline qui sera prisée dans ce manuel. Néanmoins, les principes qui seront discutés sont transférables à l’écologie générale. 1.2 À qui s’adresse ce manuel? Le cours vise à introduire des étudiants gradués en agronomie, biologie, écologie, sols, génie agroenvironnemental, génie civil et génie écologique à l’analyse et la modélisation dans leur domaine, tant pour les appuyer pour leurs travaux de recherche que pour leur fournir une trousse d’outil émancipatrice pour leur cheminement professionnel. Plus spécifiquement, vous serez accompagné à découvrir différents outils numériques qui vous permettront d’appréhender vos données, d’en faire émerger l’information et de construire des modèles. En ce sens, c’est un cours de pilotage, pas un cours de mécanique. Bien que des connaissances en programmation et en statistiques aideront grandement les étudiant.e.s à appréhender ce document, une littératie informatique n’est pas requise. Dans tous les cas, quiconque voudra tirer profit de ce manuel devra faire preuve d’autonomie. Vous serez guidés vers des ressources et des références, mais je vous suggère vivement de développer votre propre bibliothèque adaptée à vos besoins et à votre manière de comprendre. 1.3 Les logiciels libres Tous les outils numériques qui sont proposés dans ce cours sont des logiciels libres: « Logiciel libre » [free software] désigne des logiciels qui respectent la liberté des utilisateurs. En gros, cela veut dire que les utilisateurs ont la liberté d’exécuter, copier, distribuer, étudier, modifier et améliorer ces logiciels. Ainsi, « logiciel libre » fait référence à la liberté, pas au prix1 (pour comprendre ce concept, vous devez penser à « liberté d’expression », pas à « entrée libre »). - Projet GNU Donc: codes sources ouverts, développement souvent communautaire, gratuité. Plusieurs raisons éthiques, principalement liées au contrôle de l’environnement virtuel par les utilisateurs et les communautés, peuvent justifier l’utilisation de logiciels libres. Plusieurs raisons pratiques justifient aussi cette orientation. Les logiciels libres vous permettent de transporter vos outils avec vous, d’une entreprise à l’autre, au bureau, ou à la maison, et ce, sans vous soucier d’acheter de coûteuses licences. On soulève avec justesse les risques liés aux possibles erreurs dans les codes des logiciels communautaires. Pour les scientifiques, une erreur peut mener à une étude retirée de la littérature et même, potentiellement, des politiques publiques mal avisées. Pour les ingénieurs, les conséquences pourraient être dramatiques. Mais retenez qu’en toute circonstance, comme professionnel.le, vous êtes responsable des outils que vous utilisez: vous devez vous assurer de la bonne qualité d’un logiciel, qu’il soit propriétaire ou communautaire. Alors que la qualité des logiciels propriétaires est généralement suivie par audits, celle des logiciels libres est plutôt soumise à la vigilance communautaire. Chaque approche a ses avantages et inconvénients, mais elles ne sont pas exclusives. Ainsi les logiciels libres peuvent être audités à l’externe par quiconque décide de le faire. Différentes entreprises, souvent concurrentes, participent tant à cette vigilance qu’au développement des logiciels libres: elles en sont même souvent les instigatrices (comme RStudio, Anaconda et Enthought). Par ailleurs, ce manuel est distribué librement (licence MIT). 1.4 Langage de programmation 1.4.1 R Ce cours est basé sur le langage R. En plus d’être libre, R est un langage de programmation dynamique largement utilisé dans le monde universitaire, et dont l’utilisation s’étend de manière soutenue hors des tours d’ivoire. R is also the name of a popular programming language used by a growing number of data analysts inside corporations and academia. It is becoming their lingua franca partly because data mining has entered a golden age, whether being used to set ad prices, find new drugs more quickly or fine-tune financial models. New York Times, janvier 2019 Son développement est supporté par la R Foundation for Statistical Computing, basée à l’Université de Vienne. Également, l’équipe de RStudio contribue largement au développement de modules génériques. R est principalement utilisé pour le calcul statistique, mais les récents développements le rendent un outil de choix pour tout ce qui entoure la science des données, de l’interaction avec les bases de données au déploiement d’outils d’intelligence artificielle en passant par la visualisation. Une fois implémenté avec des modules de calcul scientifique spécialisés en biologie, en écologie et en agronomie (que nous couvrirons au long du cours), R devient un outil de calcul convivial, rapide et fiable pour le calcul écologique. 1.4.2 Pourquoi pas Python? La première mouture de ce cours se fondait sur le langage Python. Tout comme R, Python est un langage de programmation dynamique prisé pour le calcul scientifique. Python est un langage générique apprécié pour sa polyvalence et sa simplicité. Python est utilisé autant pour créer des logiciels ou des sites web que pour le calcul scientifique. Ainsi, Python peut être utilisé en interopérabilité avec une panoplie de logiciels libres, comme QGIS pour la cartographie et FreeCAD pour le dessin technique. Il est particulièrement apprécié en ingénierie pour ses modules de calcul par éléments finis (FeNICS, SfePy) et en bioinformatique pour ses outils liés au séquençage (scikit-bio), mais ses lacunes en analyse statistique, en particulier en statistiques multivariées m’ont amené à favoriser R. Bien que leurs possibilités se superposent largement, ce serait une erreur d’aborder R et Python comme des langages rivaux. Les deux langages s’expriment de manière similaire et s’inspirent mutuellement: apprendre à travailler avec l’un revient à apprendre l’autre. Les spécialistes en calcul scientifique tendent à apprendre à travailler avec plus d’un langage de programmation. Par ailleurs, l’entreprise Ursa labs travaille en ce moment à l’élaboration d’une infrastructure de données permettant de partager des objets R et Python, en vue d’intégrer différents langages de programmation dans un même flux de travail. 1.4.3 Pourquoi pas Matlab? Parce qu’on est en 2019. 1.4.4 Et… SAS? Parce qu’on est à l’université. 1.4.5 Mais pourquoi pas ______ ? D’autres langages, comme Julia, Scala, Javascript et même Ruby sont utilisés en calcul scientifique. Ils sont néanmoins moins garnis et moins documentés que R. Des langages de plus bas niveau, comme Fortran et C++, viennent souvent appuyer les fonctions des autres langages: ces langages sont plus ardus à utiliser au jour le jour, mais leur rapidité de calcul est imbattable. 1.5 Contenu du manuel Le pire angle avec lequel je pourrais aborder le sujet, c’est avec du code et des formules mathématiques. À travers chacun des chapitres, je tenterai de vous amener à résoudre des problèmes de la manière la plus intuitive possible. Nous aborderons l’analyse et la modélisation inférentielle, prédictive et déterministe appliquée aux agroécosystèmes. Chapitre 2 - Introduction au langage de programmation R. Qu’est-ce que R? Comment l’aborder? Quelles sont les fonctionnalités de base et comment tirer profit de tout l’écosystème de programmation? Chapitre 3 - Organisation des données et opérations sur des tableaux. Les tableaux permettent d’enchâsser l’information dans un format prêt-à-porter pour R. Comment les importer, les exporter, les filtrer, et en faire des sommaires? Chapitre 4 - Visualisation. Comment présenter l’information contenue dans un long tableau en un seul coup d’oeil? Chapitre 5 - Biostatistiques. Il est audacieux de ne consacrer qu’un seul chapitre sur ce vaste sujet. Nous irons à l’essentiel… pour vous donner les outils qui permettront d’approfondir le sujet. Chapitre 6 - Biostatistiques bayésiennes. Une très brève introduction pour qui est intéressé à l’analyse bayésienne. Chapitre 7 - Explorer R. La science des données évolue rapidement. Vous gagnerez à vous tenir au courrant de son évolution, et immanquablement vous vous buterez sur des opérations qui vous sembleront insolubles. Ce chapitre vous accompagnera à rester à jour sur le développement de R, à poser de bonnes questions et proposera des modules intéressants en écologie mathématique. Chapitre 8 - Association, partitionnement et ordination. Les écosystèmes diffèrent, mais en quoi sont-ils semblables, et en quoi dffèrent-ils? Ces questions importantes peuvent être abordés par l’écologie numérique, domaine d’étude au sein duquel l’association, le partitionnement et l’ordination sont des outils prédominants. Chapitre 9 - Détection de valeurs aberrantes et imputation. Une donnée aberrante sortira du lot, pour une raison ou pour une autre. Comment les détecter de manière systématique? D’autre part, que faire lorsqu’une donnée est manquante? Peut-on l’imputer? Comment? Chapitre 10 - Les séries temporelles. Les capteurs modernes permettent de générer des données en fonction du temps. Que ce soit des données météorologiques enregistrées quotidiennement ou des données de teneur en eau enregistrées au 5 secondes, les données en fonction du temps forment un signal. Comment analyser ces signaux? Chapitre 11 - Le travail collaboratif, le suivi de version et la science ouverte. Ce chapitre offre une introduction à l’utilisation des outils de calcul collaboratif, ainsi qu’un aperçu du système de suivi de version git et de son utilisation sur GitHub. Chapitre 12 - L’autoapprentissage. Les applications de l’intelligence artificielle ne sont limitées que par votre imagination. Encore faut-il l’utiliser intelligemment. Chapitre 13 - Les données spatiales. Non, nous n’aborderons pas les géostatistiques. Ce chapitre porte plutôt sur l’utilisation de R comme système d’information géographique de base. Nous utiliserons aussi l’autoapprentissage comme outil d’interpolation spatial. Chapitre 14 - La modélisation déterministe. Les modèles sont des maquettes simplifiées. Comment utiliser les équations différentielles ordinaires pour créer ces maquettes? Si les chapitres 3 à 5 peuvent être considérés comme fondamentaux pour bien maîtriser R, les autres peuvent être feuilletés à la pièce, bien qu’ils forment une suite logique. Chaque chapitre de ce manuel est rédigé en format R notebook, dans un environnement RStudio. Pour exécuter les commandes, les vous pourrez soit copier-coller les commandes dans R (ou RStudio), soit télécharger les fichiers-sources et exécuter les blocs de code. 1.6 Objectifs généraux À la fin du cours, l’étudiant.e sera en mesure: de programmer en langage R d’importer, de manipuler (sélection des colonnes, filtres, sommaires statistiques) et d’exporter des tableaux de générer des graphiques d’utilisation commune d’appréhender des données écologiques et agronomiques à l’aide de tests statistiques fréquentiels d’explorer par lui.elle-même les possibilités offertes par la communauté de développement de modules R d’explorer les données à l’aide des outils de l’écologie numérique (association, partitionnement et ordination) d’imputer des données manquantes dans un tableau et de détecter des valeurs aberrantes d’effectuer une analyse de série temporelle de s’assurer que ses calculs soit auditables et reproductibles dans une perspective de science ouverte de créer un modèle d’autoapprentissage d’intrapoler des données spatiales de modéliser des équations différentielles ordinaires 1.7 Lectures complémentaires 1.7.1 Écologie mathématique How to be a quantitative ecologist. Jason Mathipoulos vous prend par la main pour découvrir les notions de mathématiques fondamentales en écologie, appliquées avec le langage R. Numerical ecology. L’ouvrage hautement détaillé des frères Legendre est non seulement fondamental, mais aussi fondateur d’une science qui évolue encore aujourd’hui: l’analyse des données écologiques. A practical guide to ecological modelling. Soetaert et Herman portent une attention particulière à la présentation des principes de modélisation dans un langage accessible - ce qui est rarement le cas dans le domaine de la modélisation. Les modèles présentés concernent principalement les bilans de masse, en termes de systèmes de réactions chimiques et de relations biologiques. Modélisation mathématique en écologie. Rare livre en modélisation écologique publié en français, la première partie s’attarde aux concepts mathématiques, alors que la deuxième planche à les appliquer. Si le haut niveau d’abstraction de la première partie vous rebute, n’hésitez pas débuter par la seconde partie et de vous référer à la première au besoin. A new ecology: systems perspective. Principalement grâce au soleil, la Terre forme un ensemble de gradients d’énergie qui se déclinent en des systèmes d’une étonnante complexité. C’est ainsi que le regretté Sven Erik Jørgensen (1934-2016) et ses collaborateurs décrivent les écosystèmes dans cet ouvrage qui fait suite aux travaux fondateurs de Howard Thomas Odum. Sven Erik Jørgensen Ecological engineering. Principle and Practice. Ecological processes handbook. Modeling complex ecological dynamics 1.7.2 Programmation R for data science. L’analyse de données est une branche importante de l’écologie mathématique. Ce manuel traite des matrices et la manipulation de données chapitre 3), de la visualisation (chapitre 4) ainsi que de l’apprentissage automatique (chapitre 11). R for data science repasse ces sujets plus en profondeur. En particulier, l’ouvrage de Garrett Grolemund et Hadley Wickham offre une introduction au module graphique ggplot2. Numerical ecology with R. Daniel Borcard enseigne l’écologie numérique à l’Université de Montréal. Son cours est condensé dans ce livre recettes voué à l’application des principes lourdement décrits dans Numerical ecology. 1.7.3 Divers The truthful art. Dans cet ouvrage, Alberto Cairo s’intéresse à l’utilisation des données et de leurs présentations pour fournir une information adéquate à différents publics. 1.8 Besoin d’aide? Les ouvrages de référence reconnus vous offrent des bases solides sur lesquelles vous pouvez vous appuyer dans vos travaux. Mais au-delà des principes, au jour le jour, vous vous buterez immanquablement à toutes sortes de petits problèmes. Quel module utiliser pour cette tâche précise? Que veut dire ce message d’erreur? Comment interpréter ce résultat? Pour tous les petits accrocs du quotidien en calcul scientifique, internet offre de nombreuses ressources qui sont très hétérogènes en qualité. Vous apprendrez à reconnaître les ressources fiables à celles qui sont douteuses. Les plateformes basées sur Stack Exchange, comme Stack Overflow et Cross Validated, m’ont souvent été d’une aide précieuse. Vous aurez avantage à vous construire une petite banque d’information (Turtl, Notion, Evernote, Google Keep, One Note, etc.) en collectant des liens, en prenant en notes certaines recettes et en suivant des sites d’intérêt avec des flux RSS. 1.9 À propos de l’auteur Je m’appelle Serge-Étienne Parent. Je suis ingénieur écologue et professeur adjoint au Département des sols et de génie agroalimentaire de l’Université Laval, Québec, Canada. Je crois que la science est le meilleur moyen d’appréhender le monde pour prendre des décisions avisées. 1.10 Un cours complémentaire à d’autres cours Ce cours a été développé pour ouvrir des perspectives mathématiques en écologie et en agronomie à la FSAA de l’Université Laval. Il est complémentaire à certains cours offerts dans d’autres institutions académiques au Québec, dont ceux-ci. BIO2041. Biostatistiques 1, Université de Montréal BIO2042. Biostatistiques 2, Université de Montréal BIO109. Introduction à la programmation scientifique, Université de Sherbrooke BIO500. Méthodes en écologie computationnelle, Université de Sherbrooke. 1.11 Contribuer au manuel Je suis ouvert aux commentaires et suggestions. Pour contribuer directement, dirigez-vous sur le dépôt du manuel sur GitHub, puis ouvrez une Issue pour en discuter. Créez une nouvelle branche (fork), effectuez les modifications, puis lancer une requête de fusion (pull resquest). "],
["chapitre-intro-a-R.html", "2 La science des données avec R 2.1 Statistiques ou Science des données? 2.2 Organiser son environnement de travail en R 2.3 Préparer son flux de travail 2.4 Premiers pas avec R 2.5 Enfin…", " 2 La science des données avec R ️ Objectifs spécifiques: À la fin de ce chapitre, vous saurez contextualiser la science des données par rapport aux statistiques, serez en mesure de vous lancer dans un environnement de programmation R, serez en mesure d’effectuer des opérations de base en R, saurez différencier les grands types d’objets de R et saurez installer et charger des modules complémentaire. Un projet en science des données comprend trois grandes étapes. D’abord, vous devez collecter des données et vous les compilez adéquatement. Cela peut consister à télécharger des données existantes, exécuter un dispositif expérimental ou effectuer une recensement (étude observationnelle). Compiler les données dans un format qui puisse être importé est une tâche souvent longue et fastidieuse. Puis, vous investiguez les données collectées, c’est-à-dire vous les visualisez, vous appliquez des modèles et testez des hypothèses. Enfin, la communication des résultats consiste à présenter les connaissances qui émergent de votre analyse sous forme visuelle et narrative, avec un langage adapté à la personne qui vous écoute, qu’elle soit experte ou novice, réviseure de revue savante ou administratrice. Grolemund et Wickham (2018) propose la structure d’analyse suivante, avec de légères modifications de ma part. Le grand cadre spécifie Programmer. Oui, vous aurez besoin d’écrire du code. Mais comme je l’ai indiqué dans le premier chapitre, ceci n’est pas un cours de programmation et je préférerai les approches intuitives. 2.1 Statistiques ou Science des données? Selon Whitlock et Schluter (2015), la statistique est l’étude des méthodes pour décrire et mesurer des aspects de la nature à partir d’échantillon. Pour Grolemund et Wickham (2018), la science des données est une discipline excitante permettant de transformer des données brutes en compréhension, perspectives et connaissances. Oui, excitante! La différence entre les deux champs d’expertise est subtile, et certaines personnes n’y voient qu’une différence de ton. Data Science is statistics on a Mac. — Big Data Borat (@BigDataBorat) 27 août 2013 Confinées à ses applications traditionnelles, les statistiques sont davantage vouées à la définition de dispositifs expérimentaux et à l’exécution de tests d’hypothèses, alors que la science des données est moins linéaire, en particulier dans sa phase d’analyse, où de nouvelles questions (donc de nouvelles hypothèses) peuvent être posées au fur et à mesure de l’analyse. Cela arrive généralement davantage lorsque l’on fait face à de nombreuses observations sur lesquelles ne nombreux paramètres sont mesurés. La quantité de données et de mesures auxquelles nous avons aujourd’hui accès grâce aux technologies de mesure et de stockage relativement peu dispendieux rend la science des données une discipline particulièrement attrayante, pour ne pas dire sexy. 2.2 Organiser son environnement de travail en R R est un langage de programmation dérivé du langage S, qui fut initialement lancé en 1976. R figure parmi les langages de programmation les plus utilisés au monde. Bien qu’il soit basé sur les langages statiques C et Fortran, R est un langage dynamique, c’est-à-dire que le code peut être exécuté ligne par ligne ou bloc par bloc: un avantage majeur pour des activités qui nécessitent des interactions fréquentes. Bien que R soit surtout utilisé pour le calcul statistique, il s’impose de plus en plus comme outil privilégié en sciences des données en raison des récents développements de modules d’analyse, de modélisation et de visualisation, dont plusieurs seront utilisés dans ce manuel. 2.3 Préparer son flux de travail Il existe de nombreuses manières d’utiliser R. Parmi celles-ci, j’en couvrirai 2: Installation classique Installation avec Anaconda Pour l’instant, j’écarte l’option infonuagique, qui n’est pas tout à fait au point. Les services de Azure Notebooks et de CoCalc peuvent néanmoins s’avérer utiles… lorsqu’ils fonctionnent convenablement. 2.3.1 Installation classique Sur Windows ou Mac, dirigez-vous ici, téléchargez et installez. Sur Linux, ouvrez votre gestionnaire d’application, chercher r-base (Ubuntu, Debian), R-base (openSuse) ou R-core (Fedora) et installez-le (assurez-vous que les librairies suivantes sont aussi installées: gcc, gcc-fortran, gcc-c++ et make), vous aurez peut-être besoin d’installer des librairies supplémentaires pour faire fonctionner certains modules. Note. Les modules présentés dans ce cours devraient être disponibles sur Linux, Windows et Mac. Ce n’est pas le cas pour tous les modules R. La plupart fonctionnent néanmoins sur Linux, dont les systèmes d’opération (je recommande Ubuntu ou l’une de ses dérivées) sont de bonnes options pour le calcul scientifique. À cette étape, R devrait fonctionner dans un interpréteur de commande . Si vous lancez R dans un terminal (chercher cmd dans le menu si vous êtes sur Windows), vous obtiendrez quelque chose comme ceci. Le symbole &gt; indique que R attend que vos instructions. Vous voilà dans un état méditatif devant l’indéchiffrable vide du terminal. Afin de travailler dans un environnement de travail plus convivial, je recommande l’installation de l’interface RStudio, gratuite et open source: téléchargez l’installateur et suivez les instructions. RStudio ressemble à ceci. En haut à droite se trouve un menu Project (None). Il s’agit d’un menu de vos projets. Je recommande d’utiliser ces projets avec RStudio, qui vous permettront de mieux gérer vos environnements de travail, en particulier en lien avec les chemins vers de vos données, graphiques, etc., que vous pouvez gérer relativement à l’emplacement de votre dossier de projet plutôt qu’à l’emplacement des fichiers sur votre machine. En haut à gauche, vous avez vos feuilles de calcul, qui apparaîtront en tant qu’onglets. Je recommande de prendre en main les R notebooks, dans lesquels vous pouvez écrire du texte en format Markdown (dont il sera question plus loin) entre des blocs de code. Ceci vous permet de détailler votre flux de travail. En bas à gauche apparaît la Console, où vous voyez les commandes envoyées à R ainsi que ses sorties. Si vous travaillez en format notebook, vous n’en aurez probablement pas besoin. En haut à droite, les différents onglets indiquent où vous en êtes dans vos calculs. En particulier, la liste sous Environment indique les objets qui ont été générés jusqu’alors. En bas à droite, on retrouve des onglets de nature variés. Files contient les sous-dossiers et fichiers du dossier de projets. Plots est l’endroit où apparaîtront vos graphiques. Packages contient la liste des modules déjà installés, ainsi qu’un outil de gestion des modules pour leur installation, leur désinstallation et leur mise à jour. Help affiche les fiches d’aide des fonctions (pour obtenir de l’aide sur une fonction dans RStudio, surlignez la fonction dans votre feuille de calcul, puis appuyez sur F1). Enfin, l’onglet Viewer affichera les sorties HTML, en particulier les graphiques interactifs que vous générerez par exemple avec le module plotly. 2.3.2 R notebooks Les R notebooks offrent une approche de programmation littéraire, c’est-à-dire que vous écrivez votre code comme vous écrivez un article, une thèse ou une histoire. Cette approche permet de partager plus facilement vos codes, que ce soit avec une équipe de travail ou à la communauté scientifique pour accompagner un article scientifique en tant que matériel supplémentaire. Lorsque vous créez un notebook (File &gt; New file &gt; R notebook), les instructions de base apparaissent. Ajoutons que pour lancer du code ligne par ligne, vous pouvez surligner le code en question ou placez le curseur sur la ligne à exécuter, puis taper Ctrl + Enter. La sortie de R apparaîtra sous le bloc de code. Dans votre texte, vous pouvez ajouter des équations mathématiques en format Mathjax inspiré du format Latex, par exemple $a = \\sum_{i=1}^n x_i^2$ sera affiché comme \\(a = \\sum_{i=1}^n x_i^2\\) (pour aider dans l’édition d’équation, vous pouvez utiliser un éditeur dans les nuages). Pour les titres, les caractères gras, l’insertion d’image, les hyperliens, les tableaux, etc., référez-vous à la documentation de Markdown. Si votre environnement de travail était un avion, R serait le moteur et RStudio serait le cockpit! 2.3.3 Installation avec Anaconda Si vous cherchez une trousse complète d’analyse de données, comprenant R et Python, vous pourrez préférer Anaconda. Une fois installée, vous pourrez isoler un environnement de travail sur R, ou même isoler des environnements de travail particuliers pour vos projets. Une manière conviviale de créer des environnements de travail est de passer par l’interface Anaconda navigator, que vous lancerez soit dans le menu Windows, soit en ligne de commande anaconda-navigator sous Mac et Linux, puis d’installer r-essentials, rstudio et jupyterlab dans l’onglet Environment. Vous pourrez aussi installer RStudio et Jupyter lab via l’onglet Home de Anaconda navigator. Dans l’environnement de base, installez le package nb_conda_kernels pour vous assurer que tous les noyaux (R, Python, etc.) installés dans les environnements de travail soient automatiquement accessibles dans Jupyter. Jupyter lab est une interface notebook semblable à R notebook. À vrai dire, l’utilisation de R en Anaconda n’est pas tout à fait au point, et pourrait poser problème pour l’installation de certains modules. Si vous optez pour cette option, préparez-vous à avoir à bidouiller un peu. 2.4 Premiers pas avec R R ne fonctionne pas avec des menus, en faisant danser une souris sous une musique de clics. Vous devrez donc entrer des commandes avec votre clavier, que vous apprendrez par cœur au fur et à mesure, ou que vous retrouverez en lançant des recherches sur internet. Par expérience personnelle, lorsque je travaille avec R, j’ai toujours un navigateur ouvert prêt à recevoir une question. Les étapes qui suivent sont des premiers pas. Elles ne feront pas de vous des ceintures noires delà programmation. La plupart des utilisateurs de R ont appris R en se pratiquant sur leurs données, en frappant des murs, en apprenant comment les escalader ou les contourner… Pour l’instant, ouvrez seulement un interpréteur de commande, et lancez R. Voyons si R est aussi libre qu’on le prétend. “La liberté, c’est la liberté de dire que deux et deux font quatre. Si cela est accordé, tout le reste suit.” - George Orwell, 1984 2 + 2 ## [1] 4 Et voilà. Les opérations mathématiques sont effectuées telles que l’on devrait s’attendre. 67.1 - 43.3 ## [1] 23.8 2 * 4 ## [1] 8 1 / 2 ## [1] 0.5 L’exposant peut être noté ^, comme c’est le cas dans Excel, ou ** comme c’est le cas en Python. 2^4 ## [1] 16 2**4 ## [1] 16 1 / 2 # utilisez des espaces de part et d&#39;autre des opérateurs (sauf pour l&#39;exposant) pour éclaircir le code ## [1] 0.5 R ne lit pas ce qui suit le caractère #. Cela vous laisse l’opportunité de commenter un code comprenant une séquence de plusieurs lignes. Remarquez également que la dernière opération comporte des espaces entre les nombres et l’opérateur /. Dans ce cas (ce n’est pas toujours le cas), les espaces ne signifient rien: ils aident seulement à éclaircir le code. Il existe des guides pour l’écriture de code en R. Je recommande le guide de style de Hadley Wickahm. Assigner des objets à des variables est fondamental en programmation. En R, on assigne traditionnellement avec la flèche &lt;-, mais vous verrez parfois le =, qui est davantage utilisé comme standard dans d’autres langages de programmation. Par exemple. a &lt;- 3 Techniquement, a pointe vers le nombre entier 3. Conséquemment, on peut effectuer des opérations sur a. a * 6 ## [1] 18 #A + 2 Le message d’erreur nous dit que A n’est pas défini. Sa version minuscule, a, l’est pourtant. La raison est que R considère la case dans la définition des objets. Utiliser la mauvaise case mène donc à des erreurs. Note. Les messages d’erreur ne sont pas toujours clairs, mais vous apprendrez à les comprendre. Dans tous les cas, ils sont fait pour vous aider. Lisez-les attentivement! En général, le nom d’une variable doit toujours commencer par une lettre, et ne doit pas contenir de caractères réservés (espaces, +, *). Dans la définition des variables, plusieurs utilisent des symboles . pour délimiter les mots, mais la barre de soulignement _ est à préférer. En effet, dans d’autres langages de programmation comme Python, le . a une autre signification: son utilisation est à éviter autant que possible. Note. À ce stade, vous serez probablement plus à l’aise de copier-coller ces commandes dans votre terminal. rendement_arbre &lt;- 50 # pomme/arbre nombre_arbre &lt;- 300 # arbre nombre_pomme &lt;- rendement_arbre * nombre_arbre nombre_pomme ## [1] 15000 Comme chez la plupart des langages de programmation, R respecte les conventions des priorités des opérations mathéatiques. 10 - 9^0.5 * 2 ## [1] 4 2.4.1 Types de données Jusqu’à maintenant, nous n’avons utilisé que des nombres entiers (integer ou int) et des nombres réels (numeric ou float64). R inclut d’autres types. La chaîne de caractère (string ou character) contient un ou plusieurs symboles. Elle est définie entre des doubles guillemets &quot; &quot; ou des apostrophes ' '. Il n’existe pas de standard sur l’utilisation de l’un ou de l’autre, mais en règle générale, on utilise les apostrophes pour les expressions courtes, contenant un simple mot ou séquence de lettres, et les guillemets pour les phrases. Une raison pour cela: les guillemets sont utiles pour insérer des apostrophes dans une chaîne de caractère. a &lt;- &quot;L&#39;ours&quot; b &lt;- &quot;polaire&quot; paste(a, b) ## [1] &quot;L&#39;ours polaire&quot; On colle a et b avec la fonction paste. Notez que l’objet a a été défini précédemment. Il est possible en R de réassigner une variable, mais cela peut porter à confusion, jusqu’à générer des erreurs de calcul si une variable n’est pas assignée à l’objet auquel on voulait référer. Combien de caractères contient la chaîne &quot;L'ours polaire&quot;? R sait compter. Demandons-lui. c &lt;- paste(a, b) nchar(c) ## [1] 14 Quatorze, c’est bien cela (comptez “L’ours polaire”, en incluant l’espace). Comme paste, nchar est une fonction incluse par défaut dans l’environnement de travail de R: plus précisément, ces fonctions sont incluses dans le module base, inclut par défaut lorsque R est lancé. La fonction est appelée en écrivant nchar(). Mais une fonction de quoi? Des arguments, qui se trouvent entre les parenthèses. Dans ce cas, il y a un seul argument: c. En calcul scientifique, il est courant de lancer des requêtes sur si un résultat est vrai ou faux. a &lt;- 17 a &lt; 10 ## [1] FALSE a &gt; 10 ## [1] TRUE a == 10 ## [1] FALSE a != 10 ## [1] TRUE a == 17 ## [1] TRUE !(a == 17) ## [1] FALSE Je viens d’introduire un nouveau type de donnée: les données booléennes (boolean, ou logical), qui ne peuvent prendre que deux états - TRUE ou FALSE. En même temps, j’ai utilisé la fonction print parce que dans mon carnet, seule la dernière opération permet d’afficher le résultat. Si l’on veut forcer une sortie, on utilise print. Puis, on a vu plus haut que le symbole = est réservé pour assigner des objets: pour les tests d’égalité, on utilise le double égal, ==, ou != pour la non-égalité. Enfin, pour inverser une donnée de type booléenne, on utilise le point d’exclamation !. 2.4.2 Les collections de données Les exercices précédents ont permis de présenter les types de données offerts par défaut sur R qui sont les plus importants pour le calcul scientifique: int (integer, ou nombre entier), numeric (nombre réel), character (string, ou chaîne de caractère) et logical (booléen). D’autres s’ajouteront tout au long du cours, comme les catégories (factor) et les unités de temps (date-heure). Lorsque l’on procède à des opérations de calcul en science, nous utilisons rarement des valeurs uniques. Nous préférons les organiser et les traiter en collections. Par défaut, R offre quatre types importants de collections: les vecteurs, les matrices, les listes et les tableaux. 2.4.2.1 Vecteurs D’abord, les vecteurs sont une série de variables de même type. Un vecteur est délimité par la fonction c( ) (c pour concaténation). Les éléments de la liste sont séparés par des virgules. espece &lt;- c(&#39;Petromyzon marinus&#39;, &#39;Lepisosteus osseus&#39;, &#39;Amia calva&#39;, &#39;Hiodon tergisus&#39;) espece ## [1] &quot;Petromyzon marinus&quot; &quot;Lepisosteus osseus&quot; &quot;Amia calva&quot; ## [4] &quot;Hiodon tergisus&quot; Pour accéder aux éléments d’une liste, appelle la liste suivie de la position de l’objet désiré entre crochets. espece[1] ## [1] &quot;Petromyzon marinus&quot; espece[2] ## [1] &quot;Lepisosteus osseus&quot; espece[1:3] ## [1] &quot;Petromyzon marinus&quot; &quot;Lepisosteus osseus&quot; &quot;Amia calva&quot; espece[c(1, 3)] ## [1] &quot;Petromyzon marinus&quot; &quot;Amia calva&quot; On peut noter que le premier élément de la liste est noté 1, et non 0 comme c’est le cas de la plupart de langages. Le raccourcis 1:3 crée une liste de nombres entiers de 1 à 3 inclusivement, c’est-à-dire l’équivalent de c(1, 2, 3). En effet, on crée une liste d’indices pour soutirer des éléments d’une liste. On peut utiliser le symbole de soustraction pour retirer un ou plusieurs éléments d’un vecteur. espece[-c(1, 3)] ## [1] &quot;Lepisosteus osseus&quot; &quot;Hiodon tergisus&quot; Pour ajouter un élément à notre liste, on peut utiliser la fonction c( ). espece &lt;- c(espece, &quot;Cyprinus carpio&quot;) espece ## [1] &quot;Petromyzon marinus&quot; &quot;Lepisosteus osseus&quot; &quot;Amia calva&quot; ## [4] &quot;Hiodon tergisus&quot; &quot;Cyprinus carpio&quot; Notez que l’on efface l’objet espece par une concaténation de l’objet espece, précédemment définie, et d’un autre élément. En lançant espece[3] &lt;- &quot;Lepomis gibbosus&quot;, il est possible de changer un élément de la liste. espece[3] &lt;- &quot;Lepomis gibbosus&quot; espece ## [1] &quot;Petromyzon marinus&quot; &quot;Lepisosteus osseus&quot; &quot;Lepomis gibbosus&quot; ## [4] &quot;Hiodon tergisus&quot; &quot;Cyprinus carpio&quot; 2.4.2.2 Matrices Une matrice est un vecteur de dimension plus élevée que 1. En écologie, on dépasse rarement la deuxième dimension, quoi que les matrices en N dimensions soient courantes en modélisation mathématique. Je ne considérerai pour le moment que des matrices 2D. Comme c’est la cas des vecteurs, les matrices contiennent des valeurs de même type. En R, on peut attribuer aux matrices 2D des noms de ligne et de colonne. mat &lt;- matrix(c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12), ncol=3) mat ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 colnames(mat) &lt;- c(&#39;A&#39;, &#39;B&#39;, &#39;C&#39;) rownames(mat) &lt;- c(&#39;site_1&#39;, &#39;site_2&#39;, &#39;site_3&#39;, &#39;site_4&#39;) mat ## A B C ## site_1 1 5 9 ## site_2 2 6 10 ## site_3 3 7 11 ## site_4 4 8 12 On peut soutirer les noms de colonne et les noms de ligne. Le résultat est un vecteur. colnames(mat) ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; rownames(mat) ## [1] &quot;site_1&quot; &quot;site_2&quot; &quot;site_3&quot; &quot;site_4&quot; 2.4.2.3 Listes Les listes sont des collections hétérogènes dans lesquelles on peut placer les objets désirés, sans distinction: elles peuvent même inclure d’autres listes. Chacun des éléments de la liste peut être identifié par une clé. ma_liste &lt;- list(especes = c(&#39;Petromyzon marinus&#39;, &#39;Lepisosteus osseus&#39;, &#39;Amia calva&#39;, &#39;Hiodon tergisus&#39;), site = &#39;A101&#39;, stations_meteos = c(&#39;746583&#39;, &#39;783786&#39;, &#39;856363&#39;)) ma_liste ## $especes ## [1] &quot;Petromyzon marinus&quot; &quot;Lepisosteus osseus&quot; &quot;Amia calva&quot; ## [4] &quot;Hiodon tergisus&quot; ## ## $site ## [1] &quot;A101&quot; ## ## $stations_meteos ## [1] &quot;746583&quot; &quot;783786&quot; &quot;856363&quot; Les éléments de la liste peuvent être soutirés par le nom de la clé ou par l’indice, de cette manière. ma_liste$especes ## [1] &quot;Petromyzon marinus&quot; &quot;Lepisosteus osseus&quot; &quot;Amia calva&quot; ## [4] &quot;Hiodon tergisus&quot; ma_liste[[1]] ## [1] &quot;Petromyzon marinus&quot; &quot;Lepisosteus osseus&quot; &quot;Amia calva&quot; ## [4] &quot;Hiodon tergisus&quot; Exercice. Accéder au deuxième élément du vecteur d’espèces dans la liste ma_liste. 2.4.2.4 Tableaux Enfin, le type de collection de données le plus important est le tableau, ou data.frame. Techniquement, il s’agit d’une liste composée de vecteurs de même longueur. Chaque colonne peut ainsi prendre un type de donnée indépendamment des autres colonnes. tableau &lt;- data.frame(espece = c(&#39;Petromyzon marinus&#39;, &#39;Lepisosteus osseus&#39;, &#39;Amia calva&#39;, &#39;Hiodon tergisus&#39;), poids = c(10, 13, 21, 4), longueur = c(35, 44, 50, 8)) tableau ## espece poids longueur ## 1 Petromyzon marinus 10 35 ## 2 Lepisosteus osseus 13 44 ## 3 Amia calva 21 50 ## 4 Hiodon tergisus 4 8 En programmation classique en R (nous verrons plus loin la méthode tidyverse), les éléments d’un tableau se manipulent comme ceux d’une matrice et les colonnes peuvent être appelés comme les éléments d’une liste. tableau[, 2:3] ## poids longueur ## 1 10 35 ## 2 13 44 ## 3 21 50 ## 4 4 8 tableau$poids ## [1] 10 13 21 4 Vous verrez aussi, quoi que rarement, ce format, qui à la différence du format $ génère un tableau. tableau[&#39;poids&#39;] ## poids ## 1 10 ## 2 13 ## 3 21 ## 4 4 Le tableau est le format de collection à privilégier pour manipuler des données. Récemment, le format de tableau tibble a été créé par l’équipe de RStudio pour offrir un format plus moderne. 2.4.3 Les fonctions Lorsque vous écrivez une commande suivit de parenthèses, comme data.frame(especes = ...), vous demandez à R de passer à l’action en appelant une fonction. De manière très générale, une fonction transforme quelque chose en quelque chose d’autre. Par exemple, la fonction mean() prend une collection de nombre comme entrée, puis en sort vous devinez quoi. mean(tableau$poids) ## [1] 12 Les entrées sont appelés les arguments de la fonction. Leur définition est toujours disponible dans la documentation. Exercice. Familiarisez-vous avec la documentation de R en lançant ?mean. Truc: si vous avez pris de l’avance et que vous travaillez déjà en RStudio, mettez le terme en surbrillance, puis appuyez sur F1. Vous verrez dans la documentation que la fonction mean() demande trois arguments, x, trim et na.rm. Or nous avons seulement placé un vecteur, sans spécifier d’argument! En effet. En l’absence d’une définition des arguments, R supposera que les arguments dans la parenthèse, séparés par une virgule, sont présentés dans le même ordre que celui spécifié dans la définition de la fonction (celle qui est présentée dans le fichier d’aide). Dans le cas qui nous intéresse, mean(tableau$poids) est équivalent à mean(x = tableau$poids). Maintenant, selon la fiche d’aire l’argument na.rm est un valeur logique spécifiant si oui (TRUE) ou non (FALSE) les valeurs manquantes doivent être considérées (une moyenne d’un vecteur comprenant au moins un NA sera de NA). En ne spécifiant rien, R prend la valeur par défaut, telle que spécifiée dans la documentation. Il en va de même que l’argument trim, qui permet d’élaguer des valeurs extrêmes. Dans la fiche d’aide, mean(x, trim = 0, na.rm = FALSE, ...) signifie que par défaut, l’argument x est vide (il doit donc être spécifié), l’argument trim est de 0 et l’argument na.rm est FALSE. mean(c(6, 1, 7, 4, 9, NA, 1)) ## [1] NA mean(c(6, 1, 7, 4, 9, NA, 1), na.rm = TRUE) ## [1] 4.666667 Vous n’êtes pas emprisonné par les fonctions offertes par R. Vous pouvez installer des modules qui complètent les fonctions de base de R: on le verra un peu plus loin dans ce chapitre. Mais pour l’instant, voyons comment vous pouvez créer vos propres fonctions. Disons que vous voulez créer une fonction qui calcule la sortie de \\(x^3-2y+a\\). Pour obtenir la réponse on a besoin des arguments x, y et a. La sortie de la fonction est ici triviale: la réponse de l’équation. L’opération function permet de prendre ça en charge. operation_f &lt;- function(x, y, a = 10) { return(x^3-2*y+a) } Notez que a a une valeur par défaut. La sortie de la fonction est ce qui se trouve entre les parenthèses de return. Vous pouvez maintenant utiliser la fonction operation_nl au besoin. operation_f(x = 2, y = 3, a = 1) ## [1] 3 Une telle fonction est peu utile. Mais l’utilisation de fonctions personnalisées vous permettra d’éviter de répéter la même opération plusieurs fois dans un flux de travail, en évitant de générer trop de code, donc aussi de potentielles erreurs. Personnellement, j’utilise les fonctions surtout pour générer des graphiques personnalisés. Exercice. Afin d’acquérir de l’autonomie, vous devrez être en mesure de trouver le nom des commandes dont vous avez besoin pour effectuer la tâche que vous désirer effectuer. Cela peut causer des frustrations, mais vous vous sentirez toujours plus à l’aise avec R jour après jour. L’exercice ici est de trouver par vous-même la commande qui vous permettra mesurer la longueur d’un vecteur. 2.4.4 Les boucles Les boucles permettent d’effectuer une même suite d’opérations sur plusieurs objets. Pour faire suite à notre exemple, nous désirons obtenir le résultat de l’opération f pour des paramètres que nous enregistrons dans ce tableau. params &lt;- data.frame(x = c(2, 4, 1, 5, 6), y = c(3, 4, 8, 1, 0), a = c(6, 1, 8, 2, 5)) params ## x y a ## 1 2 3 6 ## 2 4 4 1 ## 3 1 8 8 ## 4 5 1 2 ## 5 6 0 5 Nous créons un vecteur vide, puis nous itérons ligne par ligne en remplissant le vecteur. operation_res &lt;- c() for (i in 1:nrow(params)) { operation_res[i] &lt;- operation_f(x = params[i, 1], y = params[i, 2], a = params[i, 3]) } operation_res ## [1] 8 57 -7 125 221 En faisant varier i sur des valeurs du vecteur donné par la séquence de nombre entiers de 1 au nombre de ligne du tableau de paramètres, nous demandons à R d’effectuer la suite d’opération entre les accolades {}. À chaque boucle, i prend une valeur de la séquence. i est utilisé ici comme indice de la ligne à soutirer du tableau params, qui correspond à l’indice dans le vecteur operation_res. Ainsi, chaque résultat est calculé dans l’ordre des lignes du tableau de paramètres et l’on pourra très bien y coller nos résultats: params$resultats &lt;- operation_res params ## x y a resultats ## 1 2 3 6 8 ## 2 4 4 1 57 ## 3 1 8 8 -7 ## 4 5 1 2 125 ## 5 6 0 5 221 Notez que puisque la colonne resultat n’existe pas dans le tableau params, R crée automatiquement une nouvelle colonne. Les boucles for vous permettront par exemple de générer en peu de temps 10, 100, 1000 graphiques (autant que vous voulez), chacun issu de simulations obtenues à partir de conditions initiales différentes, et de les enregistrer dans un répertoire sur votre ordinateur. Un travail qui pourrait prendre des semaines sur Excel peut être effectué en R en quelques secondes. Un second outil est disponible pour les itérations: les boucles while. Elles effectuent une opération tant qu’un critère n’est pas atteint. Elles sont utiles pour les opérations où l’on cherche une convergence. Je les couvre rapidement puisqu’elles sont rarement utilisées dans les flux de travail courants. En voici un petit exemple. x &lt;- 100 while (x &gt; 1.1) { x &lt;- sqrt(x) print(x) } ## [1] 10 ## [1] 3.162278 ## [1] 1.778279 ## [1] 1.333521 ## [1] 1.154782 ## [1] 1.074608 Nous avons initié x à une valeur de 100. Puis, tant que (while) le test x &gt; 1.1 est vrai, attribuer à x la nouvelle valeur calculée en extrayant la racine de la valeur précédente de x. Enfin, indiquer la valeur avec print. 2.4.5 Conditions: if, else if, else Si la condition 1 est remplie, effectuer une suite d’instruction 1. Si la condition 1 n’est pas remplie, et si la condition 2 est remplie, effectuer la suite d’instruction 2. Sinon, effectuer la suite d’instruction 3. Voilà comment on exprime une suite de conditions. Prenons l’exemple simple d’une discrétisation d’une valeur continue. Si \\(x&lt;10\\), il est classé comme faible. Si \\(10 \\leq x &lt;20\\), il est classé comme moyen. Si \\(x \\geq 20\\), il est classé comme élevé. Plaçons cette classification dans une fonction. classification &lt;- function(x, lim1=10, lim2=20) { if (x &lt; lim1) { categorie &lt;- &quot;faible&quot; } else if (x &lt; lim2) { categorie &lt;- &quot;moyen&quot; } else { categorie &lt;- &quot;élevé&quot; } return(categorie) } classification(-10) ## [1] &quot;faible&quot; classification(15.4) ## [1] &quot;moyen&quot; classification(1000) ## [1] &quot;élevé&quot; Une condition est définie avec le if, suivi du test à vrai ou faux entre parenthèses. Si le test retourne un vrai (TRUE), l’instruction entre accolades est exécutée. Si elle est fausse, on passe au suivant. Exercice. Explorer les commandes ifelse et cut et réfléchissez à la manière qu’elles pourraient être utilisées pour effectuer une discrétisation plus efficacement qu’avec les if et les else. 2.4.6 Installer et charger un module La plupart des opérations d’ordre général (comme les racines carrées, les tests statistiques, la gestion de matrices et de tableau, les graphiques, etc.) sont accessibles grâce aux modules de base de R, qui sont installés et chargés par défaut lors du démarrage de R. Des équipes de travail ont néanmoins développé plusieurs modules pour répondre à leurs besoins spécialisés, et les ont laissées disponibles au grand public dans des modules que vous pouvez installer d’un dépôt CRAN (le AppStore de R), d’un dépôt Anaconda (le AppStore de Anaconda, si vous utilisez cette plate-forme), d’un dépôt Github (dépôts décentralisés), etc. RStudio possède un pratique bouton Install qui vous permet d’y inscrire une liste de modules. Le navigateur anaconda offre aussi une interface d’installation. La commande R pour installer un module est install.packages(&quot;ggplot2&quot;), si par exemple vous désirez installer ggplot2, le module graphique par excellence en R. C’est la commande que RStudio lancera tout seul si vous lui demandez d’installer ggplot2. Les modules sont l’équivalent des applications spécialisées que vous installez sur un téléphone mobile. Pour les utiliser, il faut les ouvrir. Généralement, j’ouvre toutes les applications nécessaires à mon flux de travail au tout début de ma feuille de calcul (la prochaine cellule retournera un message d’erreur si les packages ne sont pas installés). library(&quot;tidyverse&quot;) # méta-package qui charge entre autres dplyr et ggplot2 ## ── Attaching packages ────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.1.0 ✔ purrr 0.3.2 ## ✔ tibble 2.1.1 ✔ dplyr 0.8.0.1 ## ✔ tidyr 0.8.3 ✔ stringr 1.4.0 ## ✔ readr 1.3.1 ✔ forcats 0.4.0 ## ── Conflicts ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(&quot;vegan&quot;) ## Loading required package: permute ## Loading required package: lattice ## This is vegan 2.5-4 library(&quot;nlme&quot;) ## ## Attaching package: &#39;nlme&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## collapse Les modules sont installés sur votre ordinateur à un endroit que vous pourrez retrouver avec la commande .libPaths() Exercice. À partir d’ici jusqu’à la fin du cours, nous utiliserons RStudio. Ouvrez-le et familiarisez-vous avec l’interface! Quelques petits trucs: pour lancer une ligne, placez votre curseur sur la ligne, puis appuyez sur Ctrl+Enter pour lancer une partie de code précise, mettez le en surbrillance, puis Ctrl+Enter utilisez toujours le gestionnaire de projets, en haut à droite! installez le module tidyverse lancez data(iris) pour obtenir un tableau d’exercice, puis cliquez sur l’objet dans la fenêtre environnement essayer R notebook 2.5 Enfin… Comme une langue, on n’apprend à s’exprimer en un langage informatique qu’en se mettant à l’épreuve, ce que vous ferez tout au long de ce cours. Pour vous encourager, voici quelques trucs pour apprendre à coder en R. R n’aime pas l’ambiguïté. Une simple virgule mal placée et il ne sait plus quoi faire. Cela peut être frustrant au début, mais cette rigidité est nécessaire pour effectuer du calcul scientifique. Le copier-coller est votre ami. En gardant à l’esprit que vous être responsable de votre code et que vous respectez les droits d’auteur, n’ayez pas peur de copier-coller des lignes de code et de personnaliser par la suite. L’erreur que vous obtenez: d’autres l’ont obtenue avant vous. Le site de question-réponse stackoverflow est une ressource inestimable où des gens ayant posté des questions ont reçu des réponses d’experts (les meilleures réponses et les meilleures questions apparaissent en premier). Apprenez à chercher intelligemment des réponses en formulant précisément vos questions! Étudiez et pratiquez. Les messages d’erreur en R sont courants, même chez les personnes expérimentées. La meilleure manière d’apprendre une langue est de la parler, d’étudier ses susceptibilités, de les tester dans une conversation, etc. "],
["chapitre-tableaux.html", "3 Organisation des données et opérations sur des tableaux 3.1 Les collections de données 3.2 Organiser un tableau de données 3.3 Formats de tableau 3.4 Entreposer ses données 3.5 Manipuler des données en mode tidyverse 3.6 Références", " 3 Organisation des données et opérations sur des tableaux ️ Objectifs spécifiques: À la fin de ce chapitre, vous comprendrez les règles guidant la création et la gestion des tableaux, saurez importer et exporter des données et saurez effectuer des opérations en cascade avec le module tidyverse, dont des filtres sur les lignes, des sélections de colonnes, des sommaires statistiques et des jointures entre tableaux. Les données sont utilisées à chaque étape dans les flux de travail en sciences. Elles alimentent l’analyse et la modélisation. Les résultats qui en découlent sont aussi des données qui peuvent alimenter les travaux subséquents. Une bonne organisation des données facilitera le flux de travail. Dicton. Proportions de temps voué aux calcul scientifique: 80% de nettoyage de données mal organisées, 20% de calcul. Qu’est-ce qu’une donnée? De manière abstraite, il s’agit d’une valeur associée à une variable. Une variable peut être une dimension, une date, une couleur, le résultat d’un test statistique, à laquelle on attribue la valeur quantitative ou qualitative d’un chiffre, d’une chaîne de caractère, d’un symbole conventionné, etc. Par exemple, lorsque vous commandez un café latte végane, au latte est la valeur que vous attribuez à la variable type de café, et végane est la valeur de la variable type de lait. L’exemple est peut être horrible. J’ai besoin d’un café… Ce chapitre traite de l’importation, l’utilisation et l’exportation de données structurées, en R, sous forme de vecteurs, matrices, tableaux et ensemble de tableaux (bases de données). Bien qu’il soit toujours préférable d’organiser les structures qui accueilleront les données d’une expérience avant-même de procéder à la collecte de données, l’analyste doit s’attendre à réorganiser ses données en cours de route. Or, des données bien organisées au départ faciliteront aussi leur réorganisation. Ce chapitre débute avec quelques définitions: les données, les matrices, les tableaux et les bases de données, ainsi que leur signification en R. Puis nous verrons comment organiser un tableau selon quelques règles simples, mais importantes pour éviter les erreurs et les opérations fastidieuses pour reconstruire un tableau mal conçu. Ensuite, nous traiterons des formats de tableau courant, pour enfin passer à l’utilisation de dplyr, le module tidyverse pour effectuer des opérations sur les tableaux. 3.1 Les collections de données Dans le chapitre 2, nous avons survolé différents types d’objets: réels, entiers, chaînes de caractères et booléens. Les données peuvent appartenir à d’autres types: dates, catégories ordinales (ordonnées: faible, moyen, élevé) et nominales (non ordonnées: espèces, cultivars, couleurs, unité pédologique, etc.). Comme mentionné en début de chapitre, une donnée est une valeur associée à une variable. Les données peuvent être organisées en collections. Nous avons aussi vu au chapitre 2 que la manière privilégiée d’organiser des données était sous forme de tableau. De manière générale, un tableau de données est une organisation de données en deux dimensions, comportant des lignes et des colonnes. Il est préférable de respecter la convention selon laquelle les lignes sont des observations et les colonnes sont des variables. Ainsi, un tableau est une collection de vecteurs de même longueur, chaque vecteur représentant une variable. Chaque variable est libre de prendre le type de données approprié. La position d’une donnée dans le vecteur correspond à une observation. Imaginez que vous consignez des données de différents sites (A, B et C), et que chaque site possède ses propres caractéristiques. Il est redondant de décrire le site pour chaque observation. Vous préférerez créer deux tableaux: un pour décrire vos observations, et un autre pour décrire les sites. De cette manière, vous créez une collection de tableaux intereliés: une base de données. R peut soutirer des données des bases de données grâce au module DBI, qui n’est pas couvert à ce stade de développement du cours. Dans R, les données structurées en tableaux, ainsi que les opérations sur les tableaux, peuvent être gérés grâce aux modules readr, dplyr et tidyr, tous des modules faisant partie du méta-module tidyverse, devenu incontoutnable. Mais avant de se lancer dans l’utilisation de ces modules, voyons quelques règles à suivre pour bien structurer ses données en format tidy, un jargon du tidyverse qui signifie proprement organisé. 3.2 Organiser un tableau de données Afin de repérer chaque cellule d’un tableau, on attribue à chaque lignes et à chaque colonne colonnes un identifiant unique, que l’on nomme indice pour les lignes et entête pour les colonnes. Règle no 1. Une variable par colonne, une observation par ligne, une valeur par cellule. Les unités expérimentales sont décrits par une ou plusieurs variables par des chiffres ou des lettres. Chaque variable devrait être présente en une seule colonne, et chaque ligne devrait correspondre à une unité expérimentale où ces variables ont été mesurées. La règle parait simple, mais elle est rarement respectée. Prenez par exemple le tableau suivant. Site Traitement A Traitement B Traitement C Sainte-Souris 4.1 8.2 6.8 Sainte-Fourmi 5.8 5.9 NA Saint-Ours 2.9 3.4 4.6 Tableau 1. Rendements obtenus sur les sites expérimentaux selon les traitements. Qu’est-ce qui cloche avec ce tableau? Chaque ligne est une observation, mais contient plusieurs observations d’une même variable, le rendement, qui devient étalé sur plusieurs colonnes. À bien y penser, le type de traitement est une variable et le rendement en est une autre: Site Traitement Rendement Sainte-Souris A 4.1 Sainte-Souris B 8.2 Sainte-Souris C 6.8 Sainte-Fourmi A 5.8 Sainte-Fourmi B 5.9 Sainte-Fourmi C NA Saint-Ours A 2.9 Saint-Ours B 3.4 Saint-Ours C 4.6 Tableau 2. Rendements obtenus sur les sites expérimentaux selon les traitements. Plus précisément, l’expression à bien y penser suggère une réflexion sur la signification des données. Certaines variables peuvent parfois être intégrées dans une même colonne, parfois pas. Par exemple, les concentrations en cuivre, zinc et plomb dans un sol contaminé peuvent être placés dans la même colonne “Concentration” ou déclinées en plusieurs colonnes Cu, Zn et Pb. La première version trouvera son utilité pour des créer des graphiques (chapitre 3), alors que la deuxième favorise le traitement statistique (chapitre 5). Il est possible de passer d’un format à l’autre grâce à la fonction gather() et spread() du module tidyr. Règle no 2. Un tableau par unité observationnelle: ne pas répéter les informations. Reprenons la même expérience. Supposons que vous mesurez la précipitation à l’échelle du site. Site Traitement Rendement Précipitations Sainte-Souris A 4.1 813 Sainte-Souris B 8.2 813 Sainte-Souris C 6.8 813 Sainte-Fourmi A 5.8 642 Sainte-Fourmi B 5.9 642 Sainte-Fourmi C NA 642 Saint-Ours A 2.9 1028 Saint-Ours B 3.4 1028 Saint-Ours C 4.6 1028 Tableau 3. Rendements obtenus sur les sites expérimentaux selon les traitements. Segmenter l’information en deux tableaux serait préférable. Site Précipitations Sainte-Souris 813 Sainte-Fourmi 642 Saint-Ours 1028 Tableau 4. Précipitations sur les sites expérimentaux. Les tableaux 2 et 4, ensemble, forment une base de données (collection organisée de tableaux). Les opérations de fusion entre les tableaux peuvent être effectuées grâce aux fonctions de jointure (left_join(), par exemple) du module tidyr. Règle no 3. Ne pas bousiller les données. Par exemple. Ajouter des commentaires dans des cellules. Si une cellule mérite d’être commentée, il est préférable de placer les commentaires soit dans un fichier décrivant le tableau de données, soit dans une colonne de commentaire juxtaposée à la colonne de la variable à commenter. Par exemple, si vous n’avez pas mesure le pH pour une observation, n’écrivez pas “échantillon contaminé” dans la cellule, mais annoter dans un fichier d’explication que l’échantillon no X a été contaminé. Si les commentaires sont systématique, il peut être pratique de les inscrire dans une colonne commentaire_pH. Inscription non systématiques. Il arrive souvent que des catégories d’une variable ou que des valeurs manquantes soient annotées différemment. Il arrive même que le séparateur décimal soit non systématique, parfois noté par un point, parfois par une virgule. Par exemple, une fois importés dans votre session, les catégories St-Ours et Saint-Ours seront traitées comme deux catégories distinctes. De même, les cellules correspondant à des valeurs manquantes ne devraient pas être inscrite parfois avec une cellule vide, parfois avec un point, parfois avec un tiret ou avec la mention NA. Le plus simple est de laisser systématiquement ces cellules vides. Inclure des notes dans un tableau. La règle “une colonne, une variable” n’est pas respectée si on ajoute des notes un peu n’importe où sous ou à côté du tableau. Ajouter des sommaires. Si vous ajoutez une ligne sous un tableau comprenant la moyenne de chaque colonne, qu’est-ce qui arrivera lorsque vous importerez votre tableau dans votre session de travail? La ligne sera considérée comme une observation supplémentaire. Inclure une hiérarchie dans le entêtes. Afin de consigner des données de texture du sol, comprenant la proportion de sable, de limon et d’argile, vous organisez votre entête en plusieurs lignes. Une ligne pour la catégorie de donnée, Texture, fusionnée sur trois colonnes, puis trois colonnes intitulées Sable, Limon et Argile. Votre tableau est joli, mais il ne pourra pas être importé conformément dans un votre session de calcul: on recherche une entête unique par colonne. Votre tableau de données devrait plutôt porter les entêtes Texture sable, Texture limon et Texture argile. Un conseil: réserver le travail esthétique à la toute fin d’un flux de travail. 3.3 Formats de tableau Plusieurs outils sont à votre disposition pour créer des tableaux. Je vous présente ici les plus communs. 3.3.1 xls ou xlsx Microsoft Excel est un logiciel de type tableur, ou chiffrier électronique. L’ancien format xls a été remplacé par le format xlsx avec l’arrivée de Microsoft Office 2010. Il s’agit d’un format propriétaire, dont l’alternative libre la plus connue est le format ods, popularisé par la suite bureautique LibreOffice. Les formats xls, xlsx ou ods sont davantage utilisés comme outils de calcul que d’entreposage de données. Ils contiennent des formules, des graphiques, du formatage de cellule, etc. Je ne les recommande pas pour stocker des données. 3.3.2 csv Le format csv, pour comma separated values, est un fichier texte, que vous pouvez ouvrir avec n’importe quel éditeur de texte brut (Bloc note, Atom, Notepad++, etc.). Chaque colonne doit être délimitée par un caractère cohérent (conventionnellement une virgule, mais en français un point-virgule ou une tabulation pour éviter la confusion avec le séparateur décimal) et chaque ligne du tableau est un retour de ligne. Il est possible d’ouvrir et d’éditer les fichiers csv dans un éditeur texte, mais il est plus pratique de les ouvrir avec des tableurs (LibreOffice Calc, Microsoft Excel, Google Sheets, etc.). Encodage des fichiers texte. Puisque le format csv est un fichier texte, un souci particulier doit être porté sur la manière dont le texte est encodé. Les caractères accentués pourrait être importer incorrectement si vous importez votre tableau en spécifiant le mauvais encodage. Pour les fichiers en langues occidentales, l’encodage UTF-8 devrait être utilisé. Toutefois, par défaut, Excel utilise un encodage de Microsoft. Si le csv a été généré par Excel, il est préférable de l’ouvrir avec votre éditeur texte et de l’enregistrer dans l’encodage UTF-8. 3.3.3 json Comme le format csv, le format json indique un fichier en texte clair. Il est utilisé davantage pour le partage de données des applications web. En analyse et modélisation, ce format est surtout utilisé pour les données géoréférencées. L’encodage est géré de la même manière qu’un fichier csv. 3.3.4 SQLite SQLite est une application pour les bases de données relationnelles de type SQL qui n’a pas besoin de serveur pour fonctionner. Les bases de données SQLite sont encodés dans des fichiers portant l’extension db, qui peuvent être facilement partagés. 3.3.5 Suggestion En csv pour les petits tableaux, en sqlite pour les bases de données plus complexes. Ce cours se concentre toutefois sur les données de type csv. 3.4 Entreposer ses données La manière la plus sécurisée pour entreposer ses données est de les confiner dans une base de données sécurisée sur un serveur sécurisé dans un environnement sécurisé et d’encrypter les communications. C’est aussi la manière la moins accessible. Des espaces de stockage nuagiques, comme Dropbox ou d’autres options similaires, peuvent être pratiques pour les backups et le partage des données avec une équipe de travail (qui risque en retour de bousiller vos données). Le suivi de version est possible chez certains fournisseurs d’espace de stockage. Mais pour un suivi de version plus rigoureux, les espaces de développement (comme GitHub et GitLab) sont plus appropriés (couverts au chapitre 11). Dans tous les cas, il est important de garder (1) des copies anciennes pour y revenir en cas d’erreurs et (2) un petit fichier décrivant les changements effectués sur les données. 3.5 Manipuler des données en mode tidyverse Le méta-module tidyverse regroupe une collection de précieux modules pour l’analyse de données en R. Il permet d’importer des données dans votre session de travail avec readr, de les explorer avec le module de visualisation ggplot2, de les transformer avec tidyr et dplyr et de les exporter avec readr. Les tableaux de classe data.frame, comme ceux de la plus moderne classe tibble, peuvent être manipulés à travers le flux de travail pour l’analyse et la modélisation (chapitres suivants). Comme c’était le cas pour le chapitre sur la visualisation, ce chapitre est loin de couvrir les nombreuses fonctionnalités qui sont offertes dans le tidyverse. 3.5.1 Importer vos données dans voter session de travail Supposons que vous avec bien organisé vos données en mode tidy. Pour les importer dans votre session et commencer à les inspecter, vous lancerez une des commandes du module readr, décrites dans la documentation dédiée. read_csv() si le séparateur de colonne est une virgule read_csv2() si le séparateur de colonne est un point-virgule et que le séparateur décimal est une virgule read_tsv() si le séparateur de colonne est une tabulation read_table() si le séparateur de colonne est un espace blanc read_delim() si le séparateur de colonne est un autre caractère (comme le point-virgule) que vous spécifierez dans l’argument delim = &quot;;&quot; Les principaux arguments sont les suivants. file: le chemin vers le fichier. Ce chemin peut aussi bien être une adresse locale (data/…) qu’une adresse internet (https://…). delim: le symbole délimitant les colonnes dans le cas de read_delim. col_names: si TRUE, la première ligne est l’entête du tableau, sinon FALSE. Si vous spécifiez un vecteur numérique, ce sont les numéros des lignes utilisées pour le nom de l’entête. Si vous utilisez un vecteur de caractères, ce sont les noms des colonnes que vous désirez donner à votre tableau. na: le symbole spécifiant une valeur manquante. L’argument na='' signifie que les cellules vides sont des données manquantes. Si les valeurs manquantes ne sont pas uniformes, vous pouvez les indiquer dans un vecteur, par exemple na = c(&quot;&quot;, &quot;NA&quot;, &quot;NaN&quot;, &quot;.&quot;, &quot;-&quot;). local: cet argument prend une fonction local() qui peut inclure des arguments de format de temps, mais aussi d’encodage (voir documentation) D’autres arguments peuvent être spécifiés au besoin, et les répéter ici dupliquerait l’information de la documentation de la fonction read_csv de readr. Je déconseille d’importer des données en format xls ou xlsx. Si toutefois cela vous convient, je vous réfère au module readxl. L’aide-mémoire de readr est à afficher près de soi. Aide-mémoire de readr, source: https://www.rstudio.com/resources/cheatsheets/ Nous allons charger des données de culture de la chicouté (Rubus chamaemorus), un petit fruit nordique, tiré de Parent et al. (2013). Ouvrons d’abord le fichier pour vérifier les séparateurs de colonne et de décimale. Le séparateur de colonne est un point-virgule et le décimal est une virgule. Avec Atom, mon éditeur texte préféré, je vais dans Edit &gt; Select Encoding et j’obtiens bien le UTF-8. Nous allons donc utiliser read_csv2() avec ses arguments par défaut. library(&quot;tidyverse&quot;) chicoute &lt;- read_csv2(&#39;data/chicoute.csv&#39;) ## Using &#39;,&#39; as decimal and &#39;.&#39; as grouping mark. Use read_delim() for more control. ## Parsed with column specification: ## cols( ## .default = col_double(), ## CodeTourbiere = col_character(), ## Ordre = col_character(), ## Traitement = col_character(), ## DemiParcelle = col_character(), ## SousTraitement = col_character() ## ) ## See spec(...) for full column specifications. Quelques commandes utiles inspecter le tableau: head() présente l’entête du tableau, soit ses 6 premières lignes str() et glimpse() présentent les variables du tableau et leur type - glimpse()est la fonction tidyverse et str() est la fonction classique (je préfère str()) summary() présente des statistiques de base du tableau names() ou colnames() sort les noms des colonnes sous forme d’un vecteur dim() donne les dimensions du tableau, ncol() son nombre de colonnes et nrow() son nombre de lignes skim est une fonction du module skimr montrant un portrait graphique et numérique du tableau Extra 1. Plusieurs modules ne se trouvent pas dans les dépôt CRAN, mais sont disponibles sur GitHub. Pour les installer, installez d’abord le module devtools disponible sur CRAN. Vous pourrez alors installer les packages de GitHub comme on le fait avec le package skimr. Extra 2. Lorsque je désire utiliser une fonction, mais sans charger le module dans la session, j’utilise la notation module::fonction. Comme dans ce cas, pour skimr. # devtools::install_github(&quot;ropenscilabs/skimr&quot;) skimr::skim(chicoute) ## Skim summary statistics ## n obs: 90 ## n variables: 31 ## ## ── Variable type:character ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## variable missing complete n min max empty n_unique ## CodeTourbiere 0 90 90 1 4 0 12 ## DemiParcelle 50 40 90 4 5 0 2 ## Ordre 0 90 90 1 2 0 20 ## SousTraitement 50 40 90 1 7 0 3 ## Traitement 50 40 90 6 11 0 2 ## ## ── Variable type:numeric ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## variable missing complete n mean sd ## Al_pourc 0 90 90 0.0027 0.0013 ## B_pourc 0 90 90 0.0031 0.00067 ## C_pourc 0 90 90 50.28 1.61 ## Ca_pourc 0 90 90 0.39 0.1 ## Cu_pourc 0 90 90 0.00041 0.00064 ## Fe_pourc 0 90 90 0.015 0.0059 ## FemelleAvorte_nombre_m2 4 86 90 8.49 14.52 ## FemelleFruit_nombre_m2 18 72 90 19.97 23.79 ## ID 0 90 90 45.5 26.12 ## K_pourc 0 90 90 0.89 0.27 ## Latitude_m 0 90 90 5701839.86 1915.5 ## Longitude_m 0 90 90 485295.54 6452.33 ## Mg_pourc 0 90 90 0.5 0.085 ## Mn_pourc 0 90 90 0.033 0.025 ## N_pourc 0 90 90 2.2 0.4 ## P_pourc 0 90 90 0.14 0.037 ## Rendement_g_5m2 50 40 90 13.33 21.56 ## S_pourc 0 90 90 0.13 0.039 ## Site 0 90 90 6.33 5.49 ## SterileFleur_nombre_m2 4 86 90 0.26 0.71 ## TotalFemelle_nombre_m2 4 86 90 27.53 29.83 ## TotalFloral_nombre_m2 4 86 90 52.08 40.41 ## TotalMale_nombre_m2 4 86 90 24.4 26.87 ## TotalRamet_nombre_m2 0 90 90 251.26 156.06 ## TotalVegetatif_nombre_m2 4 86 90 199.02 139.13 ## Zn_pourc 0 90 90 0.0067 0.0021 ## p0 p25 p50 p75 p100 ## 9e-04 0.0019 0.0024 0.0033 0.0093 ## 0.0018 0.0026 0.0032 0.0035 0.0042 ## 46.72 49.14 50.45 51.58 53.83 ## 0.19 0.32 0.37 0.44 0.88 ## 3.7e-05 3.7e-05 0.00021 0.00046 0.0042 ## 0.0091 0.011 0.014 0.017 0.052 ## 0 1.27 3.07 10.14 76.8 ## 0.4 7.64 11.46 22.83 157.88 ## 1 23.25 45.5 67.75 90 ## 0.35 0.69 0.86 1.13 1.54 ## 5695688 5701868.5 5702129 5702537 5706394 ## 459873 485927 486500 486544.75 491955 ## 0.36 0.45 0.48 0.52 0.86 ## 0.0023 0.012 0.028 0.05 0.1 ## 1.53 1.89 2.12 2.58 3.1 ## 0.071 0.12 0.14 0.16 0.23 ## 0 0 0.95 15.63 72.44 ## 0.07 0.11 0.13 0.14 0.28 ## 1 2 4 9 20 ## 0 0 0 0 3.82 ## 2.55 10.34 17.19 31.96 187.17 ## 4.8 22.92 43 69.52 198.62 ## 0 3.3 15.28 36.51 104.41 ## 40.74 122.7 212.92 347.8 651.9 ## 22.92 86.26 161.25 263.78 580.6 ## 0.0033 0.0055 0.0063 0.0072 0.016 ## hist ## ▆▇▅▂▁▁▁▁ ## ▃▂▅▃▃▇▁▅ ## ▂▃▆▃▅▇▂▁ ## ▂▇▇▃▂▁▁▁ ## ▇▁▁▁▁▁▁▁ ## ▇▅▂▁▁▁▁▁ ## ▇▂▁▁▁▁▁▁ ## ▇▂▁▁▁▁▁▁ ## ▇▇▇▇▇▇▇▇ ## ▃▂▇▆▃▆▂▁ ## ▁▁▁▂▇▆▁▁ ## ▁▁▁▁▁▁▇▁ ## ▃▇▆▂▁▁▁▁ ## ▇▃▃▃▂▁▁▁ ## ▃▆▇▆▂▅▃▂ ## ▆▂▇▇▇▃▂▁ ## ▇▁▁▁▁▁▁▁ ## ▂▆▇▂▁▁▁▁ ## ▇▅▁▁▁▁▁▁ ## ▇▁▁▁▁▁▁▁ ## ▇▂▁▁▁▁▁▁ ## ▇▇▃▂▁▁▁▁ ## ▇▃▂▂▁▁▁▁ ## ▇▃▇▂▂▂▂▁ ## ▇▇▆▃▂▂▂▁ ## ▂▇▅▁▁▁▁▁ Exercice. Inspectez le tableau. 3.5.2 Comment sélectionner et filtrer des données? On utiliser le terme sélectionner lorsque l’on désire choisir une ou plusieurs lignes et colonnes d’un tableau (la plupart du temps des colonnes). L’action de filtrer signifie de sélectionner des lignes selon certains critères. 3.5.2.1 Sélectionner Voici trois manières de sélectionner une colonne en R. Une méthode rapide mais peu expressive consiste à indiquer les valeurs numériques de l’indice de la colonne entre des crochets. Il s’agit d’appeler le tableau suivit de crochets. L’intérieur des crochets comprend deux éléments séparés par une virgule. Le premier élément sert à filtrer selon l’indice, le deuxième sert à sélectionner selon l’indice. Ainsi: chicoute[, 1]: sélectionner la première colonne chicoute[, 1:10]: sélectionner les 10 premières colonnes chicoute[, c(2, 4, 5)]: sélectionner les colonnes 2, 4 et 5 chicoute[c(10, 13, 20), c(2, 4, 5)]: sélectionner les colonnes 2, 4 et 5 et les lignes 10, 13 et 20. Une autre méthode rapide, mais plus expressive, consiste à appeler le tableau, suivi du symbole $, puis le nom de la colonne. Truc. La plupart des IDE, comme RStudio, peuvent vous proposer des colonnes dans une liste. Après avoir entrer le $, taper sur la touche de tabulation: vous pourrez sélectionner la colonne dans une liste défilante. Une autre option est d’inscrire le nom de la colonne, ou du vecteur des colonnes, entre des crochets suivant le nom du tableau, c’est-à-dire chicoute[c(&quot;Site&quot;, &quot;Latitude_m&quot;, &quot;Longitude_m&quot;)]. Enfin, dans une séquence d’opérations en mode pipeline (chaque opération est mise à la suite de la précédente en plaçant le pipe %&gt;% entre chacune), il peut être préférable de sélectionner des colonnes avec la fonction select(), i.e. chicoute %&gt;% select(Site, Latitude_m, Longitude_m) La fonction select() permet aussi de travailler en exclusion. Ainsi pour enlever des colonnes, on placera un - (signe de soustraction) devant le nom de la colonne. D’autre arguments de select() permettent une sélection rapide. Par exemple, pour obtenir les colonnes contenant des pourcentages: chicoute %&gt;% select(ends_with(&quot;pourc&quot;)) %&gt;% head(3) ## # A tibble: 3 x 13 ## C_pourc N_pourc P_pourc K_pourc Ca_pourc Mg_pourc S_pourc B_pourc ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 51.5 1.72 0.108 1.21 0.435 0.470 0.0976 0.00258 ## 2 51.3 2.18 0.0985 1.22 0.337 0.439 0.0996 0.00258 ## 3 50.6 2.12 0.0708 1.05 0.373 0.420 0.104 0.00258 ## # … with 5 more variables: Cu_pourc &lt;dbl&gt;, Zn_pourc &lt;dbl&gt;, Mn_pourc &lt;dbl&gt;, ## # Fe_pourc &lt;dbl&gt;, Al_pourc &lt;dbl&gt; 3.5.2.2 Filtrer Comme c’est le cas de la sélection, on pourra filtrer un tableau de plusieurs manières. J’ai déjà présenté comment filtrer selon les indices des lignes. Les autres manières reposent néanmoins sur une opération logique ==, &lt;, &gt; ou %in% (le %in% signifie se trouve parmi et peut être suivi d’un vecteur de valeur que l’on désire accepter). Les conditions booléennes peuvent être combinées avec les opérateurs et, &amp;, et ou, |. Pour rappel, Opération Résultat Vrai et Vrai Vrai Vrai et Faux Faux Faux et Faux Faux Vrai ou Vrai Vrai Vrai ou Faux Vrai Faux ou Faux Faux La méthode classique consiste à appliquer une opération logique entre les crochets, par exemple chicoute[chicoute$CodeTourbiere == &quot;BEAU&quot;, ] La méthode tidyverse, plus pratique en mode pipeline, passe par la fonction filter(), i.e. chicoute %&gt;% filter(CodeTourbiere == &quot;BEAU&quot;) Combiner le tout. chicoute %&gt;% filter(Ca_pourc &lt; 0.4 &amp; CodeTourbiere %in% c(&quot;BEAU&quot;, &quot;MB&quot;, &quot;WTP&quot;)) %&gt;% select(contains(&quot;pourc&quot;)) ## # A tibble: 4 x 13 ## C_pourc N_pourc P_pourc K_pourc Ca_pourc Mg_pourc S_pourc B_pourc ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 51.3 2.18 0.0985 1.22 0.337 0.439 0.0996 0.00258 ## 2 50.6 2.12 0.0708 1.05 0.373 0.420 0.104 0.00258 ## 3 53.8 2.04 0.115 0.947 0.333 0.472 0.106 0.00258 ## 4 52.6 2.11 0.0847 0.913 0.328 0.376 0.111 0.00296 ## # … with 5 more variables: Cu_pourc &lt;dbl&gt;, Zn_pourc &lt;dbl&gt;, Mn_pourc &lt;dbl&gt;, ## # Fe_pourc &lt;dbl&gt;, Al_pourc &lt;dbl&gt; 3.5.3 Le format long et le format large Dans le tableau chicoute, chaque élément possède sa propre colonne. Si l’on voulait mettre en graphique les boxplot des facettes de concentrations d’azote, de phosphore et de potassium dans les différentes tourbières, il faudrait obtenir une seule colonne de concentrations. Pour ce faire, nous utiliserons la fonction gather(). Le premier argument est le nom de la colonne des variables, le deuxième est le nom de la nouvelle colonne des valeurs. La suite consiste à décrire les colonnes à inclure ou à exclure. Dans le cas qui suit, j’exclue CodeTourbiere de la refonte j’utilise sample_n() pour présenter un échantillon du résultat. Notez la ligne comprenant la fonction mutate, que l’on verra plus loin. Cette fonction ajoute une colonne au tableau. Dans ce cas-ci, j’ajoute une colonne constituée d’une séquence de nombres allant de 1 au nombre de lignes du tableau (il y en a 90). Cet identifiant unique pour chanque ligne permettra de reconstituer par la suite le tableau initial. chicoute_long &lt;- chicoute %&gt;% select(CodeTourbiere, N_pourc, P_pourc, K_pourc) %&gt;% mutate(ID = 1:nrow(.)) %&gt;% # mutate ajoute une colonne au tableau gather(key = element, value = concentration, -CodeTourbiere, -ID) chicoute_long %&gt;% sample_n(10) ## # A tibble: 10 x 4 ## CodeTourbiere ID element concentration ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 NTP 51 K_pourc 0.398 ## 2 1 85 N_pourc 1.98 ## 3 2 23 P_pourc 0.202 ## 4 1 73 N_pourc 2.25 ## 5 MR 40 K_pourc 0.656 ## 6 NBM 47 P_pourc 0.116 ## 7 NESP 45 P_pourc 0.142 ## 8 NESP 41 P_pourc 0.148 ## 9 NTP 53 K_pourc 0.345 ## 10 WTP 89 K_pourc 0.465 L’opération inverse est spread(). chicoute_large &lt;- chicoute_long %&gt;% spread(key = &quot;element&quot;, value = &quot;concentration&quot;) %&gt;% select(-ID) # enlever l&#39;identifiant unique chicoute_large %&gt;% sample_n(10) ## # A tibble: 10 x 4 ## CodeTourbiere K_pourc N_pourc P_pourc ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.937 1.95 0.141 ## 2 1 0.833 2.31 0.156 ## 3 MR 0.958 1.90 0.129 ## 4 2 1.18 2.86 0.202 ## 5 SSP 0.734 1.99 0.136 ## 6 BP 0.778 1.77 0.166 ## 7 NBM 0.825 2.42 0.156 ## 8 MR 0.906 1.95 0.113 ## 9 1 1.32 2.15 0.161 ## 10 WTP 0.465 1.80 0.0799 3.5.4 Combiner des tableaux Nous avons introduit plus haut la notion de base de données. Nous voudrions peut-être utiliser le code des tourbières pour inclure leur nom, le type d’essai mené à ces tourbières, etc. Importons d’abord le tableau des noms liés aux codes. tourbieres &lt;- read_csv2(&quot;data/chicoute_tourbieres.csv&quot;) ## Using &#39;,&#39; as decimal and &#39;.&#39; as grouping mark. Use read_delim() for more control. ## Parsed with column specification: ## cols( ## Tourbiere = col_character(), ## CodeTourbiere = col_character(), ## Type = col_character(), ## TypeCulture = col_character() ## ) tourbieres ## # A tibble: 11 x 4 ## Tourbiere CodeTourbiere Type TypeCulture ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Beaulieu BEAU calibration naturel ## 2 Brador Path BP calibration naturel ## 3 Lichen (BS2E) 2 validation cultive sec ## 4 Mannys Brook MB calibration naturel ## 5 Middle Bay Road MR calibration naturel ## 6 North Est of Smelt Pond NESP calibration naturel ## 7 North of Blue Moon NBM calibration naturel ## 8 South of Smelt Pond SSP calibration naturel ## 9 Sphaigne (BS2F) BS2 validation cultive sec ## 10 Sphaigne (BS2F) 1 calibration naturel ## 11 West of Trout Pond WTP calibration naturel Notre information est organisée en deux tableaux, liés par la colonne CodeTourbiere. Comment fusionner l’information pour qu’elle puisse être utilisée dans son ensemble? La fonction left_join effectue cette opération typique avec les bases de données. chicoute_merge &lt;- left_join(x = chicoute, y = tourbieres, by = &quot;CodeTourbiere&quot;) # ou bien chicoute %&gt;% left_join(y = tourbieres, by = &quot;CodeTourbiere&quot;) chicoute_merge %&gt;% sample_n(4) ## # A tibble: 4 x 34 ## ID CodeTourbiere Ordre Site Traitement DemiParcelle SousTraitement ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 8 BP H 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 26 2 9 18 temoin right B ## 3 9 BP H 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 81 1 4 8 temoin right Cu ## # … with 27 more variables: Latitude_m &lt;dbl&gt;, Longitude_m &lt;dbl&gt;, ## # Rendement_g_5m2 &lt;dbl&gt;, TotalRamet_nombre_m2 &lt;dbl&gt;, ## # TotalVegetatif_nombre_m2 &lt;dbl&gt;, TotalFloral_nombre_m2 &lt;dbl&gt;, ## # TotalMale_nombre_m2 &lt;dbl&gt;, TotalFemelle_nombre_m2 &lt;dbl&gt;, ## # FemelleFruit_nombre_m2 &lt;dbl&gt;, FemelleAvorte_nombre_m2 &lt;dbl&gt;, ## # SterileFleur_nombre_m2 &lt;dbl&gt;, C_pourc &lt;dbl&gt;, N_pourc &lt;dbl&gt;, ## # P_pourc &lt;dbl&gt;, K_pourc &lt;dbl&gt;, Ca_pourc &lt;dbl&gt;, Mg_pourc &lt;dbl&gt;, ## # S_pourc &lt;dbl&gt;, B_pourc &lt;dbl&gt;, Cu_pourc &lt;dbl&gt;, Zn_pourc &lt;dbl&gt;, ## # Mn_pourc &lt;dbl&gt;, Fe_pourc &lt;dbl&gt;, Al_pourc &lt;dbl&gt;, Tourbiere &lt;chr&gt;, ## # Type &lt;chr&gt;, TypeCulture &lt;chr&gt; D’autres types de jointures sont possibles, et décrites en détails dans la documentation. Garrick Aden-Buie a préparé de jolies animations pour décrire les différents types de jointures. left_join(x, y) colle y à x seulement ce qui dans y correspond à ce que l’on trouve dans x. right_join(x, y) colle y à x seulement ce qui dans x correspond à ce que l’on trouve dans y. inner_join(x, y) colle x et y en excluant les lignes où au moins une variable de joint est absente dans x et y. full_join(x, y)garde toutes les lignes et les colonnes de x et y. 3.5.5 Opérations sur les tableaux Les tableaux peuvent être segmentés en éléments sur lesquels on calculera ce qui nous chante. On pourrait vouloir obtenir: la somme avec la function sum() la moyenne avec la function mean() ou la médiane avec la fonction median() l’écart-type avec la function sd() les maximum et minimum avec les fonctions min() et max() un décompte d’occurrence avec la fonction n() ou count() Par exemple, mean(chicoute$Rendement_g_5m2, na.rm = TRUE) ## [1] 13.32851 En mode classique, pour effectuer des opérations sur des tableaux, on utilisera la fonction apply(). Cette fonction prend, comme arguments, le tableau, l’axe (opération par ligne = 1, opération par colonne = 2), puis la fonction à appliquer. apply(chicoute %&gt;% select(contains(&quot;pourc&quot;)), 2, mean) ## C_pourc N_pourc P_pourc K_pourc Ca_pourc ## 5.027911e+01 2.199411e+00 1.388959e-01 8.887000e-01 3.884391e-01 ## Mg_pourc S_pourc B_pourc Cu_pourc Zn_pourc ## 4.980142e-01 1.347177e-01 3.090922e-03 4.089891e-04 6.662155e-03 ## Mn_pourc Fe_pourc Al_pourc ## 3.345239e-02 1.514885e-02 2.694979e-03 Les opération peuvent aussi être effectuées par ligne, par exemple une somme (je garde seulement les 10 premiers résultats). apply(chicoute %&gt;% select(contains(&quot;pourc&quot;)), 1, sum)[1:10] ## [1] 55.64299 55.76767 54.78856 55.84453 57.89671 55.53603 55.62526 ## [8] 55.10991 55.06295 55.16774 La fonction à appliquer peut être personnalisée, par exemple: apply(chicoute %&gt;% select(contains(&quot;pourc&quot;)), 2, function(x) (prod(x))^(1/length(x))) ## C_pourc N_pourc P_pourc K_pourc Ca_pourc ## 50.253429104 2.165246915 0.133754530 0.846193827 0.376192724 ## Mg_pourc S_pourc B_pourc Cu_pourc Zn_pourc ## 0.491763884 0.129900753 0.003014675 0.000000000 0.006408775 ## Mn_pourc Fe_pourc Al_pourc ## 0.024140327 0.014351745 0.002450982 Vous reconnaissez cette fonction? C’était la moyenne géométrique (la fonction prod() étant le produit d’un vecteur). En mode tidyverse, on aura besoin principalement des fonction suivantes: group_by() pour effectuer des opérations par groupe, l’opération group_by() sépare le tableau en plusieurs petits tableaux, en attendant de les recombiner. C’est un peu l’équivalent des facettes avec le module de visualisation ggplot2, que nous explorons au chapitre 4. summarise() pour réduire plusieurs valeurs en une seule, il applique un calcul sur le tableau ou s’il y a lieu sur chaque petit tableau segmenté. Il en existe quelques variantes. summarise_all() applique la fonction à toutes les colonnes summarise_at() applique la fonction aux colonnes spécifiées summarise_if() applique la fonction aux colonnes qui ressortent comme TRUE selon une opération booléenne mutate() pour ajouter une nouvelle colonne Si l’on désire ajouter une colonne à un tableau, par exemple le sommaire calculé avec summarise(). À l’inverse, la fonction transmute() retournera seulement le résultat, sans le tableau à partir duquel il a été calculé. De même que summarise(), mutate() et transmute() possèdent leurs équivalents _all(), _at() et _if(). arrange() pour réordonner le tableau On a déjà couvert arrange() dans le chapitre 3. Rappelons que cette fonction n’est pas une opération sur un tableau, mais plutôt un changement d’affichage en changeant l’ordre d’apparition des données. Ces opérations sont décrites dans l’aide-mémoire Data Transformation Cheat Sheet. Aide-mémoire de dplyr, source: https://www.rstudio.com/resources/cheatsheets/ Pour effectuer des statistiques par colonne, on utilisera summarise pour des statistiques effectuées sur une seule colonne. sumarise peut prendre le nombre désiré de statistiques dont la sortie est un scalaire. chicoute %&gt;% summarise(moyenne = mean(TotalFloral_nombre_m2, na.rm = TRUE), ecart_type = sd(TotalFloral_nombre_m2, na.rm = TRUE)) ## # A tibble: 1 x 2 ## moyenne ecart_type ## &lt;dbl&gt; &lt;dbl&gt; ## 1 52.1 40.4 Si l’on désire un sommaire sur toutes les variables sélectionnées, on utilisera summarise_all(). Pour spécifier que l’on désire la moyenne et l’écart-type on inscrit les noms des fonctions dans funs(). chicoute %&gt;% select(contains(&quot;pourc&quot;)) %&gt;% summarise_all(funs(mean, sd)) ## Warning: funs() is soft deprecated as of dplyr 0.8.0 ## please use list() instead ## ## # Before: ## funs(name = f(.) ## ## # After: ## list(name = ~f(.)) ## This warning is displayed once per session. ## # A tibble: 1 x 26 ## C_pourc_mean N_pourc_mean P_pourc_mean K_pourc_mean Ca_pourc_mean ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 50.3 2.20 0.139 0.889 0.388 ## # … with 21 more variables: Mg_pourc_mean &lt;dbl&gt;, S_pourc_mean &lt;dbl&gt;, ## # B_pourc_mean &lt;dbl&gt;, Cu_pourc_mean &lt;dbl&gt;, Zn_pourc_mean &lt;dbl&gt;, ## # Mn_pourc_mean &lt;dbl&gt;, Fe_pourc_mean &lt;dbl&gt;, Al_pourc_mean &lt;dbl&gt;, ## # C_pourc_sd &lt;dbl&gt;, N_pourc_sd &lt;dbl&gt;, P_pourc_sd &lt;dbl&gt;, ## # K_pourc_sd &lt;dbl&gt;, Ca_pourc_sd &lt;dbl&gt;, Mg_pourc_sd &lt;dbl&gt;, ## # S_pourc_sd &lt;dbl&gt;, B_pourc_sd &lt;dbl&gt;, Cu_pourc_sd &lt;dbl&gt;, ## # Zn_pourc_sd &lt;dbl&gt;, Mn_pourc_sd &lt;dbl&gt;, Fe_pourc_sd &lt;dbl&gt;, ## # Al_pourc_sd &lt;dbl&gt; On utilisera group_by() pour segmenter le tableau, et ainsi obtenir des statistiques pour chaque groupe. chicoute %&gt;% group_by(CodeTourbiere) %&gt;% summarise(moyenne = mean(TotalFloral_nombre_m2, na.rm = TRUE), ecart_type = sd(TotalFloral_nombre_m2, na.rm = TRUE)) ## # A tibble: 12 x 3 ## CodeTourbiere moyenne ecart_type ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 72.1 32.7 ## 2 2 37.1 32.9 ## 3 BEAU 149. 53.2 ## 4 BP 60.4 30.6 ## 5 BS2 27.2 15.5 ## 6 MB 64.7 40.8 ## 7 MR 35.1 10.5 ## 8 NBM 35.1 16.6 ## 9 NESP 21.4 4.88 ## 10 NTP 47.6 15.9 ## 11 SSP 25.7 11.1 ## 12 WTP 50.2 28.3 Dans le cas de summarise_all, les résultats s’affichent de la même manière. chicoute %&gt;% group_by(CodeTourbiere) %&gt;% select(N_pourc, P_pourc, K_pourc) %&gt;% summarise_all(funs(mean, sd)) ## Adding missing grouping variables: `CodeTourbiere` ## # A tibble: 12 x 7 ## CodeTourbiere N_pourc_mean P_pourc_mean K_pourc_mean N_pourc_sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2.26 0.156 0.880 0.250 ## 2 2 2.76 0.181 1.12 0.178 ## 3 BEAU 2.00 0.0967 1.12 0.179 ## 4 BP 2.05 0.158 0.747 0.161 ## 5 BS2 2.08 0.103 1.12 0.420 ## 6 MB 2.15 0.109 0.675 0.114 ## 7 MR 1.99 0.127 0.830 0.0802 ## 8 NBM 2.01 0.127 0.854 0.310 ## 9 NESP 1.76 0.135 0.945 0.149 ## 10 NTP 1.83 0.0873 0.402 0.166 ## 11 SSP 1.83 0.130 0.700 0.160 ## 12 WTP 1.79 0.0811 0.578 0.132 ## # … with 2 more variables: P_pourc_sd &lt;dbl&gt;, K_pourc_sd &lt;dbl&gt; Pour obtenir des statistiques à chaque ligne, mieux vaut utiliser apply(), tel que vu précédemment. Le point, ., représente le tableau dans la fonction. chicoute %&gt;% select(contains(&quot;pourc&quot;)) %&gt;% apply(., 1, sum) ## [1] 55.64299 55.76767 54.78856 55.84453 57.89671 55.53603 55.62526 ## [8] 55.10991 55.06295 55.16774 56.41123 55.47917 55.43537 55.79175 ## [15] 55.44561 54.85448 54.34262 55.03075 54.40533 51.89319 54.70172 ## [22] 54.62176 54.30250 53.86976 53.44731 53.86244 52.43280 54.34978 ## [29] 53.96756 51.46672 55.44267 54.70350 55.30711 56.16200 56.64710 ## [36] 55.95499 54.76370 54.32775 54.95419 53.37094 53.07855 53.04541 ## [43] 52.09520 52.40456 51.92376 53.33248 56.56405 56.35004 56.27185 ## [50] 55.56986 53.81654 55.39638 55.51961 54.88098 54.74774 51.08921 ## [57] 51.31462 53.46819 53.15640 52.82020 57.78038 57.94636 56.65558 ## [64] 56.28845 55.54463 56.51751 55.36497 56.00594 55.64247 56.56967 ## [71] 56.81674 55.87070 55.72308 56.14116 56.42611 55.35650 54.90469 ## [78] 54.03674 53.42991 53.99334 53.09085 53.23222 53.28212 53.63192 ## [85] 53.48102 52.31131 51.72026 51.10534 51.49055 51.59297 Prenons ce tableau des espèces menacées issu de l’Union internationale pour la conservation de la nature distribuées par l’OCDE. library(&quot;tidyverse&quot;) especes_menacees &lt;- read_csv(&#39;data/WILD_LIFE_09012019174644084.csv&#39;) ## Parsed with column specification: ## cols( ## IUCN = col_character(), ## `IUCN Category` = col_character(), ## SPEC = col_character(), ## Species = col_character(), ## COU = col_character(), ## Country = col_character(), ## `Unit Code` = col_character(), ## Unit = col_character(), ## `PowerCode Code` = col_double(), ## PowerCode = col_character(), ## `Reference Period Code` = col_logical(), ## `Reference Period` = col_logical(), ## Value = col_double(), ## `Flag Codes` = col_logical(), ## Flags = col_logical() ## ) Nous exécutons le pipeline suivant. especes_menacees %&gt;% dplyr::filter(IUCN == &#39;CRITICAL&#39;) %&gt;% dplyr::select(Country, Value) %&gt;% dplyr::group_by(Country) %&gt;% dplyr::summarise(n_critical_species = sum(Value)) %&gt;% dplyr::arrange(desc(n_critical_species)) %&gt;% dplyr::top_n(10) ## Selecting by n_critical_species ## # A tibble: 10 x 2 ## Country n_critical_species ## &lt;chr&gt; &lt;dbl&gt; ## 1 Czech Republic 2159 ## 2 United States 1409 ## 3 Germany 915 ## 4 Japan 628 ## 5 Austria 618 ## 6 Slovak Republic 602 ## 7 Canada 522 ## 8 Poland 485 ## 9 Switzerland 483 ## 10 Brazil 453 Ce pipeline consiste à: prendre le tableau especes_menacees, puis filtrer pour n&#39;obtenir que les espèces critiques, puis sélectionner les colonnes des pays et des valeurs (nombre d&#39;espèces), puis segmenter le tableaux en plusieurs tableaux selon le pays, puis appliquer la fonction sum pour chacun de ces petits tableaux (puis de recombiner ces sommaires), puis trier les pays en nombre décroissant de décompte d&#39;espèces, puis afficher le top 10 3.5.6 Exemple (difficile) Pour revenir à notre tableau chicoute, imaginez que vous aviez une station météo (station_A) située aux coordonnées (490640, 5702453) et que vous désiriez calculer la distance entre l’observation et la station. Prenez du temps pour réfléchir à la manière dont vous procéderez… On pourra créer une fonction qui mesure la distance entre un point x, y et les coordonnées de la station A… dist_station_A &lt;- function (x, y) { return(sqrt((x - 490640)^2 + (y - 5702453)^2)) } … puis ajouter une colonne avec mutate grâce à une fonction prenant les arguments x et y spécifiés. chicoute %&gt;% mutate(dist = dist_station_A(x = Longitude_m, y= Latitude_m)) %&gt;% select(ID, CodeTourbiere, Longitude_m, Latitude_m, dist) %&gt;% top_n(10) ## Selecting by dist ## # A tibble: 10 x 5 ## ID CodeTourbiere Longitude_m Latitude_m dist ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7 BP 484054 5706307 7631. ## 2 36 MR 459875 5701988 30769. ## 3 37 MR 459873 5701987 30771. ## 4 38 MR 459880 5701971 30764. ## 5 39 MR 459894 5701966 30750. ## 6 40 MR 459915 5701994 30728. ## 7 46 NBM 485975 5695688 8218. ## 8 48 NBM 485912 5696607 7519. ## 9 49 NBM 485903 5696611 7521. ## 10 50 NBM 485884 5696612 7532. Nous pourrions procéder de la même manière pour fusionner des données climatiques. Le tableau chicoute ne possède pas d’indicateurs climatiques, mais il est possible de les soutirer de stations météo placées près des site. Ces données ne sont pas disponibles pour le tableau de la chicouté, alors j’utiliserai des données fictives pour l’exemple. Voici ce qui pourrait être fait. Créer un tableau des stations météo ainsi que des indices météo associés à ces stations. Lier chaque site à une station (à la main où selon la plus petite distance entre le site et la station). Fusionner les indices climatiques aux sites, puis les sites aux mesures de rendement. Ces opérations demandent habituellement du tâtonnement. Il serait surprenant que même une personne expérimentée soit en mesure de compiler ces opérations sans obtenir de message d’erreur, et retravailler jusqu’à obtenir le résultat souhaité. L’objectif de cette section est de vous présenté un flux de travail que vous pourriez être amenés à effectuer et de fournir quelques éléments nouveau pour mener à bien une opération. Il peut être frustrant de ne pas saisir toutes les opérations: passez à travers cette section sans jugement. Si vous devez vous frotter à problème semblable, vous saurez que vous trouverez dans ce manuel une recette intéressante. mes_stations &lt;- data.frame(Station = c(&#39;A&#39;, &#39;B&#39;, &#39;C&#39;), Longitude_m = c(490640, 484870, 485929), Latitude_m = c(5702453, 5701870, 5696421), t_moy_C = c(13.8, 18.2, 16.30), prec_tot_mm = c(687, 714, 732)) mes_stations ## Station Longitude_m Latitude_m t_moy_C prec_tot_mm ## 1 A 490640 5702453 13.8 687 ## 2 B 484870 5701870 18.2 714 ## 3 C 485929 5696421 16.3 732 La fonction suivante calcule la distance entre des coordonnées x et y et chaque station d’un tableau de stations, puis retourne le nom de la station dont la distance est la moindre. dist_station &lt;- function (x, y, stations_df) { # stations est le tableau des stations à trois colonnes # 1iere: nom de la station # 2ieme: longitude # 3ieme: latitude distance &lt;- c() for (i in 1:nrow(stations_df)) { distance[i] &lt;- sqrt((x - stations_df[i, 2])^2 + (y - stations_df[i, 3])^2) } nom_station &lt;- as.character(stations_df$Station[which.min(distance)]) return(nom_station) } Testons la fonction avec des coordonnées. dist_station(x = 459875, y = 5701988, stations_df = mes_stations) ## [1] &quot;B&quot; Nous appliquons cette fonction à toutes les lignes du tableau, puis en retournons un échantillon. chicoute %&gt;% rowwise() %&gt;% mutate(Station = dist_station(x = Longitude_m, y = Latitude_m, stations_df = mes_stations)) %&gt;% select(ID, CodeTourbiere, Longitude_m, Latitude_m, Station) %&gt;% sample_n(10) ## Source: local data frame [10 x 5] ## Groups: &lt;by row&gt; ## ## # A tibble: 10 x 5 ## ID CodeTourbiere Longitude_m Latitude_m Station ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 76 1 486465 5702112 B ## 2 81 1 486504 5702081 B ## 3 89 WTP 487058 5700770 B ## 4 16 2 486498 5702643 B ## 5 80 1 486504 5702081 B ## 6 21 2 486524 5702537 B ## 7 15 2 486498 5702643 B ## 8 58 SSP 484466 5699784 B ## 9 62 BS2 486564 5702189 B ## 10 30 2 486350 5702494 B Cela semble fonctionner. On peut y ajouter un left_join() pour joindre les données météo au tableau principal. chicoute_weather &lt;- chicoute %&gt;% rowwise() %&gt;% mutate(Station = dist_station(x = Longitude_m, y = Latitude_m, stations_df = mes_stations)) %&gt;% left_join(y = mes_stations, by = &quot;Station&quot;) ## Warning: Column `Station` joining character vector and factor, coercing ## into character vector chicoute_weather %&gt;% sample_n(10) ## Source: local data frame [10 x 36] ## Groups: &lt;by row&gt; ## ## # A tibble: 10 x 36 ## ID CodeTourbiere Ordre Site Traitement DemiParcelle SousTraitement ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 54 NTP B 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 87 WTP E 2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 42 NESP J 2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 23 2 9 17 fertilisa… left Control ## 5 75 1 3 5 fertilisa… right Control ## 6 89 WTP E 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 7 39 MR I 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 8 77 1 3 6 temoin right Control ## 9 19 2 8 15 fertilisa… left B ## 10 64 BS2 G 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## # … with 29 more variables: Latitude_m.x &lt;dbl&gt;, Longitude_m.x &lt;dbl&gt;, ## # Rendement_g_5m2 &lt;dbl&gt;, TotalRamet_nombre_m2 &lt;dbl&gt;, ## # TotalVegetatif_nombre_m2 &lt;dbl&gt;, TotalFloral_nombre_m2 &lt;dbl&gt;, ## # TotalMale_nombre_m2 &lt;dbl&gt;, TotalFemelle_nombre_m2 &lt;dbl&gt;, ## # FemelleFruit_nombre_m2 &lt;dbl&gt;, FemelleAvorte_nombre_m2 &lt;dbl&gt;, ## # SterileFleur_nombre_m2 &lt;dbl&gt;, C_pourc &lt;dbl&gt;, N_pourc &lt;dbl&gt;, ## # P_pourc &lt;dbl&gt;, K_pourc &lt;dbl&gt;, Ca_pourc &lt;dbl&gt;, Mg_pourc &lt;dbl&gt;, ## # S_pourc &lt;dbl&gt;, B_pourc &lt;dbl&gt;, Cu_pourc &lt;dbl&gt;, Zn_pourc &lt;dbl&gt;, ## # Mn_pourc &lt;dbl&gt;, Fe_pourc &lt;dbl&gt;, Al_pourc &lt;dbl&gt;, Station &lt;chr&gt;, ## # Longitude_m.y &lt;dbl&gt;, Latitude_m.y &lt;dbl&gt;, t_moy_C &lt;dbl&gt;, ## # prec_tot_mm &lt;dbl&gt; 3.5.7 Exporter un tableau Simplement avec write_csv(). write_csv(chicoute_weather, &quot;data/chicoute_weather.csv&quot;) 3.5.8 Aller plus loin dans le tidyverse Le livre R for Data Science, de Garrett Grolemund et Hadley Wickham, est un incontournable. 3.6 Références Parent L.E., Parent, S.É., Herbert-Gentile, V., Naess, K. et Lapointe, L. 2013. Mineral Balance Plasticity of Cloudberry (Rubus chamaemorus) in Quebec-Labrador Bogs. American Journal of Plant Sciences, 4, 1508-1520. DOI: 10.4236/ajps.2013.47183 "],
["chapitre-visualisation.html", "4 Visualisation 4.1 Pourquoi explorer graphiquement? 4.2 Publier un graphique 4.3 Choisir le type de graphique le plus approprié 4.4 Choisir son outils de visualisation 4.5 Visualisation en R 4.6 Module de base pour les graphiques 4.7 La grammaire graphique ggplot2 4.8 Mon premier ggplot 4.9 Les graphiques comme outil d’exploration des données 4.10 Choisir les bonnes couleurs 4.11 Règles particulières", " 4 Visualisation ️ Objectifs spécifiques: À la fin de ce chapitre, vous comprendrez l’importance de l’exploration des données comprendrez les guides généraux pour créer un graphique approprié comprendrez la différence entre les modes impératifs et déclaratifs pour la création de graphique serez en mesure de créer des nuages de points, lignes, histogrammes, diagrammes en barres et boxplots en R saurez exporter un graphique en vue d’une publication Lorsque j’aborde un document scientifique, la première chose que je fais après avoir lu le résumé est de regarder les graphiques. Un graphique bien conçu est dense en information, de sorte qu’il met en lumière une information qui pourrait passer inaperçue dans un tableau. Reconnaissez-vous cette image? Source: GIEC, Bilan 2001 des changements climatiques : Les éléments scientifiques Elle a été conçue par Michael E. Mann, Raymond S. Bradley et Malcolm K. Hughes. Le graphique montre l’évolution des températures en °C normalisées selon la température moyenne entre 1961 et 1990 sur l’axe des Y en fonction du temps, sur l’axe des X. On le connait aujourd’hui comme le bâton de hockey, et on reconnait son rôle clé pour sensibiliser la civilisation entière face au réchauffement global. On aura recours à la visualisation des données pour plusieurs raison: en particulier, lorsque l’information d’un tableau devient difficile à interpréter. Ainsi, créer des graphiques est une tâche courante dans un flux de travail en science, que ce soit pour explorer les données ou les communiquer… ce à quoi cette section est vouée. 4.1 Pourquoi explorer graphiquement? La plupart des graphiques que vous générerez ne seront pas destinés à être publiés. Ils viseront probablement d’abord à explorer des données. Cela vous permettra de mettre en évidence de nouvelles perspectives. Prenons par exemple deux variables, \\(X\\) et \\(Y\\). Vous calculez leur moyenne, écart-type et la corrélation entre les deux variables (nous verrons les statistiques en plus de détails dans un prochain chapitre). Pour démontrer que ces statistiques ne vous apprendront pas grand chose sur la structure des données, Matejka et Fitzmaurice (2017) ont généré 12 jeux de données \\(X\\) et \\(Y\\), ayant chacun pratiquement les mêmes statistiques. Mais avec des structures bien différentes. Animation montrant la progression du jeu de données Datasaurus pour toutes les formes visées. Source: Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing. 4.2 Publier un graphique Vous voilà sensibilisé à l’importance d’explorer les données graphiquement. Mais ce qui ultimement émanera d’un projet sera le rapport que vous déposerez, l’article scientifique que vous ferez publier ou le billet de blogue que vous posterez. Les graphiques inclus dans vos publications méritent une attention particulière pour que votre audience puisse comprendre les découvertes et perspectives offertes par vos travaux. Pour ce faire, un graphique doit évidemment répondre honnêtement à la question posée, sans artifices inutiles, mais tout de même attrayante. 4.2.1 Cinq qualités d’un bon graphique Alberto Cairo, chercheur spécialisé en visualisation de données, a fait paraître en 2016 le livre The Truthful art, note cinq qualités d’une visualisation bien conçue (les citations de cette section proviennent de ma traduction de Alberto Cairo, The Truthful Art (2016), p. 45.). 1- Elle est véritable, puisqu’elle est basée sur une recherche exhaustive et honnête. Cela vaut autant pour les graphiques que pour l’analyse de données. Il s’agit froidement de présenter les données selon l’interprétation la plus exacte. Les pièges à éviter sont le picorage de cerises et la surinterprétation des données. Le picorage, c’est lorsqu’on réduit les perspectives afin de soutenir un argumentaire. Par exemple, retirer des données d’une région ou d’une décennie qui rendraient factice une conclusion fixée a priori. Ceci vaut autant pour les graphiques que pour les statistiques (nous parlerons du p-hacking au prochain chapitre). La surinterprétation, c’est lorsque l’on saute rapidement aux conclusions: par exemple, que l’on génère des corrélations, voire même des relations de causalités à partir de ce qui n’est que du bruit de fond. À ce titre, lors d’une conférence, Heather Krause insiste sur l’importance de faire en sorte que les représentations graphiques répondent correctement aux questions posées dans une étude (à voir!). 2- Elle est fonctionnelle, puisqu’elle constitue une représentation précise des données, et qu’elle est construite de manière à laisser les observateurs.trices prendre des initiatives conséquentes. “La seule chose qui est pire qu’un diagramme en pointe de tarte, c’est d’en présenter plusieurs” (Edward Tufte, designer, cité par Alberto Cairo, 2016, p. 50). Choisir le bon graphique pour représenter vos données est beaucoup moins une question de bon goût qu’une question de démarche rationnelle sur l’objectif visé par la présentation d’un graphique. Je présenterai des lignes guides pour sélectionner le type de graphique qui présentera vos données de manière fonctionnelle en fonction de l’objectif d’un graphique (d’ailleurs, avez-vous vraiment besoin d’un graphique?). 3- Elle est attrayante et intrigante, et même esthétiquement plaisante pour l’audience visée - les scientifiques d’abord, mais aussi le public en général. En sciences naturelles, la pensée rationnelle, la capacité à organiser la connaissance et créer de nouvelles avenues sont des qualités qui sont privilégiées au talent artistique. Que vous ayez où non des aptitudes en art visuel, présentez de l’information, pas des décorations. Excel vous permet d’ajouter une perspective 3D à un diagramme en barres. La profondeur contient-elle de l’information? Non. Cette décoration ne fait qu’ajouter de la confusion. Minimalisez, fournissez le plus d’information possible avec le moins d’éléments possibles. C’est ce que vous proposent les guides graphiques que j’introduirai plus loin. 4- Elle est pertinente, puisqu’elle révèle des évidences scientifiques autrement difficilement accessibles. Il s’agit de susciter un eurêka, dans le sens qu’elle génère une idée, et parfois une initiative, en un coup d’œil. Le graphique en bâton de hockey est un exemple où l’on a spontanément une idée de la situation. Cette situation peut être la présence d’un phénomène comme l’augmentation de la température globale, mais aussi l’absence de phénomènes pourtant attendus. 5- Elle est instructive, parce que si l’on saisit et accepte les évidences scientifiques qu’elle décrit, cela changera notre perception pour le mieux. En présentant cette qualité, Alberto Cairo voulait insister ses lecteurs.trices à choisir des sujets de discussion visuelle de manière à participer à un monde meilleur. En ce qui nous concerne, il s’agit de bien sélectionner l’information que l’on désire transmettre. Imaginez que vous avez travaillé quelques jours pour créer un graphique, sont vous êtes fier, mais vous (ou un collègue hiérarchiquement favorisé) vous rendez compte que le graphique soutient peu ou pas le propos ou l’objectif de votre thèse/mémoire/rapport/article. Si c’est bien le cas, vous feriez mieux de laisser tomber votre oeuvre et considérer votre démarche comme une occasion d’apprentissage. Alberto Cairo résume son livre The Truthful Art dans une entrevue avec le National Geographic. 4.3 Choisir le type de graphique le plus approprié Vous connaissez sans doute les nuages de point, les lignes, les histogrammes, les diagrammes en barre et en pointe de tarte. De nombreuses manières de présenter les données ont été développées. Les principaux types de graphique seront couverts dans ce chapitre. D’autres types spécialisés seront couverts dans les chapitres appropriés (graphiques davantage orientés vers les statistiques, les biplots, les dendrogrammes, les diagrammes ternaires, les cartes, etc.). La visualisation de données est aujourd’hui devenue une expertise en soi. Plusieurs personnes ayant acquis une expertise dans le domaine partage leurs expériences. À ce titre, le site from data to viz est à conserver dans vos marques-page. Il comprend des arbres décisionnels qui vous guident vers les options appropriées pour présenter vos données, puis fournissent des exemples en R que vous pourrez copier-coller-adapter dans vos feuilles de calcul. Également, je suggère le site internet de Ann K. Emery, qui présente des lignes guide pour présenté le graphique adéquat selon les données en main. De nombreuses recettes sont également proposées sur r-graph-gallery.com. En ce qui a trait aux couleurs, le choix n’est pas anodin. Si vous avez le souci des détails sur les éléments esthétiques de vos graphiques, je recommande la lecture de ce billet de blog de Lisa Charlotte Rost. Le Financial Times offre également ce guide visuel. Cairo (2016) propose de procéder avec ces étapes: Réfléchissez au message que vous désirez transmettre: comparer les catégories \\(A\\) et \\(B\\), visualiser une transition ou un changement de \\(A\\) vers \\(B\\), présenter une relation entre \\(A\\) et \\(B\\) ou la distribution de \\(A\\) et \\(B\\) sur une carte. Essayez différentes représentations: si le message que vous désirez transmettre a plusieurs volets, il se pourrait que vous ayez besoin de plus d’un graphique. Mettez de l’ordre dans vos données. C’était le sujet du chapitre 3. Testez le résultat. “Hé, qu’est-ce que tu comprends de cela?” Si la personne hausse les épaules, il va falloir réévaluer votre stratégie. 4.4 Choisir son outils de visualisation Les modules et logiciels de visualisation sont basés sur des approches que l’on pourrait placer sur un spectre allant de l’impératif au déclaratif. 4.4.1 Approche impérative Selon cette approche, vous indiquez comment placer l’information dans un espace graphique. Vous indiquer les symboles, les couleurs, les types de ligne, etc. Peu de choses sont automatisées, ce qui laisse une grande flexibilité, mais demande de vouer beaucoup d’énergie à la manière de coder pour obtenir le graphique désiré. Le module graphique de Excel, ainsi que le module graphique de base de R, utilisent des approches impératives. 4.4.2 Approche déclarative Les stratégies d’automatisation graphique se sont grandement améliorées au cours des dernières années. Plutôt que de vouer vos énergies à créer un graphique, il est maintenant possible de spécifier ce que l’on veut présenter. La visualisation déclarative vous permet de penser aux données et à leurs relations, plutôt que des détails accessoires. Jake Vanderplas, Declarative Statistical Visualization in Python with Altair (ma traduction) L’approche déclarative passe souvent par une grammaire graphique, c’est-à-dire un langage qui explique ce que l’on veut présenter - en mode impératif, on spécifie plutôt comment on veut présenter les données. Le module ggplot2 est le module déclaratif par excellence en R. 4.5 Visualisation en R En R, votre trousse d’outils de visualisation mériterait de comprendre les modules suivants. base. Le module de base de R contient des fonctions graphique très polyvalentes. Les axes sont générées automatiquement, on peut y ajouter des titres et des légendes, on peut créer plusieurs graphiques sur une même figure, on peut y ajouter différentes géométries (points, lignes et polygones), avec différents types de points ou de trait, et différentes couleurs, etc. Les modules spécialisés viennent souvent avec leurs graphiques spécialisés, construit à partir du module de base. En tant que module graphique impératif, on peut tout faire ou presque (pas d’interactivité), mais l’écriture du code est peut expressive. ggplot2. C’est le module graphique par excellence en R (et j’ose dire: en calcul scientifique). ggplot2 se base sur une grammaire graphique. À partir d’un tableau de données, une colonne peut définir l’axe des x, une autre l’axe des y, une autre la couleur couleur des points ou leur dimension. Une autre colonne définissant des catégories peut segmenter la visualisation en plusieurs graphiques alignés horizontalement ou verticalement. Des extensions de ggplot2 permettent de générer des cartes (ggmap), des diagrammes ternaires (ggtern), des animations (gganimate), etc. plotly. plotly offre une fonction toute simple pour rendre interactif un graphique ggplot2. plotly est aussi un module graphique en soit, particulièrement utile pour les graphiques interactifs. Nous survolerons rapidement le module de base, irons plus en profondeur avec ggplot2, puis je présenterai brièvement les graphiques interactifs avec plotly. 4.6 Module de base pour les graphiques Nous allons d’abord survoler le module de base, en mode impératif. La fonction de base pour les graphiques en R est plot(). Pour nous exercer avec cette fonction, chargeons d’abord le tableau de données d’exercice iris, publié en 1936 par le célèbre biostatisticien Ronald Fisher. data(iris) head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa Le tableau iris contient 5 colonnes, les 4 premières décrivant les longueurs et largeurs des pétales et sépales de différentes espèces d’iris dont le nom apparaît à la 5ième colonne. La manière la plus rapide d’extraire une colonne d’un tableau est d’appeler le tableau, suivit du $, puis du nom de la colonne, par exemple iris$Species. Pour générer un graphique avec la fonction plot(): plot(iris$Sepal.Length, iris$Petal.Length) Par défaut, le premier argument est le vecteur définissant l’axe des x et le deuxième est celui définissant l’axe des y. Le graphique précédent peut être amplement personnalisé en utilisant différents arguments. Exercice. Utilisez ces arguments dans la cellule de code de la figure plot(iris$Sepal.Length, iris$Petal.Length). Remarquez que la fonction a décidé toute seule de créer un nuage de point. La fonction plot() est conçue pour créer le graphique approprié selon le type des données spécifiées: lignes, boxplot, etc. Si l’on spécifiait les espèces comme argument x… plot(iris$Species, iris$Petal.Length) # ou bien # iris %&gt;% # select(Species, Petal.Length) %&gt;% # plot() De même, la fonction plot() appliquée à un tableau de données générera une représentation bivariée. plot(iris) Il est possible d’encoder des attributs grâce à des vecteurs de facteurs (catégories). plot(iris, col = iris$Species) L’argument type = &quot;&quot; permet de personnaliser l’apparence: type = &quot;p&quot;: points type = &quot;l&quot;: ligne type = &quot;o&quot; et type = &quot;b&quot;: ligne et points type = &quot;n&quot;: ne rien afficher Créons un jeu de données. time &lt;- seq(from = 0, to = 100, by = 10) height &lt;- abs(time * 0.1 + rnorm(length(time), 0, 2)) # abs pourvaleur absolue (changement de signe si négatif) plot(x = time, y = height, type = &#39;b&#39;, lty = 2, lwd = 1) Le type de ligne est spécifié par l’argument lty (qui peut prendre un chiffre ou une châine de caractères, i.e. 1 est équivalent de &quot;solid&quot;, 2 de &quot;dashed&quot;, 3 de &quot;dotted&quot;, etc.) et la largeur du trait (valeur numérique), par l’argument lwd. La fonction hist() permet quant à elle de créer des histogrammes. Parmi ses arguments, breaks est particulièrement utile, car il permet d’ajuster la segmentation des incréments. hist(iris$Petal.Length, breaks = 60) Exercice. Ajustez le titre de l’axe des x, ainsi que les limites de l’axe des x. Êtes-vous en mesure de colorer l’intérieur des barres en bleu? La fonction plot() peut être suivie de plusieurs autres couches comme des lignes (lines() ou abline()), des points (points()), du texte (text()), des polygones (polygon(), des légendes (legend())), etc. On peut aussi personnaliser les couleurs, les types de points, les types de lignes, etc. L’exemple suivant ajoute une ligne au graphique. Ne prêtez pas trop attention aux fonctions predict() et lm() pour l’instant: nous les verrons au chapitre 5. plot(time, height) lines(time, predict(lm(height ~ time))) Pour exporter un graphique, vous pouvez passer par le menu Export de RStudio. Mais pour des graphiques destinés à être publiés, je vous suggère d’exporter vos graphiques avec une haute résolution à la suite de la commande png() (ou jpg() ou svg()). png(filename = &#39;images/mon-graphique.png&#39;, width = 3000, height=2000, res=300) plot(x = iris$Petal.Length, y = iris$Sepal.Length, col = iris$Species, cex=3, # dimension des points pch = 16) # type de points dev.off() ## png ## 2 Ce format crée une version vectorielle du graphique, c’est-à-dire que l’image exportée est un fichier contenant les formes, non pas les pixels. Cela vous permet d’éditer votre graphique dans un logiciel de dessin vectoriel (comme Inkscape). J’ai utilisé le format d’image png, utile pour les images de type graphique, avec des changements de couleurs drastiques. Pour les photos, vous préférerez le format jpg. Des éditeurs demanderont peut-être des formats vectoriels comme pdf ou eps. Si vous ne trouvez pas de moyen de modifié un aspect du graphique dans le code (bouger des étiquettes ou des légendes, ajouter des éléments graphiques), vous pouvez exporter votre graphique en format svg (par la commande svg(). Ce format vectoriel peut être ouvert avec des logiciels de dessin vectoriel comme le logiciel libre Inkscape. Le module de base de R comprend une panoplie d’autres particularités que je ne couvrirai pas ici, en faveur du module ggplot2. 4.7 La grammaire graphique ggplot2 Brièvement, une grammaire graphique permet de schématiser (ma traduction de to map) des données sur des attributs esthétiques avec des géométries.Cette approche permet de dégager 5 composantes. Les données. Votre tableau est bien sûr un argument nécessaire pour générer le graphique. Les marqueurs. Un terme abstrait pour désigner les points, les lignes, les polygones, les barres, les flèches, etc. Les attributs encodés. La position, la dimension, la couleur ou la forme que prendront les géométries. En ggplot2, on les nomme les aesthetics. Les attributs globaux. Les attributs sont globaux lorsqu’ils sont constant (ils ne dépendent pas d’une variable). Les valeurs par défaut conviennent généralement, mais certains attributs peuvent être spécifiés: par exemple la forme ou la couleur des points, le type de ligne. Les thèmes. Le thème du graphique peut être spécifié dans son ensemble, c’est-à-dire en utilisant un thème prédéfini, mais l’on peut modifier certains détails. Le flux de travail pour créer un graphique à partir d’une grammaire ressemble donc à ceci: Avec mon tableau, Créer un marqueur ( encoder(position X = colonne A, position Y = colonne B, couleur = colonne C) forme globale = 1) Avec un thème noir et blanc Le module tidyverse installera des modules utilisés de manière récurrente dans ce cours, comme ggplot2, dplyr, tidyr et readr. Je recommande de le charger au début de vos sessions de travail. library(&quot;tidyverse&quot;) L’approche tidyverse est une grammaire des données. Le module ggplot2, qui en fait partie, est une grammaire graphique (d’où le gg de ggplot). 4.8 Mon premier ggplot Pour notre premier exercice, je vais charger un tableau depuis le fichier de données abalone.data depuis un dépôt sur internet. Je n’irai pas dans les détails sur les tableaux de données, puisque c’est le sujet du chapitre 3. Le fichier de données porte sur un escargot de mer et comprend le sexe (M: mâle, F: femelle et I: enfant), des poids et dimensions des individus observés, et le nombre d’anneaux comptés dans la coquille. abalone &lt;- read_csv(&quot;data/abalone.csv&quot;) ## Parsed with column specification: ## cols( ## Type = col_character(), ## LongestShell = col_double(), ## Diameter = col_double(), ## Height = col_double(), ## WholeWeight = col_double(), ## ShuckedWeight = col_double(), ## VisceraWeight = col_double(), ## ShellWeight = col_double(), ## Rings = col_double() ## ) Inspectons l’entête du tableau avec la fonction head(). head(abalone) ## # A tibble: 6 x 9 ## Type LongestShell Diameter Height WholeWeight ShuckedWeight ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 M 0.455 0.365 0.095 0.514 0.224 ## 2 M 0.35 0.265 0.09 0.226 0.0995 ## 3 F 0.53 0.42 0.135 0.677 0.256 ## 4 M 0.44 0.365 0.125 0.516 0.216 ## 5 I 0.33 0.255 0.08 0.205 0.0895 ## 6 I 0.425 0.3 0.095 0.352 0.141 ## # … with 3 more variables: VisceraWeight &lt;dbl&gt;, ShellWeight &lt;dbl&gt;, ## # Rings &lt;dbl&gt; Suivant la grammaire graphique ggplot2, on pourra créer ce graphique de points comprenant les attributs suivants suivants. data = abalone, le fichier de données. mapping = aes(...), spécifié comme attribut de la fonction ggplot(), cet encodage (ou aesthetic) reste l’encodage par défaut pour tous les marqueurs du graphique. Toutefois, l’encodage mapping = aes() peut aussi être spécifié dans la fonction du marqueur (par exemple geom_point()). Dans l’encodage global du graphique, on place en x la longueur de la coquille (x = LongestShell) et on place en y le poids de la coquille (y = ShellWeight). Pour ajouter un marqueur, on utilise le +. Généralement, on change aussi de ligne. Le marqueur ajouté est un point, geom_point(), dans lequel on spécifie un encodage de couleur sur la variable Type (colour = Type) et un encodage de dimension du point sur la variable rings (size = Rings). L’attribut alpha = 0.5 se situe hors du mapping et de la fonction aes(): c’est un attribut identique pour tous les points. ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) + geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5) Il existe plusieurs types de marqueurs: geom_point pour les points geom_line pour les lignes geom_bar pour les diagrammes en barre en décompte, geom_col en terme de grandeur et geom_histogram pour les histogrammes geom_boxplot pour les boxplots geom_errorbar, geom_pointrange ou geom_crossbar pour les marges d’erreur geom_map pour les cartes etc. Il existe plusieurs attributs d’encodage: la position x, y et z (z pertinent notamment pour le marqueur geom_tile()) la taille size la forme des points shape la couleur, qui peut être discrète ou continue : colour, pour la couleur des contours fill, pour la couleur de remplissage le type de ligne linetype la transparence alpha et d’autres types spécialisés que vous retrouverez dans la documentation des marqueurs Les types de marqueurs et leurs encodages sont décrits dans la documentation de ggplot2, qui fournit des feuilles aide-mémoire qu’il est commode d’imprimer et d’afficher près de soi. Aide-mémoire de ggplot2, source: https://www.rstudio.com/resources/cheatsheets/ 4.8.0.1 Les facettes Dans ggplot2, les facetttes sont un type spécial d’encodage utilisés pour définir des grilles de graphique. Elles prennent deux formes: Le collage, facet_wrap(). Une variable catégorielle est utilisée pour segmenter les graphiques en plusieurs graphiques, qui sont placés l’un à la suite de l’autre dans un arrangement spécifié par un nombre de colonne ou un nombre de ligne. La grille, facet_grid(). Une ou deux variables segmentent les graphiques selon les colonnes et les lignes. Les facettes peuvent être spécifiées n’importe où dans la chaîne de commande de ggplot2, mais conventionnellement, on les place tout de suite après la fonction ggplot(). ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) + facet_wrap(~Type, ncol=2) + geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5) La fonction cut() permet de discrétiser des variables continues en catégories ordonnées - les fonctions peuvent être utilisées à l’intérieur de la fonction ggplot. ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) + facet_grid(Type ~ cut(Rings, breaks = seq(0, 30, 5))) + geom_point(mapping = aes(colour = Type), alpha = 0.5) Par défaut, les axes des facettes, ainsi que leurs dimensions, sont les mêmes. Une telle représentation permet de comparer les facets sur une même échelle. Les axes peuvent être définis selon les données avec l’argument scales, tandis que l’espace des facettes peut être conditionné selon l’argument space - pour plus de détails, voir la fiche de documentation. Exercice. Personnalisez le graphique avec les données abalone en remplaçant les variables et en réorganisant les facettes. 4.8.1 Plusieurs sources de données Il peut arriver que les données pour générer un graphique proviennent de plusieurs tableaux. Lorsqu’on ne spécifie pas la source du tableau dans un marqueur, la valeur par défaut est le tableau spécifier dans l’amorce ggplot(). Il est néanmoins possible de définir une source personnalisée pour chaque marqueur en spécifiant data = ... comme argument du marqueur. abalone_siteA &lt;- data.frame(LongestShell = c(0.3, 0.8, 0.7), ShellWeight = c(0.05, 0.81, 0.77)) ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) + geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5) + geom_point(data = abalone_siteA, size = 8, shape = 4) 4.8.2 Exporter avec style Le fond gris est une marque distinctive de ggplot2. Il n’est toutefois pas apprécié de tout le monde. D’autres thèmes dits complets peuvent être utilisés (liste des thèmes complets). Les thèmes complets sont appelés avant la fonction theme(), qui permet d’effectuer des ajustements précis dont la liste exhaustive se trouve dans la documentation de ggplot2. Vous pouvez aussi personnaliser le titre des axes (xlab() et ylab()), leur limites (xlim() et ylim()) ou spécifier un titre global (ggtitle()). Pour exporter un ggplot, on pourra utiliser les commandes de R png(), svg() ou pdf(), ou les outils de RStudio. Toutefois, ggplot2 offre la fonction ggsave(), que l’on place en remorque du graphique, en spécifiant les dimensions (width et height) ainsi que la résolution (dpi). La résolution d’un graphique destiné à la publication est typiquement de plus de 300 dpi. ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) + geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5) + xlab(&quot;Length (mm)&quot;) + ylab(&quot;Shell weight (g)&quot;) + ggtitle(&quot;Abalone&quot;) + xlim(c(0, 1)) + theme_classic() + theme(axis.title = element_text(size=20), axis.text = element_text(size=20), axis.text.y = element_text(size=20, angle=90, hjust=0.5), legend.box = &quot;horizontal&quot;) ggsave(&quot;images/abalone.png&quot;, width = 8, height = 8, dpi = 300) Nous allons maintenant couvrir différents types de graphiques, accessibles selon différents marqueurs: les nuages de points les diagrammes en ligne les boxplots les histogrammes les diagrammes en barres 4.8.3 Nuages de points L’exemple précédent est un nuage de points, que nous avons généré avec le marqueur geom_point(), qui a déjà été passablement introduit. L’exploration de ces données a permis de détecter une croissance exponentielle du poids de la coquille en fonction de sa longueur. Il est clair que les abalones juvéniles (Type I) sont plus petits et moins lourds, mais nous devrons probablement procéder à des tests statistiques pour vérifier s’il y a des différences entre mâles et femelles. Le graphique étant très chargé, nous avons utilisé des stratégies pour l’alléger en utilisant de la transparence et des facettes. Le marqueur geom_jitter() peut permettre de mieux apprécier la dispersion des points en ajoutant une dispersion randomisée en x ou en y. ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) + geom_jitter(mapping = aes(colour = Type, size = Rings), alpha = 0.5, width = 0.05, height=0.1) Dans ce cas-ci, ça ne change pas beaucoup, mais retenons-le pour la suite. 4.8.4 Diagrammes en lignes Les lignes sont utilisées pour exprimer des liens entre une suite d’information. Dans la plupart des cas, il s’agit d’une suite d’information dans le temps que l’on appelle les séries temporelles. En l’occurrence, les lignes devraient être évitées si la séquence entre les variables n’est pas évidente. Nous allons utiliser un tableau de données de R portant sur la croissance des orangers. data(Orange) head(Orange) ## Grouped Data: circumference ~ age | Tree ## Tree age circumference ## 1 1 118 30 ## 2 1 484 58 ## 3 1 664 87 ## 4 1 1004 115 ## 5 1 1231 120 ## 6 1 1372 142 La première colonne spécifie le numéro de l’arbre mesuré, la deuxième son âge et la troisième sa circonférence. Le marqueur geom_line() permet de tracer la tendance de la circonférence selon l’âge. En encodant la couleur de la ligne à l’arbre, nous pourrons tracer une ligne pour chacun d’entre eux. ggplot(data = Orange, mapping = aes(x = age, y = circumference)) + geom_line(aes(colour = Tree)) La légende ne montre pas les numéros d’arbre en ordre croissance. En effet, la légende (tout comme les facettes) classe les catégories prioritairement selon l’ordre des catégories si elles sont ordinales, ou par ordre alphabétique si les catégories sont nominales. Inspectons la colonne Tree en inspectant le tableau avec la commande str() - la commande glimpse() du tidyverse donne un sommaire moins complet que str(). str(Orange) ## Classes &#39;nfnGroupedData&#39;, &#39;nfGroupedData&#39;, &#39;groupedData&#39; and &#39;data.frame&#39;: 35 obs. of 3 variables: ## $ Tree : Ord.factor w/ 5 levels &quot;3&quot;&lt;&quot;1&quot;&lt;&quot;5&quot;&lt;&quot;2&quot;&lt;..: 2 2 2 2 2 2 2 4 4 4 ... ## $ age : num 118 484 664 1004 1231 ... ## $ circumference: num 30 58 87 115 120 142 145 33 69 111 ... ## - attr(*, &quot;formula&quot;)=Class &#39;formula&#39; language circumference ~ age | Tree ## .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; ## - attr(*, &quot;labels&quot;)=List of 2 ## ..$ x: chr &quot;Time since December 31, 1968&quot; ## ..$ y: chr &quot;Trunk circumference&quot; ## - attr(*, &quot;units&quot;)=List of 2 ## ..$ x: chr &quot;(days)&quot; ## ..$ y: chr &quot;(mm)&quot; En effet, la colonne Tree est un facteur ordinal dont les niveaux sont dans le même ordre que celui la légende. 4.8.5 Les histogrammes Nous avons vu les histogrammes dans la brève section sur les fonctions graphiques de base dans R: il s’agit de segmenter l’axe des x en incréments, puis de présenter sur l’axe de y le nombre de données que l’on retrouve dans cet incrément. Le marqueur à utiliser est geom_histogram(). Revenons à nos escargots. Comment présenteriez-vous la longueur de la coquille selon la variable Type? Selon des couleurs ou des facettes? La couleur, dans le cas des histogrammes, est celle du pourtour des barres. Pour colorer l’intérieur des barres, l’argument à utiliser est fill. ggplot(data = abalone, mapping = aes(x = LongestShell)) + geom_histogram(mapping = aes(fill = Type), colour = &#39;black&#39;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. On n’y voit pas grand chose. Essayons plutôt les facettes. ggplot(data = abalone, mapping = aes(x = LongestShell)) + facet_grid(Type ~ .) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Les facettes permettent maintenant de bien distinguer la distribution des longueur des juvéniles. L’argument bins, tout comme l’argument breaks du module graphique de base, permet de spécifier le nombre d’incréments, ce qui peut être très utile en exploration de données. ggplot(data = abalone, mapping = aes(x = LongestShell)) + facet_grid(Type ~ .) + geom_histogram(bins=60, colour = &#39;white&#39;) Le nombre d’incréments est un paramètre qu’il ne faut pas sous-estimer. À preuve, ce tweet de [@NicholasStrayer](https://twitter.com/NicholasStrayer): Histograms are fantastic, but make sure your bin-width/number is chosen well. This is the exact same data, plotted with different bin-widths. Notice that the pattern doesn't necessarily get clearer as bin num increases. #dataviz pic.twitter.com/3MhSFwTVPH — Nick Strayer (@NicholasStrayer) 7 août 2018 4.8.6 Boxplots Les boxplots sont une autre manière de visualiser des distributions. L’astuce est de créer une boîte qui s’étant du premier quartile (valeur où l’on retrouve 25% de données dont la valeur est inférieure) au troisième quartile (valeur où l’on retrouve 75% de données dont la valeur est inférieure). Une barre à l’intérieur de cette boîte est placée à la médiane (qui est en fait le second quartile). De part et d’autre de la boîte, on retrouve des lignes spécifiant l’étendue hors quartile. Cette étendue peut être déterminée de plusieurs manières, mais dans le cas de ggplot2, il s’agit de 1.5 fois l’étendue de la boîte (l’écart interquartile). Au-delà de ces lignes, on retrouve les points représentant les valeurs extrêmes. Le marqueur à utiliser est geom_boxplot(). L’encodage x est la variable catégorielle et l’encodage y est la variable continue. ggplot(data = abalone, mapping = aes(x = Type, y = LongestShell)) + geom_boxplot() Exercice. On suggère parfois de présenter les mesures sur les boxplots. Utiliser geom_jitter avec un bruit horizontal. 4.8.7 Les diagrammes en barre Les diagrammes en barre représente une variable continue associée à une catégorie. Les barres sont généralement horizontales et ordonnées. Nous y reviendrons à la fin de ce chapitre, mais retenez pour l’instant que dans tous les cas, les diagrammes en barre doivent inclure le zéro pour éviter les mauvaises interprétations. Pour les diagrammes en barre, nous allons utiliser les données de l’union internationale pour la conservation de la nature distribuées par l’OCDE. # Certaines colonnes de caractère sont considérées comme booléennes # mieux vaut définir leur type pour s&#39;assurer que le bon type # soit attribué especes_menacees &lt;- read_csv(&#39;data/WILD_LIFE_09012019174644084.csv&#39;, col_types = list(&quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;d&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;d&quot;, &quot;c&quot;, &quot;c&quot;)) head(especes_menacees) ## # A tibble: 6 x 15 ## IUCN `IUCN Category` SPEC Species COU Country `Unit Code` Unit ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 TOT_… Total number o… MAMM… Mammals AUS Austra… NBR Numb… ## 2 ENDA… Number of enda… MAMM… Mammals AUS Austra… NBR Numb… ## 3 CRIT… Number of crit… MAMM… Mammals AUS Austra… NBR Numb… ## 4 VULN… Number of vuln… MAMM… Mammals AUS Austra… NBR Numb… ## 5 THRE… Total number o… MAMM… Mammals AUS Austra… NBR Numb… ## 6 TOT_… Total number o… MAMM… Mammals AUT Austria NBR Numb… ## # … with 7 more variables: `PowerCode Code` &lt;dbl&gt;, PowerCode &lt;chr&gt;, ## # `Reference Period Code` &lt;chr&gt;, `Reference Period` &lt;chr&gt;, Value &lt;dbl&gt;, ## # `Flag Codes` &lt;chr&gt;, Flags &lt;chr&gt; L’exercice consiste à créer un diagramme en barres horizontales du nombre d’espèces menacées de manière critique pour les 10 pays qui en contiennent le plus. Je vais effectuer quelques opérations sur ce tableau afin d’en arriver avec un tableau que nous pourrons convenablement mettre en graphique: n’y portez pas trop attention pour l’instant: ces opérations sont un avant-goût du prochain chapitre. Nous allons filtrer le tableau pour obtenir les espèces critiquement menacées, sélectionner seulement le pays et le nombre d’espèces, les grouper par pays, additionner toutes les espèces pour chaque pays, les placer en ordre descendant et enfin sélectionner les 10 premiers. Comme vous le voyez, la création de graphique est liée de près avec la manipulation des tableaux! especes_crit &lt;- especes_menacees %&gt;% filter(IUCN == &#39;CRITICAL&#39;) %&gt;% dplyr::select(Country, Value) %&gt;% group_by(Country) %&gt;% summarise(n_critical_species = sum(Value)) %&gt;% arrange(desc(n_critical_species)) %&gt;% head(10) especes_crit ## # A tibble: 10 x 2 ## Country n_critical_species ## &lt;chr&gt; &lt;dbl&gt; ## 1 Czech Republic 2159 ## 2 United States 1409 ## 3 Germany 915 ## 4 Japan 628 ## 5 Austria 618 ## 6 Slovak Republic 602 ## 7 Canada 522 ## 8 Poland 485 ## 9 Switzerland 483 ## 10 Brazil 453 Le premier type de diagramme en barre que nous allons couvrir est obtenu par le marqueur geom_col(). ggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) + geom_col() Ce graphique est perfectible. Les barres sont verticales et non ordonnées. Souvenons-nous que ggplot2 ordonne par ordre alphabétique si aucun autre ordre est spécifié. Nous pouvons changer l’ordre en changeant l’ordre des niveaux de la variable Country selon le nombre d’espèces grâce à la fonction fct_reorder. especes_crit &lt;- especes_crit %&gt;% mutate(Country = fct_reorder(Country, n_critical_species)) Pour faire pivoter le graphique, nous ajoutons coord_flip() à la séquence. ggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) + geom_col() + coord_flip() Une autre méthode, geom_bar(), est un raccourcis permettant de compter le nombre d’occurrence d’une variable unique. Par exemple, dans le tableau abalone, le nombre de fois que chaque niveau de la variable Type ggplot(data = abalone, mapping = aes(x = Type)) + geom_bar() + coord_flip() Personnellement, je préfère passer par un diagramme en lignes avec le marqueur geom_segment(). Cela me donne la flexibilité pour définir un largeur de trait et éventuellement d’ajouter un point au bout pour en faire un diagramme en suçon. ggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) + geom_segment(mapping = aes(xend=Country, yend = 0), lwd = 2) + geom_point(size=6, colour = &quot;black&quot;) + coord_flip() + theme_bw() Les diagrammes en barre peuvent être placés en relation avec d’autres. Reprenons notre manipulation de données précédente, mais en incluant tous les pays. especes_pays_iucn &lt;- especes_menacees %&gt;% filter(IUCN %in% c(&#39;ENDANGERED&#39;, &#39;VULNERABLE&#39;,&#39;CRITICAL&#39;)) %&gt;% dplyr::select(IUCN, Country, Value) %&gt;% group_by(Country, IUCN) %&gt;% summarise(n_species = sum(Value)) %&gt;% group_by(Country) %&gt;% mutate(n_tot = sum(n_species)) %&gt;% ungroup() %&gt;% # pour pouvoir modifier Country, non modifiable tant qu&#39;elle est une variable de regroupement (voir group_by) mutate(Country = fct_reorder(Country, n_tot)) head(especes_pays_iucn) ## # A tibble: 6 x 4 ## Country IUCN n_species n_tot ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Australia CRITICAL 228 1752 ## 2 Australia ENDANGERED 701 1752 ## 3 Australia VULNERABLE 823 1752 ## 4 Austria CRITICAL 618 2826 ## 5 Austria ENDANGERED 924 2826 ## 6 Austria VULNERABLE 1284 2826 Pour placer les barres les unes à côté des autres, nous spécifions position = &quot;dodge&quot;. ggplot(data = especes_pays_iucn, mapping = aes(x = Country, y = n_species)) + geom_col(aes(fill=IUCN), position = &quot;dodge&quot;) + coord_flip() Il est parfois plus pratique d’utiliser les facettes. ggplot(data = especes_pays_iucn, mapping = aes(x = Country, y = n_species)) + facet_grid(IUCN ~ .) + geom_col() + coord_flip() 4.8.8 Exporter un graphique Plus besoin d’utiliser la fonction png() en mode ggplot2. Utilisons plutôt ggsave(). ggplot(data = especes_pays_iucn, mapping = aes(x = Country, y = n_species)) + facet_grid(IUCN ~ .) + geom_col(aes(fill=IUCN)) + coord_flip() ggsave(&quot;images/especes_pays_iucn.png&quot;, width = 6, height = 8, dpi = 300) 4.9 Les graphiques comme outil d’exploration des données La plupart des graphiques que vous créerez ne seront pas destinés à être publiés, mais serviront d’outil d’exploration des données. Le jeu de données datasaurus, présenté en début de chapitre, permet de saisir l’importance des outils graphiques pour bien comprendre les données. datasaurus &lt;- read_tsv(&#39;data/DatasaurusDozen.tsv&#39;) ## Parsed with column specification: ## cols( ## dataset = col_character(), ## x = col_double(), ## y = col_double() ## ) head(datasaurus) ## # A tibble: 6 x 3 ## dataset x y ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 dino 55.4 97.2 ## 2 dino 51.5 96.0 ## 3 dino 46.2 94.5 ## 4 dino 42.8 91.4 ## 5 dino 40.8 88.3 ## 6 dino 38.7 84.9 Projetons d’abord les coordonnées x et y sur un graphique. J’utilise FacetGrid ici, sachant que ce sera utile pour l’exploration. ggplot(data = datasaurus, mapping = aes(x = x, y = y)) + geom_point() Ce graphique pourrait ressembler à une distribution binormale, ou un coup de 12 dans une porte de grange. Mais on aperçoit des données alignées, parfois de manière rectiligne, parfois en forme d’ellipse. Le tableau datasaurus a une colonne d’information supplémentaire. Utilisons-la comme catégorie pour générer des couleurs différente. ggplot(data = datasaurus, mapping = aes(x = x, y = y)) + geom_point(mapping = aes(colour = dataset)) Ce n’est pas vraiment plus clair. Il y a toutefois des formes qui se dégage, comme des ellipse et des lignes. Et si je regarde bien, j’y vois une étoile. La catégorisation pourrait-elle être mieux utilisée si on segmentait par facettes au lieu de des couleurs? ggplot(data = datasaurus, mapping = aes(x = x, y = y)) + facet_wrap(~dataset, nrow=2) + geom_point(size = 0.5) + coord_equal() Voilà! Fait intéressant, ni les statistiques, ni les algorithmes de regroupement ne nous auraient été utiles pour différencier les groupes! 4.9.1 Des graphiques interactifs! Les graphiques sont traditionnellement des images statiques. Toutefois, les graphiques n’étant pas dépendants de supports papiers peuvent être utilisés de manière différente, en ajoutant une couche d’interaction. Conçue à Montréal, plotly est un module graphique interactif en soi. Il peut être utilisé grâce à son outil web, tout comme il peut être interfacé avec R, Python, javascript, etc. Mais ce qui retient notre attention ici est son interface avec ggplot2. Les graphiques ggplot2 peuvent être enregistrés en tant qu’objets. Il peuvent conséquemment être manipulés par des fonctions. La fonction ggplotly permet de rendre votre ggplot interactif. library(&quot;plotly&quot;) ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout especes_crit_bar &lt;- ggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) + geom_segment(mapping = aes(xend=Country, yend = 0), lwd = 2) + geom_point(size=6) + coord_flip() # ggplotly(especes_crit_bar) # erreur en Rmd 4.9.2 Des extensions de ggplot2 ggplot2 est un module graphique élégant et polyvalent. Il a pourtant bien des limitations. Justement, le module est conçu pour être implémenté avec des extensions. Vous en trouverez plusieurs sur ggplot2-exts.org, mais en trouverez de nombreuses autres en cherchant avec le terme ggplot2 sur github.com, probablement la plate-forme (voire un réseau social) de développement de logiciels la plus utilisée dans le monde. En voici quelques unes. ggthemr: spécifier un thème graphique une seule fois dans votre session, et tout le reste suit. cowplot permet de créer des graphiques prêts pour la publication, par exemple en créant des grilles de plusieurs ggplots, en les numérotant, etc. Si les thèmes de base ne vous conviennent pas, vous en trouverez d’autres en installant ggthemes. ggmap et ggspatial sont deux extensions pour créer des cartes. Un chapitre sur les données spatiales est en développement. ggtern permet de créer des diagrammes ternaires, qui sont utiles pour la visualisation de proportions incluant trois composantes. Ce sujet est couvert au chapitre 6, en développement. 4.9.3 Aller plus loin avec ggplot2 Claus O. Wilke est professeur en biologie intégrative à l’Université du Texas à Austin. Son livre Fundamentals of Data Visualization est un guide théorique et pratique pour la visualisation de données avec ggplot2. Le site data-to-viz.com vous accompagne dans le choix du graphique à créer selon vos données. Le site r-graph-gallery.com offre des recettes pour créer des graphiques avec ggplot2. 4.10 Choisir les bonnes couleurs La couleur est une information. Les couleurs devraient être sélectionnées d’abord pour être lisibles par les personnes ne percevant pas les couleurs, selon le support (apte à être photocopié, lisible à l’écran, lisible sur des documents imprimés en noir et blanc) et selon le type de données. - Données continues ou catégorielles ordinales: gradient (transition graduelle d’une couleur à l’autre), séquence (transition saccadée selon des groupes de données continues) ou divergentes (transition saccadée d’une couleur à l’autre vers des couleurs divergentes, par exemple orange vers blanc vers bleu). - Données catégorielles nominales: couleurs éloignées d’une catégorie à une autre (plus il y a de catégories, plus les couleurs sont susceptibles de se ressembler). Capture d’écran de colorbrewer2.org, qui propose des palettes de couleurs pour créer des cartes, mais l’information est pertinente pour tout type de graphique. 4.11 Règles particulières Les mauvais graphiques peuvent survenir à cause de l’ignorance, bien sûr, mais souvent ils existent pour la même raison que la boeuferie [bullhist] verbale ou écrite. Parfois, les gens ne se soucient pas de la façon dont ils présentent les données aussi longtemps que ça appuie leurs arguments et, parfois, ils ne se soucient pas que ça porte à confusion tant qu’ils ont l’air impressionnant. \\(-\\) Carl Bergstorm et Jevin West, Calling Bullshit Read-Along Week 6: Data Visualization Une représentation visuelle est un outil tranchant qui peut autant présenter un état véritable des données qu’une perspective trompeuse. Bien souvent, une ou plusieurs des 5 qualités ne sont pas respectées. Les occasions d’erreur ne manquent pas - j’en ferai mention dans la section Choisir le bon type de graphique. Pour l’instant, notons quelques règles particulières. 4.11.1 Ne tronquez pas inutilement l’axe des \\(y\\) Tronquer l’axe vertical peut amener à porter de fausses conclusions. Effets sur la perception d’utiliser différentes références. Source: Yau (2015), Real Chart Rules to Follow. La règle semble simple: les diagrammes en barre (utilisés pour représenter une grandeur) devraient toujours présenter le 0 et les diagrammes en ligne (utilisés pour présenter des tendances) ne requiert pas nécessairement le zéro ((Bergstrom et West, Calling bullshit: Misleading axes on graphs)[http://callingbullshit.org/tools/tools_misleading_axes.html]). Mais le zéro n’est pas toujours lié à une quantité particulière, par exemple, la température ou un log-ratio. De plus, avec un diagramme en ligne on pourra toujours magnifier des tendances en zoomant sur une variation somme toute mineure. On arrive donc moins à une règle qu’une qualité d’un bon graphique, en particulier la qualité no 1 de Cairo: offrir une représentation honnête des données. Par exemple, Nathan Yau, auteur du blogue Flowing Data, propose de présenter des résultats de manière relative à la mesure initiale. C’est d’ailleurs ce qui a été fait pour générer le graphique de Michael Mann et al., ci-dessus, où le zéro correspond à la moyenne des températures enregistrées entre 1961 et 1990. Il peut être tentant de tronquer l’axe des \\(y\\) lorsque l’on désire superposer deux axes verticaux. Souvent, l’utilisation de plusieurs axes verticaux amène une perception de causalité dans des situations de fausses corrélations. On ne devrait pas utiliser plusieurs axes verticaux. 4.11.2 Utilisez un encrage proportionnel Cette règle a été proposée par Edward Tufte dans Visual Display of Quantitative Information. Une des raisons pour lesquelles on évite de tronquer l’axe des \\(y\\) en particulier pour les diagrammes en barre est que l’aire représentant une mesure (la quantité d’“encre” nécessaire pour la dessiner) devrait être proportionnelle à sa magnitude. Les diagrammes en barre sont particulièrement sensibles à cette règle, étant donnée que la largeur des barres peuvent amplifier l’aire occupée. Deux solutions dans ce cas: (1) utiliser des barres minces ou (2) préférer des “diagrammes de points” (dot charts, à ne pas confondre aux nuages de points). L’encrage a beau être proportionnel, la difficulté que les humains éprouvent à comparer la dimension des cercles, et a fortiori la dimension de parties de cercle, donne peu d’avantage à utiliser des diagrammes en pointe de tarte, souvent utilisés pour illustrer des proportions. Nathan Yau suggère de les utiliser avec suspicions et d’explorer d’autres options. Pour comparer deux proportions, une avenue intéressante est le diagramme en pente, suggéré notamment par Ann K. Emery. Par extension, le diagramme en pente devient un diagramme en ligne lorsque plusieurs types de proportions sont comparées, ou lorsque des proportions évoluent selon des données continuent. De la même manière, les diagrammes en bulles ne devraient pas être représentatifs de la quantité, mais plutôt de contextualiser des données. Justement, le graphique tiré des données de Gap minder présenté plus haut est une contextualisation: l’aire d’un cercle ne permet pas de saisir la population d’un pays, mais de comparer grossièrement la population d’un pays par rapport aux autres. 4.11.3 Publiez vos données Vous avez peut-être déjà feuilleté un article et voulu avoir accès aux données incluses dans un graphique. Il existe des outils pour digitaliser des graphiques pour en extraire les données. Mais le processus est fastidieux, long, souvent peu précis. De plus en plus, les chercheurs sont encouragés à publier leurs données et leurs calculs. Matplotlib et Seaborn sont des outils graphiques classiques qui devraient être accompagnés des données et calculs ayant servi à les générer. Mais ce n’est pas idéal non plus. En revanche, les outils graphiques modernes comme Plotly et Altair peuvent être exportés en code javascipt, qui contient toutes les informations sur les données et la manière de les représenter graphiquement. Ce chapitre a pour objectif de vous familiariser avec les outils de base les plus communément utilisés en calcul scientifique avec Python, mais je vous encourage à explorer la nouvelle génération d’outils graphiques. 4.11.4 Visitez www.junkcharts.typepad.com de temps à autre Le statisticien et blogueur Kaiser Fung s’affaire quotidiennement à proposer des améliorations à de mauvais graphiques sur son blogue Junk Charts. "],
["chapitre-biostats.html", "5 Biostatistiques 5.1 Populations et échantillons 5.2 Les variables 5.3 Les probabilités 5.4 Les distributions 5.5 Statistiques descriptives 5.6 Tests d’hypothèses à un et deux échantillons 5.7 L’analyse de variance 5.8 Les modèles statistiques", " 5 Biostatistiques ️ Objectifs spécifiques: À la fin de ce chapitre, vous serez en mesure de définir les concepts de base en statistique: population, échantillon, variable, probabilité et distribution serez en mesure de calculer des statistiques descriptives de base: moyenne et écart-type, quartiles, maximum et minimum comprendrez les notions de test d’hypothèse, d’effet et de p-value, ainsi qu’éviter les erreurs communes dans leur interprétation saurez effectuer une modélisation statistique linéaire simple, multiple et mixte, entre autre sur des catégories saurez effectuer une modélisation statistique non linéaire simple, multiple et mixte Aux chapitres précédents, nous avons vu comment visualiser, organiser et manipuler des tableaux de données. La statistique est une collection de disciplines liées à la collecte, l’organisation, l’analyse, l’interprétation et la présentation de données. Les biostatistiques est l’application de ces disciplines à la biosphère. Dans Principles and procedures of statistics: A biometrical approach, Steel, Torie et Dickey (1997) définissent les statistiques ainsi: Les statistiques forment la science, pure et appliquée, de la création, du développement, et de l’application de techniques par lesquelles l’incertitude de l’induction inférentielle peut être évaluée. (ma traduction) Alors que l’inférence consiste à généraliser des observations sur des échantillons à l’ensemble d’une population, l’induction est un type de raisonnement qui permet de généraliser des observations en théories. Les statistiques permettent d’évaluer l’incertitude découlant du processus qui permet d’abord de passer de l’échantillon à la population représentée par cet échantillon, puis de passer de cette représentation d’une population en lois générales la concernant. La définition de Whitlock et Schuluter (2015), dans The Analysis of Biological Data, est plus simple, insistant sur l’inférence: La statistique est l’étude des méthodes pour mesurer des aspects de populations à partir d’échantillons et pour quantifier l’incertitude des mesures. (ma traduction) Les statistiques consistent à faire du sens (anglicisme assumé) avec des observations dans l’objectif de répondre à une question que vous aurez formulée clairement, préalablement à votre expérience. The more time I spend as The Statistician in the room, the more I think the best skill you can cultivate is the ability to remain calm and repeatedly ask “What question are you trying to answer?” — Bryan Howie (@bryan_howie) 13 décembre 2018 Le flux de travail conventionnel consiste à collecter des échantillons, transformer les données, effectuer des tests, analyser les résultats, les interpréter et les visualiser. Bien que ces tâches soient complexes, en particulier en ce qui a trait aux tests statistiques, la plupart des opérations statistiques peuvent être effectuées sans l’assistance de statisticien.ne.s… à condition de comprendre suffisamment les concepts utilisés. Ce chapitre à lui seul est trop court pour permettre d’intégrer toutes les connaissances nécessaires à une utilisation raisonnée des statistiques, mais fourni les bases pour aller plus loin. Notez que les erreurs d’interprétation statistiques sont courantes et la consultation de spécialistes n’est souvent pas un luxe. Dans ce chapitre, nous verrons comment répondre correctement à une question valide et adéquate avec l’aide d’outils de calcul scientifique. Nous couvrirons les notions de bases des distributions et des variables aléatoires qui nous permettront d’effectuer des tests statistiques communs avec R. Nous couvrirons aussi les erreurs communément commises en recherche académique et les moyens simples de les éviter. Ce chapitre est une introduction aux statistiques avec R, et ne remplacera pas un bon cours de stats. En plus des modules de base de R nous utiliserons les modules de la tidyverse, le module de données agricoles agridat, ainsi que le module nlme spécialisé pour la modélisation mixte. Avant de survoler les applications statistiques avec R, je vais d’abord et rapidement présenter quelques notions importantes en statistiques : populations et échantillons, variables, probabilités et distributions. Nous allons effectuer des tests d’hypothèse univariés (notamment les tests de t et les analyses de variance) et détailler la notion de p-value. Mais avant tout, je vais m’attarder plus longuement aux modèles linéaires généralisés, incluant en particulier des effets fixes et aléatoires (modèles mixtes), qui fournissent une trousse d’analyse polyvalente en analyse multivariée. Je terminerai avec les perspectives multivariées que sont les matrices de covariance et de corrélation. 5.1 Populations et échantillons Le principe d’inférence consiste à généraliser des conclusions à l’échelle d’une population à partir d’échantillons issus de cette population. Alors qu’une population contient tous les éléments étudiés, un échantillon d’une population est une observation unique. Une expérience bien conçue fera en sorte que les échantillons sont représentatifs de la population qui, la plupart du temps, ne peut être observée entièrement pour des raisons pratiques. Les principes d’expérimentation servant de base à la conception d’une bonne méthodologie sont présentés dans le cours Dispositifs expérimentaux (BVG-7002). Également, je recommande le livre Principes d’expérimentation: planification des expériences et analyse de leurs résultats de Pierre Dagnelie (2012), disponible en ligne en format PDF. Un bon aperçu des dispositifs expérimentaux est aussi présenté dans Introductory Statistics with R, de Peter Dalgaard (2008). Une population est échantillonnée pour induire des paramètres: un rendement typique dans des conditions météorologiques, édaphiques et managériales données, la masse typique des faucons pèlerins, mâles et femelles, le microbiome typique d’un sol agricole ou forestier, etc. Une statistique est une estimation d’un paramètre calculée à partir des données, par exemple une moyenne et un écart-type. Par exemple, la moyenne (\\(\\mu\\)) et l’écart-type (\\(\\sigma\\)) d’une population sont estimés par les moyennes (\\(\\bar{x}\\)) et écarts-types (\\(s\\)) calculés sur les données issues de l’échantillonnage. Chaque paramètre est liée à une perspective que l’on désire connaître chez une population. Ces angles d’observations sont les variables. 5.2 Les variables Nous avons abordé au chapitre 4 la notion de variable par l’intermédiaire d’une donnée. Une variable est l’observation d’une caractéristique décrivant un échantillon et qui est susceptible de varier d’un échantillon à un autre. Si les observations varient en effet d’un échantillon à un autre, on parlera de variable aléatoire. Même le hasard est régi par certaines lois: ce qui est aléatoire dans une variable peut être décrit par des lois de probabilité, que nous verrons plus bas. Mais restons aux variables pour l’instant. Par convention, on peut attribuer aux variables un symbole mathématique. Par exemple, on peut donner à la masse volumique d’un sol (qui est le résultat d’une méthodologie précise) le symbole \\(\\rho\\). Lorsque l’on attribue une valeur à \\(\\rho\\), on parle d’une donnée. Chaque donnée d’une observation a un indice qui lui est propre, que l’on désigne souvent par \\(i\\), que l’on place en indice \\(\\rho_i\\). Pour la première donnée, on a \\(i=1\\), donc \\(\\rho_1\\). Pour un nombre \\(n\\) d’échantillons, on aura \\(\\rho_1\\), \\(\\rho_2\\), \\(\\rho_3\\), …, \\(\\rho_n\\), formant le vecteur \\(\\rho = \\left[\\rho_1, \\rho_2, \\rho_3, ..., \\rho_n \\right]\\). En R, une variable est associée à un vecteur ou une colonne d’un tableau. rho &lt;- c(1.34, 1.52, 1.26, 1.43, 1.39) # matrice 1D data &lt;- data.frame(rho = rho) # tableau data ## rho ## 1 1.34 ## 2 1.52 ## 3 1.26 ## 4 1.43 ## 5 1.39 Il existe plusieurs types de variables, qui se regroupe en deux grandes catégories: les variables quantitatives et les variables qualitatives. 5.2.1 Variables quantitatives Ces variables peuvent être continues dans un espace échantillonnal réel ou discrètes dans un espace échantillonnal ne considérant que des valeurs fixes. Notons que la notion de nombre réel est toujours une approximation en sciences expérimentales comme en calcul numérique, étant donnée que l’on est limité par la précision des appareils comme par le nombre d’octets à utiliser. Bien que les valeurs fixes des distributions discrètes ne soient pas toujours des valeurs entières, c’est bien souvent le cas en biostatistiques comme en démographie, où les décomptes d’individus sont souvent présents (et où la notion de fraction d’individus n’est pas acceptée). 5.2.2 Variables qualitatives On exprime parfois qu’une variable qualitative est une variable impossible à mesurer numériquement: une couleur, l’appartenance à espèce ou à une série de sol. Pourtant, dans bien des cas, les variables qualitatives peut être encodées en variables quantitatives. Par exemple, on peut accoler des pourcentages de sable, limon et argile à un loam sableux, qui autrement est décrit par la classe texturale d’un sol. Pour une couleur, on peut lui associer des pourcentages de rouge, vert et bleu, ainsi qu’un ton. En ce qui a trait aux variables ordonnées, il est possible de supposer un étalement. Par exemple, une variable d’intensité faible-moyenne-forte peut être transformée linéairement en valeurs quantitatives -1, 0 et 1. Attention toutefois, l’étalement peut parfois être quadratique ou logarithmique. Les séries de sol peuvent être encodées par la proportion de gleyfication (Parent et al., 2017). Quant aux catégories difficilement transformables en quantités, on pourra passer par l’encodage catégoriel, souvent appelé dummyfication, qui nous verrons plus loin. 5.3 Les probabilités « Nous sommes si éloignés de connaître tous les agens de la nature, et leurs divers modes d’action ; qu’il ne serait pas philosophique de nier les phénomènes, uniquement parce qu’ils sont inexplicables dans l’état actuel de nos connaissances. Seulement, nous devons les examiner avec une attention d’autant plus scrupuleuse, qu’il paraît plus difficile de les admettre ; et c’est ici que le calcul des probabilités devient indispensable, pour déterminer jusqu’à quel point il faut multiplier les observations ou les expériences, afin d’obtenir en faveur des agens qu’elles indiquent, une probabilité supérieure aux raisons que l’on peut avoir d’ailleurs, de ne pas les admettre. » — Pierre-Simon de Laplace Une probabilité est la vraisemblance qu’un évènement se réalise chez un échantillon. Les probabilités forment le cadre des systèmes stochastiques, c’est-à-dire des systèmes trop complexes pour en connaître exactement les aboutissants, auxquels on attribue une part de hasard. Ces systèmes sont prédominants dans les processus vivants. On peut dégager deux perspectives sur les probabilités: l’une passe par une interprétation fréquentielle, l’autre bayésienne. L’interprétation fréquentielle représente la fréquence des occurrences après un nombre infini d’évènements. Par exemple, si vous jouez à pile ou face un grand nombre de fois, le nombre de pile sera égal à la moitié du nombre de lancés. Il s’agit de l’interprétation communément utilisée. L’interprétation bayésienne vise à quantifier l’incertitude des phénomènes. Dans cette perspective, plus l’information s’accumule, plus l’incertitude diminue. Cette approche gagne en notoriété notamment parce qu’elle permet de décrire des phénomènes qui, intrinsèquement, ne peuvent être répétés infiniment (absence d’asymptote), comme celles qui sont bien définis dans le temps ou sur des populations limités. L’approche fréquentielle teste si les données concordent avec un modèle du réel, tandis que l’approche bayésienne évalue la probabilité que le modèle soit réel. Une erreur courante consiste à aborder des statistiques fréquentielles comme des statistiques bayésiennes. Par exemple, si l’on désire évaluer la probabilité de l’existence de vie sur Mars, on devra passer par le bayésien, car avec les stats fréquentielles, l’on devra plutôt conclure si les données sont conformes ou non avec l’hypothèse de la vie sur Mars (exemple tirée du blogue Dynamic Ecology). Des rivalités factices s’installent enter les tenants des différentes approches, dont chacune, en réalité, répond à des questions différentes dont il convient réfléchir sur les limitations. Bien que les statistiques bayésiennes soient de plus en plus utilisées, nous ne couvrirons dans ce chapitre que l’approche fréquentielle. L’approche bayésienne est néanmoins traitée dans le chapitre 6. 5.4 Les distributions Une variable aléatoire peut prendre des valeurs selon des modèles de distribution des probabilités. Une distribution est une fonction mathématique décrivant la probabilité d’observer une série d’évènements. Ces évènements peuvent être des valeurs continues, des nombres entiers, des catégories, des valeurs booléennes (Vrai/Faux), etc. Dépendemment du type de valeur et des observations obtenues, on peut associer des variables à différentes lois de probabilité. Toujours, l’aire sous la courbe d’une distribution de probabilité est égale à 1. En statistiques inférentielles, les distributions sont les modèles, comprenant certains paramètres comme la moyenne et la variance pour les distributions normales, à partir desquelles les données sont générées. Il existe deux grandes familles de distribution: discrètes et continues. Les distributions discrètes sont contraintes à des valeurs prédéfinies (finies ou infinies), alors que les distributions continues prennent nécessairement un nombre infini de valeur, dont la probabilité ne peut pas être évaluée ponctuellement, mais sur un intervalle. L’espérance mathématique est une fonction de tendance centrale, souvent décrite par un paramètre. Il s’agit de la moyenne d’une population pour une distribution normale. La variance, quant à elle, décrit la variabilité d’une population, i.e. son étalement autour de l’espérance. Pour une distribution normale, la variance d’une population est aussi appelée variance, souvent présentée par l’écart-type. 5.4.1 Distribution binomiale En tant que scénario à deux issues possibles, des tirages à pile ou face suivent une loi binomiale, comme toute variable booléenne prenant une valeur vraie ou fausse. En biostatistiques, les cas communs sont la présence/absence d’une espèce, d’une maladie, d’un trait phylogénétique, ainsi que les catégories encodées. Lorsque l’opération ne comprend qu’un seul échantillon (i.e. un seul tirage à pile ou face), il s’agit d’un cas particulier d’une loi binomiale que l’on nomme une loi de Bernouilli. Pour 25 tirages à pile ou face indépendants (i.e. dont l’ordre des tirages ne compte pas), on peut dessiner une courbe de distribution dont la somme des probabilités est de 1. La fonction dbinom est une fonction de distribution de probabilités. Les fonctions de distribution de probabilités discrètes sont appelées des fonctions de masse. library(&quot;tidyverse&quot;) x &lt;- 0:25 y &lt;- dbinom(x = x, size = 25, prob = 0.5) print(paste(&#39;La somme des probabilités est de&#39;, sum(y))) ## [1] &quot;La somme des probabilités est de 1&quot; ggplot(data = tibble(x, y), mapping = aes(x, y)) + geom_segment(aes(x = x, xend = x, y = 0, yend = y), color = &quot;grey50&quot;) + geom_point() 5.4.2 Distribution de Poisson La loi de Poisson (avec un P majuscule, introduite par le mathématicien français Siméon Denis Poisson et non pas l’animal) décrit des distributions discrètes de probabilité d’un nombre d’évènements se produisant dans l’espace ou dans le temps. Les distributions de Poisson décrive ce qui tient du décompte. Il peut s’agir du nombre de grenouilles traversant une rue quotidiennement, du nombre de plants d’asclépiades se trouvant sur une terre cultivée, ou du nombre d’évènements de précipitation au mois de juin, etc. La distribution de Poisson n’a qu’un seul paramètre, \\(\\lambda\\), qui décrit tant la moyenne des décomptes. Par exemple, en un mois de 30 jours, et une moyenne de 8 évènements de précipitation pour ce mois, on obtient la distribution suivante. x &lt;- 1:30 y &lt;- dpois(x, lambda = 8) print(paste(&#39;La somme des probabilités est de&#39;, sum(y))) ## [1] &quot;La somme des probabilités est de 0.999664536835124&quot; ggplot(data = data.frame(x, y), mapping = aes(x, y)) + geom_segment(aes(x = x, xend = x, y = 0, yend = y), color = &quot;grey50&quot;) + geom_point() 5.4.3 Distribution uniforme La distribution la plus simple est probablement la distribution uniforme. Si la variable est discrète, chaque catégorie est associée à une probabilité égale. Si la variable est continue, la probabilité est directement proportionnelle à la largeur de l’intervalle. On utilise rarement la distribution uniforme en biostatistiques, sinon pour décrire des a priori vagues pour l’analyse bayésienne (ce sujet est traité dans le chapitre 6). Nous utilisons la fonction dunif. À la différence des distributions discrètes, les fonctions de distribution de probabilités continues sont appelées des fonctions de densité d’une loi de probabilité (probability density function). increment &lt;- 0.01 x &lt;- seq(-4, 4, by = increment) y1 &lt;- dunif(x, min = -3, max = 3) y2 &lt;- dunif(x, min = -2, max = 2) y3 &lt;- dunif(x, min = -1, max = 1) print(paste(&#39;La somme des probabilités est de&#39;, sum(y3 * increment))) ## [1] &quot;La somme des probabilités est de 1.005&quot; gg_unif &lt;- data.frame(x, y1, y2, y3) %&gt;% gather(variable, value, -x) ggplot(data = gg_unif, mapping = aes(x = x, y = value)) + geom_line(aes(colour = variable)) 5.4.4 Distribution normale La plus répandue de ces lois est probablement la loi normale, parfois nommée loi gaussienne et plus rarement loi laplacienne. Il s’agit de la distribution classique en forme de cloche. La loi normale est décrite par une moyenne, qui désigne la tendance centrale, et une variance, qui désigne l’étalement des probabilités autour de la moyenne. La racine carrée de la variance est l’écart-type. Les distributions de mesures exclusivement positives (comme le poids ou la taille) sont parfois avantageusement approximées par une loi log-normale, qui est une loi normale sur le logarithme des valeurs: la moyenne d’une loi log-normale est la moyenne géométrique. increment &lt;- 0.01 x &lt;- seq(-10, 10, by = increment) y1 &lt;- dnorm(x, mean = 0, sd = 1) y2 &lt;- dnorm(x, mean = 0, sd = 2) y3 &lt;- dnorm(x, mean = 0, sd = 3) print(paste(&#39;La somme des probabilités est de&#39;, sum(y3 * increment))) ## [1] &quot;La somme des probabilités est de 0.999147010743368&quot; gg_norm &lt;- data.frame(x, y1, y2, y3) %&gt;% gather(variable, value, -x) ggplot(data = gg_norm, mapping = aes(x = x, y = value)) + geom_line(aes(colour = variable)) Quelle est la probabilité d’obtenir le nombre 0 chez une observation continue distribuée normalement dont la moyenne est 0 et l’écart-type est de 1? Réponse: 0. La loi normale étant une distribution continue, les probabilités non-nulles ne peuvent être calculés que sur des intervalles. Par exemple, la probabilité de retrouver une valeur dans l’intervalle entre -1 et 2 est calculée en soustrayant la probabilité cumulée à -1 de la probabilité cumulée à 2. increment &lt;- 0.01 x &lt;- seq(-5, 5, by = increment) y &lt;- dnorm(x, mean = 0, sd = 1) prob_between &lt;- c(-1, 2) gg_norm &lt;- data.frame(x, y) gg_auc &lt;- gg_norm %&gt;% filter(x &gt; prob_between[1], x &lt; prob_between[2]) %&gt;% rbind(c(prob_between[2], 0)) %&gt;% rbind(c(prob_between[1], 0)) ggplot(data.frame(x, y), aes(x, y)) + geom_polygon(data = gg_auc, fill = &#39;#71ad50&#39;) + # #71ad50 est un code de couleur format hexadécimal geom_line() prob_norm_between &lt;- pnorm(q = prob_between[2], mean = 0, sd = 1) - pnorm(q = prob_between[1], mean = 0, sd = 1) print(paste(&quot;La probabilité d&#39;obtenir un nombre entre&quot;, prob_between[1], &quot;et&quot;, prob_between[2], &quot;est d&#39;environ&quot;, round(prob_norm_between, 2) * 100, &quot;%&quot;)) ## [1] &quot;La probabilité d&#39;obtenir un nombre entre -1 et 2 est d&#39;environ 82 %&quot; La courbe normale peut être utile pour évaluer la distribution d’une population. Par exemple, on peut calculer les limites de région sur la courbe normale qui contient 95% des valeurs possibles en tranchant 2.5% de part et d’autre de la moyenne. Il s’agit ainsi de l’intervalle de confiance sur la déviation de la distribution. increment &lt;- 0.01 x &lt;- seq(-5, 5, by = increment) y &lt;- dnorm(x, mean = 0, sd = 1) alpha &lt;- 0.05 prob_between &lt;- c(qnorm(p = alpha/2, mean = 0, sd = 1), qnorm(p = 1 - alpha/2, mean = 0, sd = 1)) gg_norm &lt;- data.frame(x, y) gg_auc &lt;- gg_norm %&gt;% filter(x &gt; prob_between[1], x &lt; prob_between[2]) %&gt;% rbind(c(prob_between[2], 0)) %&gt;% rbind(c(prob_between[1], 0)) ggplot(data = data.frame(x, y), mapping = aes(x, y)) + geom_polygon(data = gg_auc, fill = &#39;#71ad50&#39;) + # #71ad50 est un code de couleur format hexadécimal geom_line() + geom_text(data = data.frame(x = prob_between, y = c(0, 0), labels = round(prob_between, 2)), mapping = aes(label = labels)) On pourrait aussi être intéressé à l’intervalle de confiance sur la moyenne. En effet, la moyenne suit aussi une distribution normale, dont la tendance centrale est la moyenne de la distribution, et dont l’écart-type est noté erreur standard. On calcule cette erreur en divisant la variance par le nombre d’observation, ou en divisant l’écart-type par la racine carrée du nombre d’observations. Ainsi, pour 10 échantillons: increment &lt;- 0.01 x &lt;- seq(-5, 5, by = increment) y &lt;- dnorm(x, mean = 0, sd = 1) alpha &lt;- 0.05 prob_between &lt;- c(qnorm(p = alpha/2, mean = 0, sd = 1) / sqrt(10), qnorm(p = 1 - alpha/2, mean = 0, sd = 1) / sqrt(10)) gg_norm &lt;- data.frame(x, y) gg_auc &lt;- gg_norm %&gt;% filter(x &gt; prob_between[1], x &lt; prob_between[2]) %&gt;% rbind(c(prob_between[2], 0)) %&gt;% rbind(c(prob_between[1], 0)) ggplot(data = data.frame(x, y), mapping = aes(x, y)) + geom_polygon(data = gg_auc, fill = &#39;#71ad50&#39;) + # #71ad50 est un code de couleur format hexadécimal geom_line() + geom_text(data = data.frame(x = prob_between, y = c(0, 0), labels = round(prob_between, 2)), mapping = aes(label = labels)) 5.5 Statistiques descriptives On a vu comment générer des statistiques sommaires en R avec la fonction summary(). Reprenons les données d’iris. data(&quot;iris&quot;) summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## Pour précisément effectuer une moyenne et un écart-type sur un vecteur, passons par les fonctions mean() et sd(). mean(iris$Sepal.Length) ## [1] 5.843333 sd(iris$Sepal.Length) ## [1] 0.8280661 Pour effectuer un sommaire de tableau piloté par une fonction, nous passons par la gamme de fonctions summarise(), de dplyr. Dans ce cas, avec group_by(), nous fragmentons le tableau par espèce pour effectuer un sommaire sur toutes les variables. iris %&gt;% group_by(Species) %&gt;% summarise_all(mean) ## # A tibble: 3 x 5 ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5.01 3.43 1.46 0.246 ## 2 versicolor 5.94 2.77 4.26 1.33 ## 3 virginica 6.59 2.97 5.55 2.03 Vous pourriez être intéressé par les quartiles à 25, 50 et 75%. Mais la fonction summarise() n’autorise que les fonctions dont la sortie est d’un seul objet, alors faisons sorte que l’objet soit une liste - lorsque l’on imbrique une fonction funs, le tableau à insérer dans la fonction est indiqué par un .. iris %&gt;% group_by(Species) %&gt;% summarise_all(funs(list(quantile(.)))) ## # A tibble: 3 x 5 ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;fct&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 setosa &lt;dbl [5]&gt; &lt;dbl [5]&gt; &lt;dbl [5]&gt; &lt;dbl [5]&gt; ## 2 versicolor &lt;dbl [5]&gt; &lt;dbl [5]&gt; &lt;dbl [5]&gt; &lt;dbl [5]&gt; ## 3 virginica &lt;dbl [5]&gt; &lt;dbl [5]&gt; &lt;dbl [5]&gt; &lt;dbl [5]&gt; En mode programmation classique de R, on pourra générer les quartiles à la pièce. quantile(iris$Sepal.Length[iris$Species == &#39;setosa&#39;]) ## 0% 25% 50% 75% 100% ## 4.3 4.8 5.0 5.2 5.8 quantile(iris$Sepal.Length[iris$Species == &#39;versicolor&#39;]) ## 0% 25% 50% 75% 100% ## 4.9 5.6 5.9 6.3 7.0 quantile(iris$Sepal.Length[iris$Species == &#39;virginica&#39;]) ## 0% 25% 50% 75% 100% ## 4.900 6.225 6.500 6.900 7.900 La fonction table() permettra d’obtenir des décomptes par catégorie, ici par plages de longueurs de sépales. Pour obtenir les proportions du nombre total, il s’agit d’encapsuler le tableau croisé dans la fonction prop.table(). tableau_croise &lt;- table(iris$Species, cut(iris$Sepal.Length, breaks = quantile(iris$Sepal.Length))) tableau_croise ## ## (4.3,5.1] (5.1,5.8] (5.8,6.4] (6.4,7.9] ## setosa 35 14 0 0 ## versicolor 4 20 17 9 ## virginica 1 5 18 26 prop.table(tableau_croise) ## ## (4.3,5.1] (5.1,5.8] (5.8,6.4] (6.4,7.9] ## setosa 0.234899329 0.093959732 0.000000000 0.000000000 ## versicolor 0.026845638 0.134228188 0.114093960 0.060402685 ## virginica 0.006711409 0.033557047 0.120805369 0.174496644 5.6 Tests d’hypothèses à un et deux échantillons Un test d’hypothèse permet de décider si une hypothèse est confirmée ou rejetée à un seuil de probabilité prédéterminé. Cette section est inspirée du chapitre 5 de Dalgaard, 2008. 5.6.0.1 Information: l’hypothèse nulle Les tests d’hypothèse évalue des effets statistiques (qui ne sont pas nécessairement des effets de causalité). L’effet à évaluer peut être celui d’un traitement, d’indicateurs météorologiques (e.g. précipitations totales, degré-jour, etc.), de techniques de gestion des paysages, etc. Une recherche est menée pour évaluer l’hypothèse que l’on retrouve des différences entre des unités expérimentales. Par convention, l’hypothèse nulle (écrite \\(H_0\\)) est l’hypothèse qu’il n’y ait pas d’effet (c’est l’hypothèse de l’avocat du diable 😈) à l’échelle de la population (et non pas à l’échelle de l’échantillon). À l’inverse, l’hypothèse alternative (écrite \\(H_1\\)) est l’hypothèse qu’il y ait un effet à l’échelle de la population. À titre d’exercice en stats, on débute souvent par en testant si deux vecteurs de valeurs continues proviennent de populations à moyennes différentes ou si un vecteur de valeurs a été généré à partir d’une population ayant une moyenne donner. Dans cette section, nous utiliserons la fonction t.test() pour les tests de t et la fonction wilcox.test() pour les tests de Wilcoxon (aussi appelé de Mann-Whitney). 5.6.1 Test de t à un seul échantillon Nous devons assumer, pour ce test, que l’échantillon est recueillit d’une population dont la distribution est normale, \\(\\mathcal{N} \\sim \\left( \\mu, \\sigma^2 \\right)\\), et que chaque échantillon est indépendant l’un de l’autre. L’hypothèse nulle est souvent celle de l’avocat du diable, que la moyenne soit égale à une valeur donnée (donc la différence entre la moyenne de la population et une moyenne donnée est de zéro): ici, que \\(\\mu = \\bar{x}\\). L’erreur standard sur la moyenne (ESM) de l’échantillon, \\(\\bar{x}\\) est calculée comme suit. \\[ESM = \\frac{s}{\\sqrt{n}}\\] où \\(s\\) est l’écart-type de l’échantillon et \\(n\\) est le nombre d’échantillons. Pour tester l’intervalle de confiance de l’échantillon, on multiplie l’ESM par l’aire sous la courbe de densité couvrant une certaine proportion de part et d’autre de l’échantillon. Pour un niveau de confiance de 95%, on retranche 2.5% de part et d’autre. set.seed(33746) x &lt;- rnorm(20, 16, 4) level &lt;- 0.95 alpha &lt;- 1-level x_bar &lt;- mean(x) s &lt;- sd(x) n &lt;- length(x) error &lt;- qnorm(1 - alpha/2) * s / sqrt(n) error ## [1] 1.483253 intervalle de confiance est l’erreur de par et d’autre de la moyenne. c(x_bar - error, x_bar + error) ## [1] 14.35630 17.32281 Si la moyenne de la population est de 16, un nombre qui se situe dans l’intervalle de confiance on accepte l’hypothèse nulle au seuil 0.05. Si le nombre d’échantillon est réduit (généralement &lt; 30), on passera plutôt par une distribution de t, avec \\(n-1\\) degrés de liberté. error &lt;- qt(1 - alpha/2, n-1) * s / sqrt(n) c(x_bar - error, x_bar + error) ## [1] 14.25561 17.42351 Plus simplement, on pourra utiliser la fonction t.test() en spécifiant la moyenne de la population. Nous avons généré 20 données avec une moyenne de 16 et un écart-type de 4. Nous savons donc que la vraie moyenne de l’échantillon est de 16. Mais disons que nous testons l’hypothèse que ces données sont tirées d’une population dont la moyenne est 18 (et implicitement que sont écart-type est de 4). t.test(x, mu = 18) ## ## One Sample t-test ## ## data: x ## t = -2.8548, df = 19, p-value = 0.01014 ## alternative hypothesis: true mean is not equal to 18 ## 95 percent confidence interval: ## 14.25561 17.42351 ## sample estimates: ## mean of x ## 15.83956 La fonction retourne la valeur de t (t-value), le nombre de degrés de liberté (\\(n-1 = 19\\)), une description de l’hypothèse alternative (alternative hypothesis: true mean is not equal to 18), ainsi que l’intervalle de confiance au niveau de 95%. Le test contient aussi la p-value. Bien que la p-value soit largement utilisée en science 5.6.1.1 Information: la p-value La p-value, ou valeur-p ou p-valeur, est utilisée pour trancher si, oui ou non, un résultat est significatif (en langage scientifique, le mot significatif ne devrait être utilisé que lorsque l’on réfère à un test d’hypothèse statistique). Vous retrouverez des p-value partout en stats. Les p-values indiquent la confiance que l’hypothèse nulle soit vraie, selon les données et le modèle statistique utilisées. La p-value est la probabilité que les données aient été générées pour obtenir un effet équivalent ou plus prononcé si l’hypothèse nulle est vraie. Une p-value élevée indique que le modèle appliqué à vos données concordent avec la conclusion que l’hypothèse nulle est vraie, et inversement si la p-value est faible. Le seuil arbitraire utilisée en écologie et en agriculture, comme dans plusieurs domaines, est 0.05. Les six principes de l’American Statistical Association guident l’interprétation des p-values. [ma traduction] Les p-values indique l’ampleur de l’incompatibilité des données avec le modèle statistique Les p-values ne mesurent pas la probabilité que l’hypothèse étudiée soit vraie, ni la probabilité que les données ont été générées uniquement par la chance. Les conclusions scientifiques et décisions d’affaire ou politiques ne devraient pas être basées sur si une p-value atteint un seuil spécifique. Une inférence appropriée demande un rapport complet et transparent. Une p-value, ou une signification statistique, ne mesure pas l’ampleur d’un effet ou l’importance d’un résultat. En tant que tel, une p-value n’offre pas une bonne mesure des évidences d’un modèle ou d’une hypothèse. Cet encadré est inspiré d’un billet de blogue de Jim Frost et d’un rapport de l’American Statistical Association. Dans le cas précédent, la p-value était de 0.01014. Pour aider notre interprétation, prenons l’hypothèse alternative: true mean is not equal to 18. L’hypothèse nulle était bien que la vraie moyenne est égale à 18. Insérons la p-value dans la définition: la probabilité que les données aient été générées pour obtenir un effet équivalent ou plus prononcé si l’hypothèse nulle est vraie est de 0.01014. Il est donc très peu probable que les données soient tirées d’un échantillon dont la moyenne est de 18. Au seuil de signification de 0.05, on rejette l’hypothèse nulle et l’on conclut qu’à ce seuil de confiance, l’échantillon ne provient pas d’une population ayant une moyenne de 18. 5.6.2 Attention: mauvaises interprétations des p-values “La p-value n’a jamais été conçue comme substitut au raisonnement scientifique” Ron Wasserstein, directeur de l’American Statistical Association [ma traduction]. Un résultat montrant une p-value plus élevée que 0.05 est-il pertinent? Lors d’une conférence, Dr Evil ne présentent que les résultats significatifs de ses essais au seuil de 0.05. Certains essais ne sont pas significatifs, mais bon, ceux-ci ne sont pas importants… En écartant ces résultats, Dr Evil commet 3 erreurs: La p-value n’est pas un bon indicateur de l’importance d’un test statistique. L’importance d’une variable dans un modèle devrait être évaluée par la valeur de son coefficient. Son incertitude devrait être évaluée par sa variance. Une manière d’évaluer plus intuitive la variance est l’écart-type ou l’intervalle de confiance. À un certain seuil d’intervalle de confiance, la p-value traduira la probabilité qu’un coefficient soit réellement nul ait pu générer des données démontrant un coefficient égal ou supérieur. Il est tout aussi important de savoir que le traitement fonctionne que de savoir qu’il ne fonctionne pas. Les résultats démontrant des effets sont malheureusement davantage soumis aux journaux et davantage publiés que ceux ne démontrant pas d’effets (Decullier et al., 2005). Le seuil de 0.05 est arbitraire. 5.6.2.1 Attention au p-hacking Le p-hacking (ou data dredging) consiste à manipuler les données et les modèles pour faire en sorte d’obtenir des p-values favorables à l’hypothèse testée et, éventuellement, aux conclusions recherchées. À éviter dans tous les cas. Toujours. Toujours. Toujours. Vidéo suggérée (en anglais). 5.6.3 Test de Wilcoxon à un seul échantillon Le test de t suppose que la distribution des données est normale… ce qui est rarement le cas, surtout lorsque les échantillons sont peu nombreux. Le test de Wilcoxon ne demande aucune supposition sur la distribution: c’est un test non-paramétrique basé sur le tri des valeurs. wilcox.test(x, mu = 18) ## ## Wilcoxon signed rank test ## ## data: x ## V = 39, p-value = 0.01208 ## alternative hypothesis: true location is not equal to 18 Le V est la somme des rangs positifs. Dans ce cas, la p-value est semblable à celle du test de t, et les mêmes conclusions s’appliquent. 5.6.4 Tests de t à deux échantillons Les tests à un échantillon servent plutôt à s’exercer: rarement en aura-t-on besoin en recherche, où plus souvent, on voudra comparer les moyennes de deux unités expérimentales. L’expérience comprend donc deux séries de données continues, \\(x_1\\) et \\(x_2\\), issus de lois de distribution normale \\(\\mathcal{N} \\left( \\mu_1, \\sigma_1^2 \\right)\\) et \\(\\mathcal{N} \\left( \\mu_2, \\sigma_2^2 \\right)\\), et nous testons l’hypothèse nulle que \\(\\mu_1 = \\mu_2\\). La statistique t est calculée comme suit. \\[t = \\frac{\\bar{x_1} - \\bar{x_2}}{ESDM}\\] L’ESDM est l’erreur standard de la différence des moyennes: \\[ESDM = \\sqrt{ESM_1^2 + ESM_2^2}\\] Si vous supposez que les variances sont identiques, l’erreur standard (s) est calculée pour les échantillons des deux groupes, puis insérée dans le calcul des ESM. La statistique t sera alors évaluée à \\(n_1 + n_2 - 2\\) degrés de liberté. Si vous supposez que la variance est différente (procédure de Welch), vous calculez les ESM avec les erreurs standards respectives, et la statistique t devient une approximation de la distribution de t avec un nombre de degrés de liberté calculé à partir des erreurs standards et du nombre d’échantillon dans les groupes: cette procédure est considérée comme plus prudente (Dalgaard, 2008, page 101). Prenons les données d’iris pour l’exemple en excluant l’iris setosa étant donnée que les tests de t se restreignent à deux groupes. Nous allons tester la longueur des pétales. iris_pl &lt;- iris %&gt;% filter(Species != &quot;setosa&quot;) %&gt;% select(Species, Petal.Length) sample_n(iris_pl, 5) ## Species Petal.Length ## 1 virginica 5.2 ## 2 virginica 5.8 ## 3 virginica 5.1 ## 4 versicolor 4.0 ## 5 versicolor 4.2 Dans la prochaine cellule, nous introduisons l’interface-formule de R, où l’on retrouve typiquement le ~, entre les variables de sortie à gauche et les variables d’entrée à droite. Dans notre cas, la variable de sortie est la variable testée, Petal.Length, qui varie en fonction du groupe Species, qui est la variable d’entrée (variable explicative) - nous verrons les types de variables plus en détails dans la section Les modèles statistiques, plus bas. t.test(formula = Petal.Length ~ Species, data = iris_pl, var.equal = FALSE) ## ## Welch Two Sample t-test ## ## data: Petal.Length by Species ## t = -12.604, df = 95.57, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.49549 -1.08851 ## sample estimates: ## mean in group versicolor mean in group virginica ## 4.260 5.552 Nous obtenons une sortie similaire aux précédentes. L’intervalle de confiance à 95% exclu le zéro, ce qui est cohérent avec la p-value très faible, qui nous indique le rejet de l’hypothèse nulle au seuil 0.05. Les groupes ont donc des moyennes de longueurs de pétale significativement différentes. 5.6.4.1 Enregistrer les résultats d’un test Il est possible d’enregistrer un test dans un objet. tt_pl &lt;- t.test(formula = Petal.Length ~ Species, data = iris_pl, var.equal = FALSE) summary(tt_pl) ## Length Class Mode ## statistic 1 -none- numeric ## parameter 1 -none- numeric ## p.value 1 -none- numeric ## conf.int 2 -none- numeric ## estimate 2 -none- numeric ## null.value 1 -none- numeric ## alternative 1 -none- character ## method 1 -none- character ## data.name 1 -none- character str(tt_pl) ## List of 9 ## $ statistic : Named num -12.6 ## ..- attr(*, &quot;names&quot;)= chr &quot;t&quot; ## $ parameter : Named num 95.6 ## ..- attr(*, &quot;names&quot;)= chr &quot;df&quot; ## $ p.value : num 4.9e-22 ## $ conf.int : num [1:2] -1.5 -1.09 ## ..- attr(*, &quot;conf.level&quot;)= num 0.95 ## $ estimate : Named num [1:2] 4.26 5.55 ## ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;mean in group versicolor&quot; &quot;mean in group virginica&quot; ## $ null.value : Named num 0 ## ..- attr(*, &quot;names&quot;)= chr &quot;difference in means&quot; ## $ alternative: chr &quot;two.sided&quot; ## $ method : chr &quot;Welch Two Sample t-test&quot; ## $ data.name : chr &quot;Petal.Length by Species&quot; ## - attr(*, &quot;class&quot;)= chr &quot;htest&quot; 5.6.5 Comparaison des variances Pour comparer les variances, on a recours au test de F (F pour Fisher). var.test(formula = Petal.Length ~ Species, data = iris_pl) ## ## F test to compare two variances ## ## data: Petal.Length by Species ## F = 0.72497, num df = 49, denom df = 49, p-value = 0.2637 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.411402 1.277530 ## sample estimates: ## ratio of variances ## 0.7249678 Il semble que l’on pourrait relancer le test de t sans la procédure Welch, avec var.equal = TRUE. 5.6.6 Tests de Wilcoxon à deux échantillons Cela ressemble au test de t! wilcox.test(formula = Petal.Length ~ Species, data = iris_pl, var.equal = TRUE) ## ## Wilcoxon rank sum test with continuity correction ## ## data: Petal.Length by Species ## W = 44.5, p-value &lt; 2.2e-16 ## alternative hypothesis: true location shift is not equal to 0 5.6.7 Les tests pairés Les tests pairés sont utilisés lorsque deux échantillons proviennent d’une même unité expérimentale: il s’agit en fait de tests sur la différence entre deux observations. set.seed(2555) n &lt;- 20 avant &lt;- rnorm(n, 16, 4) apres &lt;- rnorm(n, 18, 3) Il est important de spécifier que le test est pairé, la valeur par défaut de paired étant FALSE. t.test(avant, apres, paired = TRUE) ## ## Paired t-test ## ## data: avant and apres ## t = -1.5168, df = 19, p-value = 0.1458 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -4.5804586 0.7311427 ## sample estimates: ## mean of the differences ## -1.924658 L’hypothèse nulle qu’il n’y ait pas de différence entre l’avant et l’après traitement est acceptée au seuil 0.05. Exercice. Effectuer un test de Wilcoxon pairé. 5.7 L’analyse de variance L’analyse de variance consiste à comparer des moyennes de plusieurs groupe distribués normalement et de même variance. Cette section sera élaborée prochainement plus en profondeur. Considérons-la pour le moment comme une régression sur une variable catégorielle. pl_aov &lt;- aov(Petal.Length ~ Species, iris) summary(pl_aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Species 2 437.1 218.55 1180 &lt;2e-16 *** ## Residuals 147 27.2 0.19 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 La prochaine section, justement, est vouée aux modèles statistiques explicatifs, qui incluent la régression. 5.8 Les modèles statistiques La modélisation statistique consiste à lier de manière explicite des variables de sortie \\(y\\) (ou variables-réponse ou variables dépendantes) à des variables explicatives \\(x\\) (ou variables prédictives / indépendantes / covariables). Les variables-réponse sont modélisées par une fonction des variables explicatives ou prédictives. Pourquoi garder les termes explicatives et prédictives? Parce que les modèles statistiques (basés sur des données et non pas sur des mécanismes) sont de deux ordres. D’abord, les modèles prédictifs sont conçus pour prédire de manière fiable une ou plusieurs variables-réponse à partir des informations contenues dans les variables qui sont, dans ce cas, prédictives. Ces modèles sont couverts dans le chapitre 11 de ce manuel (en développement). Lorsque l’on désire tester des hypothèses pour évaluer quelles variables expliquent la réponse, on parlera de modélisation (et de variables) explicatives. En inférence statistique, on évaluera les corrélations entre les variables explicatives et les variables-réponse. Un lien de corrélation n’est pas un lien de causalité. L’inférence causale peut en revanche être évaluée par des modèles d’équations structurelles, sujet qui fera éventuellement partie de ce cours. Cette section couvre la modélisation explicative. Les variables qui contribuent à créer les modèles peuvent être de différentes natures et distribuées selon différentes lois de probabilité. Alors que les modèles linéaires simples (lm) impliquent une variable-réponse distribuée de manière continue, les modèles linéaires généralisés peuvent aussi expliquer des variables de sorties discrètes. Dans les deux cas, on distinguera les variables fixes et les variables aléatoires. Les variables fixes sont les variables testées lors de l’expérience: dose du traitement, espèce/cultivar, météo, etc. Les variables aléatoires sont les sources de variation qui génèrent du bruit dans le modèle: les unités expérimentales ou le temps lors de mesures répétées. Les modèles incluant des effets fixes seulement sont des modèles à effets fixes. Généralement, les modèles incluant des variables aléatoires incluent aussi des variables fixes: on parlera alors de modèles mixtes. Nous couvrirons ces deux types de modèle. 5.8.1 Modèles à effets fixes Les tests de t et de Wilcoxon, explorés précédemment, sont des modèles statistiques à une seule variable. Nous avons vu dans l’interface-formule qu’une variable-réponse peut être liée à une variable explicative avec le tilde ~. En particulier, le test de t est régression linéaire univariée (à une seule variable explicative) dont la variable explicative comprend deux catégories. De même, l’anova est une régression linéaire univariée dont la variable explicative comprend plusieurs catégories. Or l’interface-formule peut être utilisé dans plusieurs circonstances, notamment pour ajouter plusieurs variables de différents types: on parlera de régression multivariée. La plupart des modèles statistiques peuvent être approximés comme une combinaison linéaire de variables: ce sont des modèles linéaires. Les modèles non-linéaires impliquent des stratégies computationnelles complexes qui rendent leur utilisation plus difficile à manœuvrer. Un modèle linéaire univarié prendra la forme \\(y = \\beta_0 + \\beta_1 x + \\epsilon\\), où \\(\\beta_0\\) est l’intercept et \\(\\beta_1\\) est la pente et \\(\\epsilon\\) est l’erreur. Vous verrez parfois la notation \\(\\hat{y} = \\beta_0 + \\beta_1 x\\). La notation avec le chapeau \\(\\hat{y}\\) exprime qu’il s’agit des valeurs générées par le modèle. En fait, \\(y = \\hat{y} - \\epsilon\\). 5.8.1.1 Modèle linéaire univarié avec variable continue Prenons les données lasrosas.corn incluses dans le module agridat, où l’on retrouve le rendement d’une production de maïs à dose d’azote variable, en Argentine. library(&quot;agridat&quot;) data(&quot;lasrosas.corn&quot;) sample_n(lasrosas.corn, 10) ## year lat long yield nitro topo bv rep nf ## 1 2001 -33.05109 -63.84451 94.15 50.6 E 173.54 R1 N2 ## 2 2001 -33.04908 -63.84830 90.19 99.8 W 91.74 R3 N4 ## 3 1999 -33.05165 -63.84257 73.60 0.0 LO 173.23 R2 N0 ## 4 2001 -33.05175 -63.84320 108.95 75.4 LO 163.38 R1 N3 ## 5 2001 -33.05111 -63.84444 101.67 50.6 E 174.00 R1 N2 ## 6 2001 -33.05139 -63.84244 98.74 50.6 LO 167.86 R2 N2 ## 7 1999 -33.05121 -63.84224 81.70 29.0 LO 166.59 R3 N1 ## 8 2001 -33.05088 -63.84491 50.59 39.0 E 179.56 R1 N1 ## 9 1999 -33.05103 -63.84404 61.44 0.0 E 166.84 R3 N0 ## 10 2001 -33.05052 -63.84613 46.06 50.6 HT 178.54 R1 N2 Ces données comprennent plusieurs variables. Prenons le rendement (yield) comme variable de sortie et, pour le moment, ne retenons que la dose d’azote (nitro) comme variable explicative: il s’agit d’une régression univariée. Les deux variables sont continues. Explorons d’abord le nuage de points de l’une et l’autre. ggplot(data = lasrosas.corn, mapping = aes(x = nitro, y = yield)) + geom_point() L’hypothèse nulle est que la dose d’azote n’affecte pas le rendement, c’est à dire que le coefficient de pente et nul. Une autre hypothèse est que l’intercept est nul: donc qu’à dose de 0, rendement de 0. Un modèle linéaire à variable de sortie continue est créé avec la fonction lm(), pour linear model. modlin_1 &lt;- lm(yield ~ nitro, data = lasrosas.corn) summary(modlin_1) ## ## Call: ## lm(formula = yield ~ nitro, data = lasrosas.corn) ## ## Residuals: ## Min 1Q Median 3Q Max ## -53.183 -15.341 -3.079 13.725 45.897 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 65.843213 0.608573 108.193 &lt; 2e-16 *** ## nitro 0.061717 0.007868 7.845 5.75e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 19.66 on 3441 degrees of freedom ## Multiple R-squared: 0.01757, Adjusted R-squared: 0.01728 ## F-statistic: 61.54 on 1 and 3441 DF, p-value: 5.754e-15 Le diagnostic du modèle comprend plusieurs informations. D’abord la formule utilisée, affichée pour la traçabilité. Viens ensuite un aperçu de la distribution des résidus. La médiane devrait s’approcher de la moyenne des résidus (qui est toujours de 0). Bien que le -3.079 peut sembler important, il faut prendre en considération de l’échelle de y, et ce -3.079 est exprimé en terme de rendement, ici en quintaux (i.e. 100 kg) par hectare. La distribution des résidus mérite d’être davantage investiguée. Nous verrons cela un peu plus tard. Les coefficients apparaissent ensuite. Les estimés sont les valeurs des effets. R fournit aussi l’erreur standard associée, la valeur de t ainsi que la p-value (la probabilité d’obtenir cet effet ou un effet plus extrême si en réalité il y avait absence d’effet). L’intercept est bien sûr plus élevé que 0 (à dose nulle, on obtient 65.8 quintaux par hectare en moyenne). La pente de la variable nitro est de ~0.06: pour chaque augmentation d’un kg/ha de dose, on a obtenu ~0.06 quintaux/ha de plus de maïs. Donc pour 100 kg/ha de N, on a obtenu un rendement moyen de 6 quintaux de plus que l’intercept. Soulignons que l’ampleur du coefficient est très important pour guider la fertilisation: ne rapporter que la p-value, ou ne rapporter que le fait qu’elle est inférieure à 0.05 (ce qui arrive souvent dans la littérature), serait très insuffisant pour l’interprétation des statistiques. La p-value nous indique néanmoins qu’il serait très improbable qu’une telle pente ait été générée alors que celle-ci est nulle en réalité. Les étoiles à côté des p-values indiquent l’ampleur selon l’échelle Signif. codes indiquée en-dessous du tableau des coefficients. Sous ce tableau, R offre d’autres statistiques. En outre, les R² et R² ajustés indiquent si la régression passe effectivement par les points. Le R² prend un maximum de 1 lorsque la droite passe exactement sur les points. Enfin, le test de F génère une p-value indiquant la probabilité que les coefficients de pente ait été générés si les vrais coefficients étaient nuls. Dans le cas d’une régression univariée, cela répète l’information sur l’unique coefficient. On pourra également obtenir les intervalles de confiance avec la fonction confint(). confint(modlin_1, level = 0.95) ## 2.5 % 97.5 % ## (Intercept) 64.65001137 67.03641474 ## nitro 0.04629164 0.07714271 Ou soutirer l’information de différentes manières, comme avec la fonction coefficients(). coefficients(modlin_1) ## (Intercept) nitro ## 65.84321305 0.06171718 Également, on pourra exécuter le modèle sur les données qui ont servi à le générer: predict(modlin_1)[1:5] ## 1 2 3 4 5 ## 73.95902 73.95902 73.95902 73.95902 73.95902 Ou sur des données externes. nouvelles_donnees &lt;- data.frame(nitro = seq(from = 0, to = 100, by = 5)) predict(modlin_1, newdata = nouvelles_donnees)[1:5] ## 1 2 3 4 5 ## 65.84321 66.15180 66.46038 66.76897 67.07756 5.8.1.2 Analyse des résidus Les résidus sont les erreurs du modèle. C’est le vecteur \\(\\epsilon\\), qui est un décalage entre les données et le modèle. Le R² est un indicateur de l’ampleur du décalage, mais une régression linéaire explicative en bonne et due forme devrait être accompagnée d’une analyse des résidus. On peut les calculer par \\(\\epsilon = y - \\hat{y}\\), mais aussi bien utiliser la fonction residuals(). res_df &lt;- data.frame(nitro = lasrosas.corn$nitro, residus_lm = residuals(modlin_1), residus_calcul = lasrosas.corn$yield - predict(modlin_1)) sample_n(res_df, 10) ## nitro residus_lm residus_calcul ## 1 66.0 -18.5165468 -18.5165468 ## 2 66.0 -7.5965468 -7.5965468 ## 3 53.0 -2.6342234 -2.6342234 ## 4 39.0 26.5998170 26.5998170 ## 5 99.8 -20.7825873 -20.7825873 ## 6 0.0 -0.1132131 -0.1132131 ## 7 0.0 -16.9032131 -16.9032131 ## 8 131.5 2.6109781 2.6109781 ## 9 39.0 36.5598170 36.5598170 ## 10 66.0 -20.2665468 -20.2665468 Dans une bonne régression linéaire, on ne retrouvera pas de structure identifiable dans les résidus, c’est-à-dire que les résidus sont bien distribués de part et d’autre du modèle de régression. ggplot(res_df, aes(x = nitro, y = residus_lm)) + geom_point() + labs(x = &quot;Dose N&quot;, y = &quot;Résidus&quot;) + geom_hline(yintercept = 0, col = &quot;red&quot;, size = 1) Bien que le jugement soit subjectif, on peut dire avec confiance qu’il n’y a pas structure particulière. En revanche, on pourrait générer un \\(y\\) qui varie de manière quadratique avec \\(x\\), un modèle linéaire montrera une structure évidente. set.seed(36164) x &lt;- 0:100 y &lt;- 10 + x*1 + x^2 * 0.05 + rnorm(length(x), 0, 50) modlin_2 &lt;- lm(y ~ x) ggplot(data.frame(x, residus = residuals(modlin_2)), aes(x = x, y = residus)) + geom_point() + labs(x = &quot;x&quot;, y = &quot;Résidus&quot;) + geom_hline(yintercept = 0, col = &quot;red&quot;, size = 1) De même, les résidus ne devraient pas croître avec \\(x\\). set.seed(3984) x &lt;- 0:100 y &lt;- 10 + x + x * rnorm(length(x), 0, 2) modlin_3 &lt;- lm(y ~ x) ggplot(data.frame(x, residus = residuals(modlin_3)), aes(x = x, y = residus)) + geom_point() + labs(x = &quot;x&quot;, y = &quot;Résidus&quot;) + geom_hline(yintercept = 0, col = &quot;red&quot;, size = 1) On pourra aussi inspecter les résidus avec un graphique de leur distribution. Reprenons notre modèle de rendement du maïs. ggplot(res_df, aes(x = residus_lm)) + geom_histogram(binwidth = 2, color = &quot;white&quot;) + labs(x = &quot;Residual&quot;) L’histogramme devrait présenter une distribution normale. Les tests de normalité comme le test de Shapiro-Wilk peuvent aider, mais ils sont généralement très sévères. shapiro.test(res_df$residus_lm) ## ## Shapiro-Wilk normality test ## ## data: res_df$residus_lm ## W = 0.94868, p-value &lt; 2.2e-16 L’hypothèse nulle que la distribution est normale est rejetée au seuil 0.05. Dans notre cas, il est évident que la sévérité du test n’est pas en cause, car les résidus semble générer trois ensembles. Ceci indique que les variables explicatives sont insuffisantes pour expliquer la variabilité de la variable-réponse. 5.8.1.3 Régression multiple Comme c’est le cas pour bien des phénomènes en écologie, le rendement d’une culture n’est certainement pas expliqué seulement par la dose d’azote. Lorsque l’on combine plusieurs variables explicatives, on crée un modèle de régression multivariée, ou une régression multiple. Bien que les tendances puissent sembler non-linéaires, l’ajout de variables et le calcul des coefficients associés reste un problème d’algèbre linéaire. On pourra en effet généraliser les modèles linéaires, univariés et multivariés, de la manière suivante. \\[ y = X \\beta + \\epsilon \\] où: \\(X\\) est la matrice du modèle à \\(n\\) observations et \\(p\\) variables. \\[ X = \\left( \\begin{matrix} 1 &amp; x_{11} &amp; \\cdots &amp; x_{1p} \\\\ 1 &amp; x_{21} &amp; \\cdots &amp; x_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; \\cdots &amp; x_{np} \\end{matrix} \\right) \\] \\(\\beta\\) est la matrice des \\(p\\) coefficients, \\(\\beta_0\\) étant l’intercept qui multiplie la première colonne de la matrice \\(X\\). \\[ \\beta = \\left( \\begin{matrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{matrix} \\right) \\] \\(\\epsilon\\) est l’erreur de chaque observation. \\[ \\epsilon = \\left( \\begin{matrix} \\epsilon_0 \\\\ \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n \\end{matrix} \\right) \\] 5.8.1.4 Modèles linéaires univariés avec variable catégorielle nominale Une variable catégorielle nominale (non ordonnée) utilisée à elle seule dans un modèle comme variable explicative, est un cas particulier de régression multiple. En effet, l’encodage catégoriel (ou dummyfication) transforme une variable catégorielle nominale en une matrice de modèle comprenant une colonne désignant l’intercept (une série de 1) désignant la catégorie de référence, ainsi que des colonnes pour chacune des autres catégories désignant l’appartenance (1) ou la non appartenance (0) de la catégorie désignée par la colonne. 5.8.1.4.1 L’encodage catégoriel Une variable à \\(C\\) catégories pourra être déclinée en \\(C\\) variables dont chaque colonne désigne par un 1 l’appartenance au groupe de la colonne et par un 0 la non-appartenance. Pour l’exemple, créons un vecteur désignant le cultivar de pomme de terre. data &lt;- data.frame(cultivar = c(&#39;Superior&#39;, &#39;Superior&#39;, &#39;Superior&#39;, &#39;Russet&#39;, &#39;Kenebec&#39;, &#39;Russet&#39;)) model.matrix(~cultivar, data) ## (Intercept) cultivarRusset cultivarSuperior ## 1 1 0 1 ## 2 1 0 1 ## 3 1 0 1 ## 4 1 1 0 ## 5 1 0 0 ## 6 1 1 0 ## attr(,&quot;assign&quot;) ## [1] 0 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$cultivar ## [1] &quot;contr.treatment&quot; Nous avons trois catégories, encodées en trois colonnes. La première colonne est un intercept et les deux autres décrivent l’absence (0) ou la présence (1) des cultivars Russet et Superior. Le cultivar Kenebec est absent du tableau. En effet, en partant du principe que l’appartenance à une catégorie est mutuellement exclusive, c’est-à-dire qu’un échantillon ne peut être assigné qu’à une seule catégorie, on peut déduire une catégorie à partir de l’information sur toutes les autres. Par exemple, si cultivar_Russet et cultivar_Superior sont toutes deux égales à \\(0\\), on conclura que cultivar_Kenebec est nécessairement égal à \\(1\\). Et si l’un d’entre cultivar_Russet et cultivar_Superior est égal à \\(1\\), cultivar_Kenebec est nécessairement égal à \\(0\\). L’information contenue dans un nombre \\(C\\) de catégorie peut être encodée dans un nombre \\(C-1\\) de colonnes. C’est pourquoi, dans une analyse statistique, on désignera une catégorie comme une référence, que l’on détecte lorsque toutes les autres catégories sont encodées avec des \\(0\\): cette référence sera incluse dans l’intercept. La catégorie de référence par défaut en R est celle la première catégorie dans l’ordre alphabétique. On pourra modifier cette référence avec la fonction relevel(). data$cultivar &lt;- relevel(data$cultivar, ref = &quot;Superior&quot;) model.matrix(~cultivar, data) ## (Intercept) cultivarKenebec cultivarRusset ## 1 1 0 0 ## 2 1 0 0 ## 3 1 0 0 ## 4 1 0 1 ## 5 1 1 0 ## 6 1 0 1 ## attr(,&quot;assign&quot;) ## [1] 0 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$cultivar ## [1] &quot;contr.treatment&quot; Pour certains modèles, vous devrez vous assurer vous-même de l’encodage catégoriel. Pour d’autre, en particulier avec l’interface par formule de R, ce sera fait automatiquement. 5.8.1.4.2 Exemple d’application Prenons la topographie du terrain, qui peut prendre plusieurs niveaux. levels(lasrosas.corn$topo) ## [1] &quot;E&quot; &quot;HT&quot; &quot;LO&quot; &quot;W&quot; Explorons le rendement selon la topographie. ggplot(lasrosas.corn, aes(x = topo, y = yield)) + geom_boxplot() Les différences sont évidentes, et la modélisation devrait montrer des effets significatifs. L’encodage catégoriel peut être visualisé en générant la matrice de modèle avec la fonction model.matrix() et l’interface-formule - sans la variable-réponse. model.matrix(~ topo, data = lasrosas.corn) %&gt;% tbl_df() %&gt;% # tbl_df pour transformer la matrice en tableau sample_n(10) ## # A tibble: 10 x 4 ## `(Intercept)` topoHT topoLO topoW ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 1 0 ## 2 1 0 0 1 ## 3 1 0 0 0 ## 4 1 0 0 1 ## 5 1 0 0 1 ## 6 1 0 0 1 ## 7 1 0 0 0 ## 8 1 0 0 0 ## 9 1 1 0 0 ## 10 1 0 0 1 Dans le cas d’un modèle avec une variable catégorielle nominale seule, l’intercept représente la catégorie de référence, ici E. Les autres colonnes spécifient l’appartenance (1) ou la non-appartenance (0) de la catégorie pour chaque observation. Cette matrice de modèle utilisée pour la régression donnera un intercept, qui indiquera l’effet de la catégorie de référence, puis les différences entre les catégories subséquentes et la catégorie de référence. modlin_4 &lt;- lm(yield ~ topo, data = lasrosas.corn) summary(modlin_4) ## ## Call: ## lm(formula = yield ~ topo, data = lasrosas.corn) ## ## Residuals: ## Min 1Q Median 3Q Max ## -47.371 -11.933 -1.593 11.080 44.119 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 78.6653 0.5399 145.707 &lt;2e-16 *** ## topoHT -30.0526 0.7500 -40.069 &lt;2e-16 *** ## topoLO 6.2832 0.7293 8.615 &lt;2e-16 *** ## topoW -11.8841 0.7039 -16.883 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.59 on 3439 degrees of freedom ## Multiple R-squared: 0.4596, Adjusted R-squared: 0.4591 ## F-statistic: 975 on 3 and 3439 DF, p-value: &lt; 2.2e-16 Le modèle linéaire est équivalent à l’anova, mais les résultats de lm sont plus élaborés. summary(aov(yield ~ topo, data = lasrosas.corn)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## topo 3 622351 207450 975 &lt;2e-16 *** ## Residuals 3439 731746 213 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 L’analyse de résidus peut être effectuée de la même manière. 5.8.1.5 Modèles linéaires univariés avec variable catégorielle ordinale Bien que j’introduise la régression sur variable catégorielle ordinale à la suite de la section sur les variables nominales, nous revenons dans ce cas à une régression simple, univariée. Voyons un cas à 5 niveaux. statut &lt;- c(&quot;Totalement en désaccord&quot;, &quot;En désaccord&quot;, &quot;Ni en accord, ni en désaccord&quot;, &quot;En accord&quot;, &quot;Totalement en accord&quot;) statut_o &lt;- factor(statut, levels = statut, ordered=TRUE) model.matrix(~statut_o) # ou bien, sans passer par model.matrix, contr.poly(5) où 5 est le nombre de niveaux ## (Intercept) statut_o.L statut_o.Q statut_o.C statut_o^4 ## 1 1 -0.6324555 0.5345225 -3.162278e-01 0.1195229 ## 2 1 -0.3162278 -0.2672612 6.324555e-01 -0.4780914 ## 3 1 0.0000000 -0.5345225 -4.095972e-16 0.7171372 ## 4 1 0.3162278 -0.2672612 -6.324555e-01 -0.4780914 ## 5 1 0.6324555 0.5345225 3.162278e-01 0.1195229 ## attr(,&quot;assign&quot;) ## [1] 0 1 1 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$statut_o ## [1] &quot;contr.poly&quot; La matrice de modèle a 5 colonnes, soit le nombre de niveaux: un intercept, puis 4 autres désignant différentes valeurs que peuvent prendre les niveaux. Ces niveaux croient-ils linéairement? De manière quadratique, cubique ou plus loin dans des distributions polynomiales? modmat_tidy &lt;- data.frame(statut, model.matrix(~statut_o)[, -1]) %&gt;% gather(variable, valeur, -statut) modmat_tidy$statut &lt;- factor(modmat_tidy$statut, levels = statut, ordered=TRUE) ggplot(data = modmat_tidy, mapping = aes(x = statut, y = valeur)) + facet_wrap(. ~ variable) + geom_point() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) Règle générale, pour les variables ordinales, on préférera une distribution linéaire, et c’est l’option par défaut de la fonction lm(). L’utilisation d’une autre distribution peut être effectuée à la mitaine en utilisant dans le modèle la colonne désirée de la sortie de la fonction model.matrix(). 5.8.1.6 Régression multiple à plusieurs variables Reprenons le tableau de données du rendement de maïs. head(lasrosas.corn) ## year lat long yield nitro topo bv rep nf ## 1 1999 -33.05113 -63.84886 72.14 131.5 W 162.60 R1 N5 ## 2 1999 -33.05115 -63.84879 73.79 131.5 W 170.49 R1 N5 ## 3 1999 -33.05116 -63.84872 77.25 131.5 W 168.39 R1 N5 ## 4 1999 -33.05117 -63.84865 76.35 131.5 W 176.68 R1 N5 ## 5 1999 -33.05118 -63.84858 75.55 131.5 W 171.46 R1 N5 ## 6 1999 -33.05120 -63.84851 70.24 131.5 W 170.56 R1 N5 Pour ajouter des variables au modèle dans l’interface-formule, on additionne les noms de colonne. La variable lat désigne la latitude, la variable long désigne la latitude et la variable bv (brightness value) désigne la teneur en matière organique du sol (plus bv est élevée, plus faible est la teneur en matière organique). modlin_5 &lt;- lm(yield ~ lat + long + nitro + topo + bv, data = lasrosas.corn) summary(modlin_5) ## ## Call: ## lm(formula = yield ~ lat + long + nitro + topo + bv, data = lasrosas.corn) ## ## Residuals: ## Min 1Q Median 3Q Max ## -48.405 -11.071 -1.251 10.592 40.078 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.946e+05 3.309e+04 5.882 4.45e-09 *** ## lat 5.541e+03 4.555e+02 12.163 &lt; 2e-16 *** ## long 1.776e+02 4.491e+02 0.395 0.693 ## nitro 6.867e-02 5.451e-03 12.597 &lt; 2e-16 *** ## topoHT -2.665e+01 1.087e+00 -24.520 &lt; 2e-16 *** ## topoLO 5.565e+00 1.035e+00 5.378 8.03e-08 *** ## topoW -1.465e+01 1.655e+00 -8.849 &lt; 2e-16 *** ## bv -5.089e-01 3.069e-02 -16.578 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13.47 on 3435 degrees of freedom ## Multiple R-squared: 0.5397, Adjusted R-squared: 0.5387 ## F-statistic: 575.3 on 7 and 3435 DF, p-value: &lt; 2.2e-16 L’ampleur des coefficients est relatif à l’échelle de la variable. En effet, un coefficient de 5541 sur la variable lat n’est pas comparable au coefficient de la variable bv, de -0.5089, étant donné que les variables ne sont pas exprimées avec la même échelle. Pour les comparer sur une même base, on peut centrer (soustraire la moyenne) et réduire (diviser par l’écart-type). scale_vec &lt;- function(x) as.vector(scale(x)) # la fonction scale génère une matrice: nous désirons un vecteur lasrosas.corn_sc &lt;- lasrosas.corn %&gt;% mutate_at(c(&quot;lat&quot;, &quot;long&quot;, &quot;nitro&quot;, &quot;bv&quot;), scale_vec) modlin_5_sc &lt;- lm(yield ~ lat + long + nitro + topo + bv, data = lasrosas.corn_sc) summary(modlin_5_sc) ## ## Call: ## lm(formula = yield ~ lat + long + nitro + topo + bv, data = lasrosas.corn_sc) ## ## Residuals: ## Min 1Q Median 3Q Max ## -48.405 -11.071 -1.251 10.592 40.078 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 78.9114 0.6666 118.376 &lt; 2e-16 *** ## lat 3.9201 0.3223 12.163 &lt; 2e-16 *** ## long 0.3479 0.8796 0.395 0.693 ## nitro 2.9252 0.2322 12.597 &lt; 2e-16 *** ## topoHT -26.6487 1.0868 -24.520 &lt; 2e-16 *** ## topoLO 5.5647 1.0347 5.378 8.03e-08 *** ## topoW -14.6487 1.6555 -8.849 &lt; 2e-16 *** ## bv -4.9253 0.2971 -16.578 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13.47 on 3435 degrees of freedom ## Multiple R-squared: 0.5397, Adjusted R-squared: 0.5387 ## F-statistic: 575.3 on 7 and 3435 DF, p-value: &lt; 2.2e-16 Typiquement, les variables catégorielles, qui ne sont pas mises à l’échelle, donneront des coefficients plus élevées, et devrons être évaluées entre elles et non comparativement aux variables mises à l’échelle. Une manière conviviale de représenter des coefficients consiste à créer un tableau (fonction tibble()) incluant les coefficients ainsi que leurs intervalles de confiance, puis à les porter graphiquement. intervals &lt;- tibble(Estimate = coefficients(modlin_5_sc)[-1], # [-1] enlever l&#39;intercept LL = confint(modlin_5_sc)[-1, 1], # [-1, ] enlever la première ligne, celle de l&#39;intercept UL = confint(modlin_5_sc)[-1, 2], variable = names(coefficients(modlin_5_sc)[-1])) intervals ## # A tibble: 7 x 4 ## Estimate LL UL variable ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 3.92 3.29 4.55 lat ## 2 0.348 -1.38 2.07 long ## 3 2.93 2.47 3.38 nitro ## 4 -26.6 -28.8 -24.5 topoHT ## 5 5.56 3.54 7.59 topoLO ## 6 -14.6 -17.9 -11.4 topoW ## 7 -4.93 -5.51 -4.34 bv ggplot(data = intervals, mapping = aes(x = Estimate, y = variable)) + geom_vline(xintercept = 0, lty = 2) + geom_segment(mapping = aes(x = LL, xend = UL, y = variable, yend = variable)) + geom_point() + labs(x = &quot;Coefficient standardisé&quot;, y = &quot;&quot;) On y voit qu’à l’exception de la variable long, tous les coefficients sont différents de 0. Le coefficient bv est négatif, indiquant que plus la valeur de bv est élevé (donc plus le sol est pauvre en matière organique), plus le rendement est faible. Plus la latitude est élevée (plus on se dirige vers le Nord de l’Argentine), plus le rendement est élevé. La dose d’azote a aussi un effet statistique positif sur le rendement. Quant aux catégories topographiques, elles sont toutes différentes de la catégorie E, ne croisant pas le zéro. De plus, les intervalles de confiance ne se chevauchant pas, on peut conclure en une différence significative d’une à l’autre. Bien sûr, tout cela au seuil de confiance de 0.05. On pourra retrouver des cas où l’effet combiné de plusieurs variables diffère de l’effet des deux variables prises séparément. Par exemple, on pourrait évaluer l’effet de l’azote et celui de la topographie dans un même modèle, puis y ajouter une interaction entre l’azote et la topographie, qui définira des effets supplémentaires de l’azote selon chaque catégorie topographique. C’est ce que l’on appelle une interaction. Dans l’interface-formule, l’interaction entre l’azote et la topographie est notée nitro:topo. Pour ajouter cette interaction, la formule deviendra yield ~ nitro + topo + nitro:topo. Une approche équivalente est d’utiliser le raccourci yield ~ nitro*topo. modlin_5_sc &lt;- lm(yield ~ nitro*topo, data = lasrosas.corn_sc) summary(modlin_5_sc) ## ## Call: ## lm(formula = yield ~ nitro * topo, data = lasrosas.corn_sc) ## ## Residuals: ## Min 1Q Median 3Q Max ## -47.984 -11.985 -1.388 10.339 40.636 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 78.6999 0.5322 147.870 &lt; 2e-16 *** ## nitro 1.8131 0.5351 3.388 0.000711 *** ## topoHT -30.0052 0.7394 -40.578 &lt; 2e-16 *** ## topoLO 6.2026 0.7190 8.627 &lt; 2e-16 *** ## topoW -11.9628 0.6939 -17.240 &lt; 2e-16 *** ## nitro:topoHT 1.2553 0.7461 1.682 0.092565 . ## nitro:topoLO 0.5695 0.7186 0.792 0.428141 ## nitro:topoW 0.7702 0.6944 1.109 0.267460 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.38 on 3435 degrees of freedom ## Multiple R-squared: 0.4756, Adjusted R-squared: 0.4746 ## F-statistic: 445.1 on 7 and 3435 DF, p-value: &lt; 2.2e-16 Les résultats montre des effets de l’azote et des catégories topographiques, mais il y a davantage d’incertitude sur les interactions, indiquant que l’effet statistique de l’azote est sensiblement le même indépendamment des niveaux topographiques. 5.8.1.7 Attention à ne pas surcharger le modèle Il est possible d’ajouter des interactions doubles, triples, quadruples, etc. Mais plus il y a d’interactions, plus votre modèle comprendra de variables et vos tests d’hypothèse perdront en puissance statistique. 5.8.1.8 Les modèles linéaires généralisés Dans un modèle linéaire ordinaire, un changement constant dans les variables explicatives résulte en un changement constant de la variable-réponse. Cette supposition ne serait pas adéquate si la variable-réponse était un décompte, si elle est booléenne ou si, de manière générale, la variable-réponse ne suivait pas une distribution continue. Ou, de manière plus spécifique, il n’y a pas moyen de retrouver une distribution normale des résidus? On pourra bien sûr transformer les variables (sujet du chapitre 6, en développement). Mais il pourrait s’avérer impossible, ou tout simplement non souhaitable de transformer les variables. Le modèle linéaire généralisé (MLG, ou generalized linear model - GLM) est une généralisation du modèle linéaire ordinaire chez qui la variable-réponse peut être caractérisé par une distribution de Poisson, de Bernouilli, etc. Prenons d’abord cas d’un décompte de vers fil-de-fer (worms) retrouvés dans des parcelles sous différents traitements (trt). Les décomptes sont typiquement distribué selon une loi de Poisson. cochran.wireworms %&gt;% ggplot(aes(x = worms)) + geom_histogram(bins = 10) Explorons les décomptes selon les traitements. cochran.wireworms %&gt;% ggplot(aes(x = trt, y = worms)) + geom_boxplot() Les traitements semble à première vue avoir un effet comparativement au contrôle. Lançons un MLG avec la fonction glm(), et spécifions que la sortie est une distribution de Poisson. Bien que la fonction de lien (link = &quot;log&quot;) soit explictement imposée, le log est la valeur par défaut pour les distributions de Poisson. Ainsi, les coefficients du modèles devront être interprétés selon un modèle \\(log \\left(worms \\right) = intercept + pente \\times coefficient\\). modglm_1 &lt;- glm(worms ~ trt, cochran.wireworms, family = stats::poisson(link=&quot;log&quot;)) summary(modglm_1) ## ## Call: ## glm(formula = worms ~ trt, family = stats::poisson(link = &quot;log&quot;), ## data = cochran.wireworms) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.8279 -0.9455 -0.2862 0.6916 3.1888 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.1823 0.4082 0.447 0.655160 ## trtM 1.6422 0.4460 3.682 0.000231 *** ## trtN 1.7636 0.4418 3.991 6.57e-05 *** ## trtO 1.5755 0.4485 3.513 0.000443 *** ## trtP 1.3437 0.4584 2.931 0.003375 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 64.555 on 24 degrees of freedom ## Residual deviance: 38.026 on 20 degrees of freedom ## AIC: 125.64 ## ## Number of Fisher Scoring iterations: 5 L’interprétation spécifique des coefficients d’une régression de Poisson doit passer par la fonction de lien \\(log \\left(worms \\right) = intercept + pente \\times coefficient\\). Le traitement de référence (K), qui correspond à l’intercept, sera accompagné d’un nombre de vers de \\(exp \\left(0.1823\\right) = 1.20\\) vers, et le traitement M, à \\(exp \\left(1.6422\\right) = 5.17\\) vers. Cela correspond à ce que l’on observe sur les boxplots plus haut. Il est très probable (p-value de ~0.66) qu’un intercept (traitement K) de 0.18 ayant une erreur standard de 0.4082 ait été généré depuis une population dont l’intercept est nul. Quant aux autres traitements, leurs effets sont tous significatifs au seuil 0.05, mais peuvent-ils être considérés comme équivalents? intervals &lt;- tibble(Estimate = coefficients(modglm_1), # [-1] enlever l&#39;intercept LL = confint(modglm_1)[, 1], # [-1, ] enlever la première ligne, celle de l&#39;intercept UL = confint(modglm_1)[, 2], variable = names(coefficients(modglm_1))) ## Waiting for profiling to be done... ## Waiting for profiling to be done... intervals ## # A tibble: 5 x 4 ## Estimate LL UL variable ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.182 -0.740 0.888 (Intercept) ## 2 1.64 0.840 2.62 trtM ## 3 1.76 0.972 2.74 trtN ## 4 1.58 0.766 2.56 trtO ## 5 1.34 0.509 2.34 trtP ggplot(data = intervals, mapping = aes(x = Estimate, y = variable)) + geom_vline(xintercept = 0, lty = 2) + geom_segment(mapping = aes(x = LL, xend = UL, y = variable, yend = variable)) + geom_point() + labs(x = &quot;Coefficient&quot;, y = &quot;&quot;) Les intervalles de confiance se superposant, on ne peut pas conclure qu’un traitement est lié à une réduction plus importante de vers qu’un autre, au seuil 0.05. Maintenant, à défaut de trouver un tableau de données plus approprié, prenons le tableau mtcars, qui rassemble des données sur des modèles de voitures. La colonne vs, pour v-shaped, inscrit 0 si les pistons sont droit et 1 s’ils sont placés en V dans le moteur. Peut-on expliquer la forme des pistons selon le poids du véhicule (wt)? mtcars %&gt;% sample_n(6) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## 2 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## 3 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## 4 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## 5 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## 6 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 mtcars %&gt;% ggplot(aes(x = wt, y = vs)) + geom_point() Il semble y avoir une tendance: les véhicules plus lourds ont plutôt des pistons droits (vs = 0). Vérifions cela. modglm_2 &lt;- glm(vs ~ wt, data = mtcars, family = stats::binomial()) summary(modglm_2) ## ## Call: ## glm(formula = vs ~ wt, family = stats::binomial(), data = mtcars) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.9003 -0.7641 -0.1559 0.7223 1.5736 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 5.7147 2.3014 2.483 0.01302 * ## wt -1.9105 0.7279 -2.625 0.00867 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.860 on 31 degrees of freedom ## Residual deviance: 31.367 on 30 degrees of freedom ## AIC: 35.367 ## ## Number of Fisher Scoring iterations: 5 Exercice. Analyser les résultats. 5.8.1.9 Les modèles non-linéaires La hauteur d’un arbre en fonction du temps n’est typiquement pas linéaire. Elle tend à croître de plus en plus lentement jusqu’à un plateau. De même, le rendement d’une culture traité avec des doses croissantes de fertilisants tend à atteindre un maximum, puis à se stabiliser. Ces phénomènes ne peuvent pas être approximés par des modèles linéaires. Examinons les données du tableau engelstad.nitro. engelstad.nitro %&gt;% sample_n(10) ## loc year nitro yield ## 1 Knoxville 1963 67 73.2 ## 2 Knoxville 1963 201 91.2 ## 3 Jackson 1965 134 60.5 ## 4 Knoxville 1962 268 78.4 ## 5 Jackson 1965 201 70.2 ## 6 Jackson 1963 335 87.0 ## 7 Jackson 1965 335 73.0 ## 8 Jackson 1965 67 47.6 ## 9 Jackson 1964 134 55.2 ## 10 Knoxville 1964 335 84.5 engelstad.nitro %&gt;% ggplot(aes(x = nitro, y = yield)) + facet_grid(year ~ loc) + geom_line() + geom_point() Le modèle de Mitscherlich pourrait être utilisé. \\[ y = A \\left( 1 - e^{-R \\left( E + x \\right)} \\right) \\] où \\(y\\) est le rendement, \\(x\\) est la dose, \\(A\\) est l’asymptote vers laquelle la courbe converge à dose croissante, \\(E\\) est l’équivalent de dose fourni par l’environnement et \\(R\\) est le taux de réponse. Explorons la fonction. mitscherlich_f &lt;- function(x, A, E, R) { A * (1 - exp(-R*(E + x))) } x &lt;- seq(0, 350, by = 5) y &lt;- mitscherlich_f(x, A = 75, E = 30, R = 0.02) ggplot(tibble(x, y), aes(x, y)) + geom_point(data = engelstad.nitro, aes(x = nitro, y = yield)) + geom_line() + ylim(c(0, 100)) Exercice. Changez les paramètres pour visualiser comment la courbe réagit. Nous pouvons décrire le modèle grâce à l’interface formule dans la fonction nls(). Notez que les modèles non-linéaires demandent des stratégies de calcul différentes de celles des modèles linéaires. En tout temps, nous devons identifier des valeurs de départ raisonnables pour les paramètres dans l’argument start. Vous réussirez rarement à obtenir une convergence du premier coup avec vos paramètres de départ. Le défi est d’en trouver qui permettront au modèle de converger. Parfois, le modèle ne convergera jamais. D’autres fois, il convergera vers des solutions différentes selon les variables de départ choisies. &lt; #modnl_1 &lt;- nls(yield ~ A * (1 - exp(-R*(E + nitro))), # data = engelstad.nitro, # start = list(A = 50, E = 10, R = 0.2)) Le modèle ne converge pas. Essayons les valeurs prises plus haut, lors de la création du graphique, qui semblent bien s’ajuster. modnl_1 &lt;- nls(yield ~ A * (1 - exp(-R*(E + nitro))), data = engelstad.nitro, start = list(A = 75, E = 30, R = 0.02)) Bingo! Voyons maintenant le sommaire. summary(modnl_1) ## ## Formula: yield ~ A * (1 - exp(-R * (E + nitro))) ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## A 75.023427 3.331860 22.517 &lt;2e-16 *** ## E 66.164111 27.251592 2.428 0.0184 * ## R 0.012565 0.004881 2.574 0.0127 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 12.34 on 57 degrees of freedom ## ## Number of iterations to convergence: 5 ## Achieved convergence tolerance: 8.063e-06 Les paramètres sont significativement différents de zéro au seuil 0.05, et donnent la courbe suivante. x &lt;- seq(0, 350, by = 5) y &lt;- mitscherlich_f(x, A = coefficients(modnl_1)[1], E = coefficients(modnl_1)[2], R = coefficients(modnl_1)[3]) ggplot(tibble(x, y), aes(x, y)) + geom_point(data = engelstad.nitro, aes(x = nitro, y = yield)) + geom_line() + ylim(c(0, 100)) Et les résidus… tibble(res = residuals(modnl_1)) %&gt;% ggplot(aes(x = res)) + geom_histogram(bins = 20) tibble(nitro = engelstad.nitro$nitro, res = residuals(modnl_1)) %&gt;% ggplot(aes(x = nitro, y = res)) + geom_point() + geom_hline(yintercept = 0, colour = &quot;red&quot;) Les résidus ne sont pas distribués normalement, mais semble bien partagés de part et d’autre de la courbe. 5.8.2 Modèles à effets mixtes Lorsque l’on combine des variables fixes (testées lors de l’expérience) et des variables aléatoire (variation des unités expérimentales), on obtient un modèle mixte. Les modèles mixtes peuvent être univariés, multivariés, linéaires ordinaires ou généralisés ou non linéaires. À la différence d’un effet fixe, un effet aléatoire sera toujours distribué normalement avec une moyenne de 0 et une certaine variance. Dans un modèle linéaire où l’effet aléatoire est un décalage d’intercept, cet effet s’additionne aux effets fixes: \\[ y = X \\beta + Z b + \\epsilon \\] où: \\(Z\\) est la matrice du modèle à \\(n\\) observations et \\(p\\) variables aléatoires. Les variables aléatoires sont souvent des variables nominales qui subissent un encodage catégoriel. \\[ Z = \\left( \\begin{matrix} z_{11} &amp; \\cdots &amp; z_{1p} \\\\ z_{21} &amp; \\cdots &amp; z_{2p} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ z_{n1} &amp; \\cdots &amp; z_{np} \\end{matrix} \\right) \\] \\(b\\) est la matrice des \\(p\\) coefficients aléatoires. \\[ b = \\left( \\begin{matrix} b_0 \\\\ b_1 \\\\ \\vdots \\\\ b_p \\end{matrix} \\right) \\] Le tableau lasrosas.corn, utilisé précédemment, contenait trois répétitions effectués au cours de deux années, 1999 et 2001. Étant donné que la répétition R1 de 1999 n’a rien à voir avec la répétition R1 de 2001, on dit qu’elle est emboîtée dans l’année. Le module nlme nous aidera à monter notre modèle mixte. library(&quot;nlme&quot;) mmodlin_1 &lt;- lme(fixed = yield ~ lat + long + nitro + topo + bv, random = ~ 1|year/rep, data = lasrosas.corn) À ce stade vous devriez commencer à être familier avec l’interface formule et vous deviez saisir l’argument fixed, qui désigne l’effet fixe. L’effet aléatoire, random, suit un tilde ~. À gauche de la barre verticale |, on place les variables désignant les effets aléatoire sur la pente. Nous n’avons pas couvert cet aspect, alors nous le laissons à 1. À droite, on retrouve un structure d’emboîtement désignant l’effet aléatoire: le premier niveau est l’année, dans laquelle est emboîtée la répétition. summary(mmodlin_1) ## Linear mixed-effects model fit by REML ## Data: lasrosas.corn ## AIC BIC logLik ## 26535.37 26602.93 -13256.69 ## ## Random effects: ## Formula: ~1 | year ## (Intercept) ## StdDev: 20.35425 ## ## Formula: ~1 | rep %in% year ## (Intercept) Residual ## StdDev: 11.17447 11.35617 ## ## Fixed effects: yield ~ lat + long + nitro + topo + bv ## Value Std.Error DF t-value p-value ## (Intercept) -1379436.9 55894.55 3430 -24.679273 0.000 ## lat -25453.0 1016.53 3430 -25.039084 0.000 ## long -8432.3 466.05 3430 -18.092988 0.000 ## nitro 0.0 0.00 3430 1.739757 0.082 ## topoHT -27.7 0.92 3430 -30.122438 0.000 ## topoLO 6.8 0.88 3430 7.804733 0.000 ## topoW -16.7 1.40 3430 -11.944793 0.000 ## bv -0.5 0.03 3430 -19.242424 0.000 ## Correlation: ## (Intr) lat long nitro topoHT topoLO topoW ## lat 0.897 ## long 0.866 0.555 ## nitro 0.366 0.391 0.247 ## topoHT 0.300 -0.017 0.582 0.024 ## topoLO -0.334 -0.006 -0.621 -0.038 -0.358 ## topoW 0.403 -0.004 0.762 0.027 0.802 -0.545 ## bv -0.121 -0.012 -0.214 -0.023 -0.467 0.346 -0.266 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -4.32360269 -0.66781575 -0.07450856 0.61587533 3.96434001 ## ## Number of Observations: 3443 ## Number of Groups: ## year rep %in% year ## 2 6 La sortie est semblable à celle de la fonction lm(). 5.8.2.1 Modèles mixtes non-linéaires Le modèle non linéaire créé plus haut liait le rendement à la dose d’azote. Toutefois, les unités expérimentales (le site loc et l’année year) n’étaient pas pris en considération. Nous allons maintenant les considérer. Nous devons décider la structure de l’effet aléatoire, et sur quelles variables il doit être appliqué - la décision appartient à l’analyste. Il me semble plus convenable de supposer que le site et l’année affectera le rendement maximum plutôt que l’environnement et le taux: les effets aléatoires seront donc affectés à la variable A. Les effets aléatoires n’ont pas de structure d’emboîtement. L’effet de l’année sur A sera celui d’une pente et l’effet de site sera celui de l’intercept. La fonction que nous utiliserons est nlme(). mm &lt;- nlme(yield ~ A * (1 - exp(-R*(E + nitro))), data = engelstad.nitro, start = c(A = 75, E = 30, R = 0.02), fixed = list(A ~ 1, E ~ 1, R ~ 1), random = A ~ year | loc) summary(mm) ## Nonlinear mixed-effects model fit by maximum likelihood ## Model: yield ~ A * (1 - exp(-R * (E + nitro))) ## Data: engelstad.nitro ## AIC BIC logLik ## 477.2285 491.8889 -231.6143 ## ## Random effects: ## Formula: A ~ year | loc ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## A.(Intercept) 2.602881636 A.(In) ## A.year 0.003066058 -0.556 ## Residual 11.152760033 ## ## Fixed effects: list(A ~ 1, E ~ 1, R ~ 1) ## Value Std.Error DF t-value p-value ## A.(Intercept) 74.58277 4.722806 56 15.792046 0.0000 ## E 65.57006 25.534747 56 2.567876 0.0129 ## R 0.01308 0.004807 56 2.720245 0.0087 ## Correlation: ## A.(In) E ## E 0.379 ## R -0.483 -0.934 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -1.83376801 -0.89290179 0.07419027 0.68354461 1.82431474 ## ## Number of Observations: 60 ## Number of Groups: 2 Et sur graphique: engelstad.nitro %&gt;% ggplot(aes(x = nitro, y = yield)) + facet_grid(year ~ loc) + geom_line(data = tibble(nitro = engelstad.nitro$nitro, yield = predict(mm, level = 0)), colour = &quot;grey35&quot;) + geom_point() + ylim(c(0, 95)) Les modèles mixtes non linéaires peuvent devenir très complexes lorsque les paramètres, par exemple A, E et R, sont eux-même affectés linéairement par des variables (par exemple A ~ topo). Pour aller plus loin, consultez Parent et al. (2017) ainsi que les calculs associés à l’article. Ou écrivez-moi un courriel pour en discuter! Note. L’interprétation de p-values sur les modèles mixtes est controversée. À ce sujet, Douglas Bates a écrit une longue lettre à la communauté de développement du module lme4, une alternative à nlme, qui remet en cause l’utilisation des p-values, ici. De plus en plus, pour les modèles mixtes, on se tourne vers les statistiques bayésiennes, couvertes dans le chapitre 6 avec le module greta. Mais en ce qui a trait aux modèles mixtes, le module brms automatise bien des aspects de l’approche bayésienne. 5.8.3 Aller plus loin 5.8.3.1 Statistiques générales: The analysis of biological data 5.8.3.2 Statistiques avec R Disponibles en version électronique à la bibliothèque de l’Université Laval: Introduction aux statistiques avec R: Introductory statistics with R Approfondir les statistiques avec R: The R Book, Second edition Approfondir les modèles à effets mixtes avec R: Mixed Effects Models and Extensions in Ecology with R ModernDive, un livre en ligne offrant une approche moderne avec le package moderndive. "],
["chapitre-biostats-bayes.html", "6 Introduction à l’analyse bayésienne en écologie 6.1 Qu’est-ce que c’est? 6.2 Pourquoi l’utiliser? 6.3 Comment l’utiliser? 6.4 Faucons pélerins 6.5 Statistiques d’une population 6.6 Test de t: Différence entre des groupes 6.7 Pour aller plus loin", " 6 Introduction à l’analyse bayésienne en écologie ️ Objectifs spécifiques: Ce chapitre est un extra. Il ne fait pas partie des objectifs du cours. Il ne sera pas évalué. À la fin de ce chapitre, vous serez en mesure de définir ce que sont les statistiques bayésiennes serez en mesure de calculer des statistiques descriptives de base en mode bayésien avec le module greta. Les statistiques bayésiennes forment une trousse d’outils à garder dans votre pack sack. 6.1 Qu’est-ce que c’est? En deux mots: modélisation probabiliste. Un approche de modélisation probabiliste se servant au mieux de l’information disponible. Pour calculer les probabilités d’une variable inconnu en mode bayésien, nous avons besoin: De données D’un modèle D’une idée plus ou moins précise du résultat avant d’avoir analysé les données De manière plus formelle, le théorème de Bayes (qui forme la base de l’analyse bayéseienne), dit que la distribution de probabilité des paramètres d’un modèle (par exemple, la moyenne ou une pente) est proportionnelle à la mutliplication de la distribution de probabilité estimée des paramètres et la distribution de probabilité émergeant des données. Plus formellement, \\[P\\left(\\theta | y \\right) = \\frac{P\\left(y | \\theta \\right) \\times P\\left(\\theta\\right)}{P\\left(y \\right)}\\], où \\(P\\left(\\theta | y \\right)\\) \\(-\\) la probabilité d’obtenir des paramètres \\(\\theta\\) à partir des données \\(y\\) \\(-\\) est la distribution de probabilité a posteriori, calculée à partir de votre a prioti \\(P\\left(\\theta\\right)\\) \\(-\\) la probabilité d’obtenir des paramètres \\(\\theta\\) sans égard aux données, selon votre connaissance du phénomène \\(-\\) et vos données observées \\(P\\left(y | \\theta \\right)\\) \\(-\\) la probabilité d’obtenir les données \\(y\\) étant donnés les paramètres \\(\\theta\\) qui régissent le phénomène. \\(P\\left(y\\right)\\), la probabilité d’observer les données, est appellée la vraissemblance marginale, et assure que la somme des probabilités est nulle. 6.2 Pourquoi l’utiliser? Avec la notion fréquentielle de probabilité, on teste la probabilité d’observer les données recueillies étant donnée l’absence d’effet réel (qui est l’hypothèse nulle généralement adoptée). La notion bayésienne de probabilité combine la connaissance que l’on a d’un phénomène et les données observées pour estimer la probabilité qu’il existe un effet réel. En d’autre mots, les stats fréquentielles testent si les données concordent avec un modèle du réel, tandis que les stats bayésiennes évaluent, selon les données, la probabilité que le modèle soit réel. Le hic, c’est que lorsqu’on utilise les statistiques fréquentielles pour répondre à une question bayésienne, on s’expose à de mauvaises interprétations. Par exemple, lors d’un projet considérant la vie sur Mars, les stats fréquentielles évalueront si les données recueillies sont conformes ou non avec l’hypothèse de la vie sur Mars. Par contre, pour évaluer la probabilité de l’existance de vie sur Mars, on devra passer par les stats bayésiennes (exemple tirée du billet Dynamic Ecology – Frequentist vs. Bayesian statistics: resources to help you choose). 6.3 Comment l’utiliser? Bien que la formule du théorème de Bayes soit plutôt simple, calculer une fonction a posteriori demandera de passer par des algorithmes de simulation, ce qui pourrait demander une bonne puissance de calcul, et des outils appropriés. R comporte une panoplie d’outils pour le calcul bayésien générique (rstan, rjags, MCMCpack, etc.), et d’autres outils pour des besoins particuliers (brms: R package for Bayesian generalized multivariate non-linear multilevel models using Stan). Nous utiliserons ici le module générique greta, qui permet de générer de manière conviviale plusieurs types de modèles bayésiens. Pour installer greta, vous devez préalablement installer Python, gréé des modules tensorflow et tensorflow-probability en suivant le guide. En somme, vous devez d’abord installer greta (install.packages(&quot;greta&quot;)). Puis vous devez installer une distribution de Python – je vous suggère Anaconda (~500 Mo) ou Miniconda pour une installation minimale (~60 Mo). Enfin, lancez les commandes suivantes (une connection internet est nécessaire pour télécharger les modules). ``` install_tensorflow(method = “conda”) reticulate::conda_install(“r-tensorflow”, “tensorflow-probability”, pip = TRUE) ``` Enfin… selon la documentation. De mon côté, sur Linux, j’ai installé tensorflow et tensorflow-probability via l’interface anaconda-navigator pour la version Python 3.7. 6.4 Faucons pélerins Empruntons un exemple du livre Introduction to WinBUGS for Ecologists: A Bayesian Approach to Regression, ANOVA and Related Analyses, de Marc Kéry et examinons la masse de faucons pélerins. Mais alors que Marc Kéry utilise WinBUGS, un logiciel de résolution de problème en mode bayésien, nous utiliserons greta. Source: Wikimedia Commons Pour une première approche, nous allons estimer la masse moyenne d’une population de faucons pélerins. À titre de données, générons des nombres aléatoires. Cette stratégie permet de valider les statistiques en les comparant aux paramètre que l’on impose. Ici, nous imposons une moyenne de 600 grammes et un écart-type de 30 grammes. Générons une séries de données avec 20 échantillons. library(&quot;tidyverse&quot;) set.seed(5682) y20 &lt;- rnorm(n = 20, mean=600, sd = 30) y200 &lt;- rnorm(n = 200, mean=600, sd = 30) par(mfrow = c(1, 2)) hist(y20, breaks=5) hist(y200, breaks=20) Je crée une fonction qui retourne la moyenne et l’erreur sur la moyenne ou sur la distribution. Calculons les statistiques classiques. confidence_interval &lt;- function(x, on=&quot;deviation&quot;, distribution=&quot;t&quot;, level=0.95) { m &lt;- mean(x) se &lt;- sd(x) n &lt;- length(x) if (distribution == &quot;t&quot;) { error &lt;- se * qt((1+level)/2, n-1) } else if (distribution == &quot;normal&quot;) { error &lt;- se * qnorm((1+level)/2) } if (on == &quot;error&quot;) { error &lt;- error/sqrt(n) } return(c(ll = m-error, mean = m, ul = m+error)) } print(&quot;Déviation, 95%&quot;) print(round(confidence_interval(y20, on=&#39;deviation&#39;, level=0.95), 2)) print(&quot;Erreur, 95%&quot;) print(round(confidence_interval(y20, on=&#39;error&#39;, level=0.95), 2)) print(&quot;Écart-type&quot;) print(round(sd(y20), 2)) En faisant cela, nous prenons pour acquis que les données sont distribuées normalement. En fait, nous savons qu’elles devraient l’être pour de grands échantillons, puisque nous avons nous-même généré les données. Par contre, comme observateur par exemple de la série de 20 données générées, la distribution est définitivement asymétrique. Sous cet angle, la moyenne, ainsi que l’écart-type, pourraient être des paramètres biaisés. Nous pouvons justifier le choix d’une loi normale par des connaissances a priori des distributions de masse parmi des espèces d’oiseau. Ou bien transformer les données pour rendre leur distribution normale (chapitre 7). 6.5 Statistiques d’une population 6.5.1 greta En mode bayésien, nous devons définir la connaissance a priori sous forme de variables aléatoires non-observées selon une distribution. Prenons l’exemple des faucons pélerins. Disons que nous ne savons pas à quoi ressemble la moyenne du groupe a priori. Nous pouvons utiliser un a priori vague, où la masse moyenne peut prendre n’importe quelle valeur entre 0 et 2000 grammes, sans préférence: nous lui imposons donc un a priori selon une distribution uniforme. Idem pour l’écart-type library(&quot;greta&quot;) library(&quot;DiagrammeR&quot;) library (&quot;bayesplot&quot;) library(&quot;tidybayes&quot;) param_mean &lt;- uniform(min = 0, max = 2000) param_sd &lt;- uniform(min = 0, max = 100) La fonction a porteriori inclue la fonction de vraissemblance ainsi que la connaissancew a priori. distribution(y20) &lt;- normal(param_mean, param_sd) Le tout forme un modèle pour apprécier y, la masse des faucons pélerins. m &lt;- model(param_mean, param_sd) plot(m) Légende: Nous pouvons enfin lancer le modèle . draws &lt;- mcmc(m, n_samples = 1000) L’inspection de l’échantillonnage peut être effectuée grâce au module bayesplot. mcmc_combo(draws, combo = c(&quot;hist&quot;, &quot;trace&quot;)) L’échantillonnage semble stable. Voyons la distribution a posteriori des paramètres. draws_tidy &lt;- draws %&gt;% spread_draws(param_mean, param_sd) print(&quot;Moyenne:&quot;) confidence_interval(x = draws_tidy$param_mean, on = &quot;deviation&quot;, distribution = &quot;normal&quot;, level = 0.95) print(&quot;Écart-type:&quot;) confidence_interval(x = draws_tidy$param_sd, on = &quot;deviation&quot;, distribution = &quot;normal&quot;, level = 0.95) L’a priori étant vague, les résultats de l’analyse bayésienne sont comparables aux statistiques fréquentielles. print(&quot;Erreur, 95%&quot;) print(round(confidence_interval(y20, on=&#39;error&#39;, level=0.95), 2)) Les résultats des deux approches peuvent néanmoins être interprétés de manière différente. En ce qui a trait à la moyenne: Fréquentiel. Il y a une probabilité de 95% que mes données aient été générées à partir d’une moyenne se situant entre 584 et 614 grammes. Bayésien. Étant donnée mes connaissances (vagues) de la moyenne et de l’écart-type avant de procéder à l’analyse (a priori), il y a une probabilité de 95% que la moyenne de la masse de la population se situe entre 583 et 614 grammes. Nous avons maintenant une idée de la distribution de moyenne de la population. Mais, rarement, une analyse s’arrêtera à ce stade. Il arrive souvent que l’on doive comparer les pparamètres de deux, voire plusieurs groupes. Par exemple, comparer des populations vivants dans des écosystèmes différents, ou comparer un traitement à un placébo. Ou bien, comparer, dans une même population de faucons pélerins, l’envergure des ailes des mâles et celle des femelles. 6.6 Test de t: Différence entre des groupes Pour comparer des groupes, on exprime généralement une hypothèse nulle, qui typiquement pose qu’il n’y a pas de différence entre les groupes. Puis, on choisit un test statistique pour déterminer si les distributions des données observées sont plausibles dans si l’hypothèse nulle est vraie. En d’autres mots, le test statistique exprime la probabilité que l’on obtienne les données obtenues s’il n’y avait pas de différence entre les groupes. Par exemple, si vous obtenez une p-value de moins de 0.05 après un test de comparaison et l’hypothèse nulle pose qu’il n’y a pas de différence entre les groupes, cela signifie qu’il y a une probabilité de 5% que vous ayiez obtenu ces données s’il n’y avait en fait pas de différence entre les groupe. Il serait donc peu probable que vos données euent été générées comme telles s’il n’y avait en fait pas de différence. n_f &lt;- 30 moy_f &lt;- 105 n_m &lt;- 20 moy_m &lt;- 77.5 sd_fm &lt;- 2.75 set.seed(21526) envergure_f &lt;- rnorm(mean=moy_f, sd=sd_fm, n=n_f) envergure_m &lt;- rnorm(mean=moy_m, sd=sd_fm, n=n_m) envergure_f_df &lt;- data.frame(Sex = &quot;Female&quot;, Wingspan = envergure_f) envergure_m_df &lt;- data.frame(Sex = &quot;Male&quot;, Wingspan = envergure_m) envergure_df &lt;- rbind(envergure_f_df, envergure_m_df) envergure_df %&gt;% ggplot(aes(x=Wingspan)) + geom_histogram(aes(y=..density.., fill=Sex)) + geom_density(aes(value=Sex, y=..density..)) Et les statistiques des deux groupesL envergure_df %&gt;% group_by(Sex) %&gt;% summarise(mean = mean(Wingspan), sd = sd(Wingspan), n = n()) Évaluer s’il y a une différence significative peut se faire avec un test de t (ou de Student). t.test(envergure_f, envergure_m) La probabilité que les données ait été générées de la sorte si les deux groupes n’était semblables est très faible (p-value &lt; 2.2e-16). On obtiendrait sensiblement les mêmes résultats avec une régression linéaire. linmod &lt;- lm(Wingspan ~ Sex, envergure_df) summary(linmod) Le modèle linéaire est plus informatif. Il nous apprend que l’envergure des ailes des mâles est en moyenne plus faible de 28.0 cm que celle des femelles… confint(linmod, level = 0.95) … avec un intervalle de confiance entre -29.6 cm à -26.4 cm. Utilisons l’information dérivée de statistiques classiques dans nos a priori. Oui-oui, on peut faire ça. Mais attention, un a priori trop précis ou trop collé sur nos données orientera le modèle vers une solution préalablement établie: ce qui constituerait aucune avancée par rapport à l’a priori. Nous allons utiliser a priori pour les deux groupes la moyenne des deux groupes, et comme dispersion la moyenne le double de l’écart-type. Rappelons que cet écart-type est l’a priori de écart-type sur la moyenne, non pas de la population. Procédons à la création d’un modèle greta. Nous utiliserons la régression linéaire préférablement au test de t. is_female &lt;- model.matrix(~envergure_df$Sex)[, 2] int &lt;- normal(600, 30) coef &lt;- normal(30, 10) sd &lt;- cauchy(0, 10, truncation = c(0, Inf)) mu &lt;- int + coef * is_female distribution(envergure_df$Wingspan) &lt;- normal(mu, sd) m &lt;- model(int, coef, sd, mu) plot(m) Go! draws &lt;- mcmc(m, n_samples = 1000) Et les résultats. mcmc_combo(draws, combo = c(&quot;dens&quot;, &quot;trace&quot;), pars = c(&quot;int&quot;, &quot;coef&quot;, &quot;sd&quot;)) draws_tidy &lt;- draws %&gt;% spread_draws(int, coef, sd) draws_tidy print(&quot;Intercept:&quot;) confidence_interval(x = draws_tidy$int, on = &quot;deviation&quot;, distribution = &quot;normal&quot;, level = 0.95) print(&quot;Pente:&quot;) confidence_interval(x = draws_tidy$coef, on = &quot;deviation&quot;, distribution = &quot;normal&quot;, level = 0.95) 6.7 Pour aller plus loin Le module greta est conçu et maintenu par Nick Golding, du Quantitative &amp; Applied Ecology Group de l’University of Melbourne, Australie. La documentation de greta offre des recettes pour toutes sortes d’analyses en écologie. Les livres de Mark Kéry, bien que rédigés pour les calculs en langage R et WinBUGS, offre une approche bien structurée et traduisible en greta, qui est plus moderne que WinBUGS. Introduction to WinBUGS for Ecologists (2010) Bayesian Population Analysis using WinBUGS: A Hierarchical Perspective (2011) Applied Hierarchical Modeling in Ecology: Analysis of distribution, abundance and species richness in R and BUGS (2015) "],
["chapitre-explorer.html", "7 Explorer R 7.1 R sur le web 7.2 R en chaire et en os 7.3 Quelques outils en écologie mathématique avec R", " 7 Explorer R L’apprentissage de R peut être étourdissant. Cette section est une petite pause fourre-tout qui vous introduira aux nombreuses possibilités de R. ️ Objectifs spécifiques: À la fin de ce chapitre, vous serez en mesure d’identifier les sources d’information principales sur le développement de R et de ses modules comprendrez l’importance du prétraitement des données, en particulier dans le cadre de l’analyse de données compositionnelles, et saurez effectuer un prétraitement adéquat saurez comment acquérir des données météo d’Environnement Canada avec le module weathercan saurez identifier les modules d’analyse de sols (soiltexture et aqp) saurez comment débuter un projet de méta-analyse et de déploiement d’un logiciel sur R Pour certains, le langage R est un labyrinthe. Pour d’autres, c’est une myriade de portes ouvertes. Si vous lisez ce manuel, vous vous êtes peut-être engagé dans un labyrinthe dans l’objectif d’y trouver la clé qui dévérouillera une porte bien précise qui mène à un trésor, un objet magique… ou un diplôme. Peut-être aussi prendrez-vous le goût d’errer dans ce labyrinthe, explorant ses débouchés, pour y dénicher au hasard des petits outils et des débouchés. Séquence du jeu vidéo The legend of Zelda. Cette section est un amalgame de plusieurs outils de R pertinents en analyse écologique. 7.1 R sur le web Dans un environnement de travail en évolution rapide et constante, il est difficile de considérer que ses compétences sont abouties. Rester informé sur le développement de R vous permettra de dénicher de résoudre des problèmes persistants de manière plus efficace ou par de nouvelles avenues, et vous offrira même l’occasion de dénicher des problèmes dont vous ne soupçonniez pas l’existance. Plusieurs sources d’information vous permettront de vous tenir à jour sur le développement de R, de ses environnement de travail (RStudio, Jupyter, Atom, etc.) et des nouveaux modules qui s’y greffent. Plus largement, vous gagnerez à vous informer sur les dernières tendances en calcul scientifique sur d’autres plate-forme que R (Python, Javascript, Julia, etc.). Évidemment, nos tâches quotidiennes ne nous permettent pas de tout suivre. Même si vous pouviez n’attrapper qu’1% du défilement, ce sera déjà 1% de plus que rien du tout. Évidemment, rester au courant aide parce que vous en apprenez davantage sur les outils et leurs applications. Mais ça aide aussi parce que ça vous permet de connaître des gens et des organisations! Il est très utile de savoir qui travaille sur quoi et où se déroulent les développements sur un sujet donné, car si vous cherchez consciemment quelque chose plus tard, ça vous aidera à trouver votre chemin plus facilement. - Maëlle Salmon, Keeping up to date with R news (ma traduction) Je vous propose une liste de ressources. Ne vous y tenez surtout pas: discartez ce qui ne vous convient pas, et partez à l’aventure! The Hobbit: An Unexpected Journey, Peter Jackson (2012) 7.1.1 GitHub Nous verrons au chapitre 11 l’importance d’utilser des outils d’archivage et de suivi de version, comme git, dans le déploiement de la science ouverte. Pour l’instant, retenons que GitHub est une plate-forme git en ligne acquise par Microsoft qui est devenue un réseau social de développement informatique. De nombreux modules de R y sont développés. Au chapitre 11, vous serez invités à y ouvrir un compte et à y archiver du contenu. Vous pourrez alors suivre (dans le même sens que sur Facebook ou Twitter) le développement de projets et suivre les travaux des personnes qui vous semblent d’intérêt. 7.1.2 Twitter Le hashtag #rstats rassemble sur Twitter ce qui se tweete sur le sujet. On y retrouve les comptes de R-bloggers, RStudio et rOpenSci. Certaines communauté y sont aussi actives, comme R4DS online learning community, qui partage des nouvelles sur R, et R-Ladies Global, qui vise à amener davantage de diversité à la communauté de R. Des comptes thématiques comme Daily R Cheatsheets et One R Package a Day permettent de découvrir quotidiennement de nouvelles possibilités. Enfin, plusieurs personnes contribuent positivement à la communauté R. Hadley Wickham brille parmi les étoiles de R. Les comptes de Mara Averick, Claus Wilke et David Robinson sont aussi intéressants. 7.1.3 Nouvelles Le site d’aggrégation R-bloggers, mis à jour quotidiennement, republie des articles en anglais tirés d’un peu partout sur la toile. On y trouve principalement des tutoriels et des annonces de nouveaux développement. Deux fois par mois, l’organisation rOpenSci offre un portrait de l’univ-R (💩), ce que R Weekely offre de manière hebdomadaire (l’information sera probablement redondante). Le tidyverse a quant à lui son propre blogue. 7.1.4 Des questions? Bien que davantage voués à la résolution de problème qu’ à l’exploration de nouvelles opportunités, Stackoverflow et Cross Validated sont des plate-forme prisées. De plus, la liste de courriels r-sig-ecology permet des échanges entre professionnels et novices en analyse de données écologiques avec R. 7.1.5 Participer R est un logiciel basé sur une communauté de développement, d’utilisation et de vulgarisation. Des personnes offrent généreusement du temps de support. Si vous vous sentez à l’aise, offrez aussi le vôtre! 7.1.6 Mise en garde Les modules de R sont développés par quiconque le veut bien: leur qualité n’est pas nécessairement auditée. Souvent, ils ne sont vérifiés que par une vigilance communautaire: dans ce cas, vous êtes les cobailles. Ce qui n’est pas nécessairement une mauvaise chose, mais cela nécessite de prendre ses précautions. Dans sa conférence How to be a resilient R user, Maëlle Salmon propose quelques guides pour juger de la qualité d’un module. 1. Le module est-il activement développé? Bien! Attention! 2. Le module est-il bien testé? Vérifiez si le module a fait l’objet d’une publication scientifique, s’il a été utilisé avec succès dans la litérature ou dans des documents crédibles. 3. Le module est-il bien documenté? Un site internet dédié est-il utilisé pour documenter l’utilisation du module? Les fichiers d’aide sont-ils complets, et sont-ils de bonne qualité? 4. Le module est-il largement utilisé? Un module peu populaire n’est pas nécessessairement de mauvaise qualité: peut-être est-il seulement destiné à des applications de niche. S’il n’est pas un indicateur à lui seul de la solidité ou la validité d’un module, une masse critique indique que le module a passé sous la surveillance de plusieurs utilisateurs. Dans GitHub, ceci peut être évalué par le nombre d’étoiles attribué au module (équivalent à un J’aime). 5. Le module est-il développé par une personne ou une organisation crédible? On peut affirmer sans trop se compromettre que l’équipe de RStudio développe des modules de confiance. Tout comme il faudrait se méfier d’un module développé par une personne anonyme. Le module packagemetrics permet d’évaluer ces critères. library(&quot;packagemetrics&quot;) pm &lt;- package_list_metrics(c(&quot;dplyr&quot;, &quot;ggplot2&quot;, &quot;vegan&quot;, &quot;greta&quot;)) metrics_table(pm) 7.1.7 Prendre tout ça en note Un logiciel de prise de notes (comme Evernote, OneNote, Notion, Simplenote, Turtl, etc.) pourrait vous être utile pour retrouver l’information soutirée de vos flux d’information. Mais certaines personnes consignent simplement leurs informations dans un carnet ou un document de traitement de texte. 7.2 R en chaire et en os L’Université Laval (institution auprès de laquelle ce manuel est développé) sera haute en mai 2019 de la conférence R à Québec 2019. Des ateliers seront offerts pour les utilisateurs novices et avancés. 7.3 Quelques outils en écologie mathématique avec R 7.3.1 Prétraitement des données Il arrive souvent ques les données brutes ne soient pas exprimées de manière appropriée ou optimale pour l’analyse statistique ou la modélisation. Vous devrez alors effectuer un prétraitement sur ces données. Lors du chapitre 5, nous avons abordé la mise à l’échelle, où des variables numériques étaient transformées pour avoir une moyenne de zéro et un écart-type de 1. Cette opération permettait d’apprécier les coefficients et leur incertitude sur une même échelle. L’encodage catégorielle a quant à lui permi d’utiliser des méthodes quantitatives sur des données qualitatives. Dans les deux cas, nous n’avons pas utilisé le terme, mais il s’agissait d’un prétraitement, c’est-à-dire une transformation des données préalable à l’analyse ou la modélisation. Un prétraitement peut consister simplement en une transformation logarithmique ou exponentielle. En particulier, si vos données forment une partie d’un tout (exprimées en pourcentages ou fractions), vous devriez probablement utiliser un prétraitement grâce aux outils de l’analyse compositionnelle. Avant de les aborder, nous allons traiter des transformations de base. 7.3.1.1 Standardisation La standardisation consiste à centrer vos données à une moyenne de 0 et à les échelonner à une variance de 1, c’est-à-dire \\[x_{standard} = \\frac{x - \\bar{x}}{\\sigma}\\] où \\(\\bar{x}\\) est la moyenne du vecteur \\(x\\) et où \\(\\sigma\\) est son écart-type. Ce prétraitement des données peut s’avérér utile lorsque la modélisation tient compte de l’échelle de vos mesures (par exemple, les paramètres de régression vus au chapitre 5 ou les distances que nous verrons au chapitre 8). En effet, les pentes d’une régression linéaire multiple ne pourront être comparées entre elles que si elles sont une même échelle. Par exemple, on veut modéliser la consommation en miles au gallon (mpg) de voitures en fonction de leur puissance (hp), le temps en secondes pour parcourir un quart de mile (qsec) et le nombre de cylindre. data(&quot;mtcars&quot;) modl &lt;- lm(mpg ~ hp + qsec + cyl, mtcars) summary(modl) ## ## Call: ## lm(formula = mpg ~ hp + qsec + cyl, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.3223 -1.9483 -0.5656 1.5452 7.7773 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 55.30540 9.03697 6.120 1.33e-06 *** ## hp -0.03552 0.01622 -2.190 0.03700 * ## qsec -0.89424 0.42755 -2.092 0.04567 * ## cyl -2.26960 0.54505 -4.164 0.00027 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.003 on 28 degrees of freedom ## Multiple R-squared: 0.7757, Adjusted R-squared: 0.7517 ## F-statistic: 32.29 on 3 and 28 DF, p-value: 3.135e-09 Les pentes signifient que la distance parcourue par gallon d’essence diminue de 0.03552 miles au gallon pour chaque HP, de 0.89242 par seconde au quart de mile et de 2.2696 par cyclindre additionnel. L’interprétation est conviviale à cette échelle. Mais lequel de ces effets est le plus important? L t value indique que ce seraient les cylindres. Mais pour juger l’importance en terme de pente, il vaudrait mieux standardiser. library(&quot;tidyverse&quot;) standardise &lt;- function(x) (x-mean(x))/sd(x) mtcars_sc &lt;- mtcars %&gt;% mutate_if(is.numeric, standardise) # ou bien scale(mtcars, center = TRUE, scale = TRUE) modl_sc &lt;- lm(mpg ~ hp + qsec + cyl, mtcars_sc) summary(modl_sc) ## ## Call: ## lm(formula = mpg ~ hp + qsec + cyl, data = mtcars_sc) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.71716 -0.32326 -0.09384 0.25639 1.29042 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.061e-16 8.808e-02 0.000 1.00000 ## hp -4.041e-01 1.845e-01 -2.190 0.03700 * ## qsec -2.651e-01 1.268e-01 -2.092 0.04567 * ## cyl -6.725e-01 1.615e-01 -4.164 0.00027 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4983 on 28 degrees of freedom ## Multiple R-squared: 0.7757, Adjusted R-squared: 0.7517 ## F-statistic: 32.29 on 3 and 28 DF, p-value: 3.135e-09 Les valeurs des pentes ne peuvent plus être interprétées directement, mais peuvent maintenant être comparées entre elles. Dans ce cas, le nombre de cilyndres a en effet une importance plus grande que la puissance et le temps pour parcourir un 1/4 de mile. Les algorithmes basés sur des distances auront, de même, avantage à être standardisés. 7.3.1.2 À l’échelle de la plage Si vous désirez préserver le zéro dans le cas de données positives ou plus généralement vous voulez que vos données prétraitées soient positives, vous pouvez les transformer à l’échelle de la plage, c’est-à-dire les forcer à s’étaler de 0 à 1: \\[ x_{range01} = \\frac{x - x_{min}}{x_{max} - x_{min}} \\] Cette transformation est sensible aux valeurs aberrantes, et une fois le vecteur transformé les valeurs aberrantes seront toutefois plus difficiles à détecter. range_01 &lt;- function(x) (x-min(x))/(max(x) - min(x)) mtcars %&gt;% mutate_if(is.numeric, range_01) %&gt;% # en fait, toutes les colonnes sont numériques, alors mutate_all aurait pu être utilisé au lieu de mutate_if sample_n(4) ## mpg cyl disp hp drat wt qsec vs am ## 1 0.8510638 0.0 0.05986530 0.21554770 0.46543779 0.0000000 0.2857143 1 1 ## 2 0.0000000 1.0 1.00000000 0.54063604 0.07834101 0.9555101 0.4142857 0 0 ## 3 0.9361702 0.0 0.01895735 0.04946996 0.60829493 0.1756584 0.5916667 1 1 ## 4 0.4680851 0.5 0.46620105 0.20494700 0.14746544 0.4351828 0.5880952 1 0 ## gear carb ## 1 1.0 0.1428571 ## 2 0.0 0.4285714 ## 3 0.5 0.0000000 ## 4 0.0 0.0000000 7.3.1.3 Normaliser Le terme normaliser est associer à des opérations différentes dans la littérature. Nous prendrons la nomenclature de scikit-learn, pour qui la normalisation consiste à faire en sorte que la longueur du vecteur (sa norme, d’où normaliser) soit unitaire. Cette opération est le plus souvent utilisée par observation (ligne), non pas par variable (colonne). Il existe plusieurs manières de mesures la distance d’un vecteur, mais la plus commune est la distance euclidienne. La seule fois que j’ai eu à utiliser ce prétraitement était en analyse spectrale (Chemometrics with R, Ron Wehrens, 2011, chapitre 3.5). En R, library(&quot;pls&quot;) ## ## Attaching package: &#39;pls&#39; ## The following object is masked from &#39;package:vegan&#39;: ## ## scores ## The following object is masked from &#39;package:stats&#39;: ## ## loadings data(&quot;gasoline&quot;) spectro &lt;- gasoline$NIR %&gt;% unclass() %&gt;% as_tibble() normalise &lt;- function(x) x/sqrt(sum(x^2)) spectro_norm &lt;- spectro %&gt;% rowwise() %&gt;% # différentes approches possibles pour les opérations sur les lignes normalise() spectro_norm[1:4, 1:4] ## 900 nm 902 nm 904 nm 906 nm ## 1 -0.0011224834 -0.0010265446 -0.0009434425 -0.0008314021 ## 2 -0.0009890637 -0.0008856332 -0.0007977676 -0.0006912734 ## 3 -0.0010481029 -0.0009227116 -0.0008269742 -0.0007035061 ## 4 -0.0010444801 -0.0009446277 -0.0008623530 -0.0007718261 7.3.1.4 Analyse compositionnelle en R En 1898, le statisticien Karl Pearson nota que des corrélations étaient induites lorsque l’on effectuait des ratios par rapport à une variable commune. Source Karl Pearson, 1897. Mathematical contributions to the theory of evolution.—on a form of spurious correlation which may arise when indices are used in the measurement of organs. Proceedings of the royal society of London Faisons l’exercice! Nous générons au hasard 1000 données (comme le proposait Pearson) pour trois dimensions: le fémur, le tibia et l’humérus. Ces dimensions ne sont pas générées par des distributions corrélées. set.seed(3570536) n &lt;- 1000 bones &lt;- tibble(femur = rnorm(n, 10, 3), tibia = rnorm(n, 8, 2), humerus = rnorm(n, 6, 2)) plot(bones) cor(bones) ## femur tibia humerus ## femur 1.000000000 -0.069006171 0.002652292 ## tibia -0.069006171 1.000000000 -0.008994704 ## humerus 0.002652292 -0.008994704 1.000000000 Pourtant, si j’utilise des ratios allométriques avec l’humérus comme base, bones_r &lt;- bones %&gt;% transmute(fh = femur/humerus, th = tibia/humerus) plot(bones_r) text(30, 20, paste(&quot;corrélation =&quot;, round(cor(bones_r$fh, bones_r$th), 2)), col = &quot;blue&quot;) Nous avons induit ce que Pearson appelait une fausse corrélation (spurious correlation). En 1960, Chayes proposa que de telles fausses corrélations sont induites non seulement sur des ratios de valeurs absolues, mais aussi sur des ratios d’une somme totale. Par exemple, dans une composition simple de deux types d’utilisation du territoire, si une proportion augmente, l’autre doit nécessairement diminuer. n &lt;- 100 tibble(A = runif(n, 0, 1)) %&gt;% mutate(B = 1 - A) %&gt;% ggplot(aes(x=A, y=B)) + geom_point() Les variables exprimées relativement à une somme totale sont dites compositionnelles. Elles possèdent les caractéristiques suivantes. Redondance d’information. Un système de deux proportions ne contient qu’une seule variable du fait que l’on puisse déduire l’une en soutrayant l’autre de la somme totale. Un vecteur compositionnel contient de l’information redondante. Pourtant, effectuer des statistiques sur l’une plutôt que sur l’autre donnera des résultats différents. Dépendance d’échelle. Les statistiques devraient être indépendantes de la somme totale utilisée. Pourtant, elles différeront sur l’on utilise par exemple, une proportion des mâles d’une part et des femelles d’autre part, ou la proportion de la somme des deux, de même que les résultats d’un test sanguin différera si l’on utilise une base sèche ou une base humide. Distribution théorique des données. Étant donnée que les proportions sont confinées entre 0 et 1 (ou 100%, ou une somme totale quelconque), la distribution normale (qui s’étend de -∞ à +∞) n’est souvent pas appropriée. On pourra utiliser la distribution de Dirichlet ou la distribution logitique-normale, mais d’autres approches sont souvent plus pratiques. Pour illustrer l’effet de la distribution, voyons un diagramme ternaire incluant le sable, le limon et l’argile. En utilisant des écart-types univariés, nous obtenons l’ellipse en rouge, qui non seulement représente peu l’étalement des données, mais elle dépasse les bornes du triangle, admettant ainsi des proportions négatives. En bleu, la distribution logistique normale (issue des méthodes présentées plus loin dans cette section) convient davantage. Les conséquences d’effectuer des statistiques linéaires sur des données compositionnelles brutes peuvent être majeures. En outre, Pawlowksy-Glahn et Egozcue (2006), s’appuyant en outre sur Rock (1988), note les problèmes suivants (exprimés en mes mots). les régressions, les regroupements et les analyses en composantes principales peuvent avoir peu ou pas de signification les propriétés des distributions peuvent être générées par l’opération de fermeture de la composition (s’assurer que le total des proportions donne 100%) les résultats d’analyses discriminantes linéaires sont propices à être illusoires tous les coefficients de corrélation seront affectés à des degrés inconnus les résultats des tests d’hypothèses seront intrinsèquement faussés Pour contourner ces problèmes, il faut d’abord aborder les données compositionnelles pour ce qu’elles sont: des données intrinsèquement multivariées. Elles sont un nuage de point, et non pas une collection de variables individuelles. Ceci qui n’empêche pas d’effectuer des analyses consciencieusement sous des angles particuliers. En R, on pourra aisément rapporter une composition en somme unitaire grâce à la fonction apply. Mais auparavant, chargeons le module compositions (n’oubliez pas de l’installer au préalable) pour accéder à des données fictives de proportions de sable, limon et argile dans des sédiments. library(&quot;compositions&quot;) ## Loading required package: tensorA ## ## Attaching package: &#39;tensorA&#39; ## The following object is masked from &#39;package:base&#39;: ## ## norm ## Loading required package: robustbase ## Loading required package: energy ## Loading required package: bayesm ## Welcome to compositions, a package for compositional data analysis. ## Find an intro with &quot;? compositions&quot; ## ## Attaching package: &#39;compositions&#39; ## The following object is masked from &#39;package:pls&#39;: ## ## R2 ## The following objects are masked from &#39;package:stats&#39;: ## ## cor, cov, dist, var ## The following objects are masked from &#39;package:base&#39;: ## ## %*%, scale, scale.default data(&quot;ArcticLake&quot;) ArcticLake &lt;- ArcticLake %&gt;% as_tibble() head(ArcticLake) ## # A tibble: 6 x 4 ## sand silt clay depth ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 77.5 19.5 3 10.4 ## 2 71.9 24.9 3.2 11.7 ## 3 50.7 36.1 13.2 12.8 ## 4 52.2 40.9 6.6 13 ## 5 70 26.5 3.5 15.7 ## 6 66.5 32.2 1.3 16.3 comp &lt;- ArcticLake %&gt;% select(-depth) %&gt;% apply(., 1, function(x) x/sum(x)) %&gt;% t() comp[1:5, ] ## sand silt clay ## [1,] 0.7750000 0.1950000 0.0300000 ## [2,] 0.7190000 0.2490000 0.0320000 ## [3,] 0.5070000 0.3610000 0.1320000 ## [4,] 0.5235707 0.4102307 0.0661986 ## [5,] 0.7000000 0.2650000 0.0350000 On pourra aussi utiliser la fonction acomp (pour Aitchison-composition) pour fermer la composition à une somme de 1. comp &lt;- ArcticLake %&gt;% select(-depth) %&gt;% acomp(.) comp[1:5, ] ## sand silt clay ## [1,] 0.7750000 0.1950000 0.0300000 ## [2,] 0.7190000 0.2490000 0.0320000 ## [3,] 0.5070000 0.3610000 0.1320000 ## [4,] 0.5235707 0.4102307 0.0661986 ## [5,] 0.7000000 0.2650000 0.0350000 Cette stratégie a pour avantage d’attribuer à la variable comp la classe acomp, qui automatise les opérations dans l’espace compositionnel (que l’on nomme aussi le simplex). La représentation ternaire est souvent utilisée pour présenter des compositions. Toutefois, il est difficile d’interpréter les compositions de plus de trois parties. La classe acomp automatise aussi la représentation teranaire. plot(comp) Afin de transposer cet espace clôt en un espace ouvert, on pourra diviser chaque proportion par une proportion de référence choisie parmi n’importe quelle proportion. Du coup, on retire une dimension redondante! Dans ce ratio, on choisit d’utiliser la proportion de référence au dénominateur, ce qui est arbitraire. En utilisant le log du ratio, l’inverse du ratio ne sera qu’un changement de signe, ce qui est pratique en statistiques linéaries. Cette solution, proposée par Aitchison (1986), s’applique non seulement sur les compositions à deux composantes, mais sur toute composition. Il s’agit alors d’utiliser une composition de référence pour effecteur les ratios. Pour une composition de \\(A\\), \\(B\\), \\(C\\), \\(D\\) et \\(E\\): \\[alr_A = log \\left( \\frac{A}{E} \\right), alr_B = log \\left( \\frac{B}{E} \\right), alr_C = log \\left( \\frac{C}{E} \\right), alr_D = log \\left( \\frac{D}{E} \\right)\\] Dans R, la colonne de référence est par défaut la dernière colonne de la matrice des compositions. add_lr &lt;- alr(comp) Cette dernière stratégie se nomme les log-ratios aditifs (\\(alr\\) pour additive log-ratio). Bien que valide pour effectuer des tests statistiques, cette stratégie a le désavantage de dépendre de la décision arbitraire de la composante à utiliser au numérateur. Deuxième restriction des alr: les axes de l’espace des alr n’étant pas orthogonaux, ils ne peuvent pas être utilisés pour effectuer des statistiques basées sur les distances (que nous couvrirons au chapitre 8). L’autre stratégie proposée par Aitchison était d’effectuer un log-ratio entre chaque composante et la moyenne géométrique de toutes les composantes. Cette transformation se nomme le log-ratio centré (\\(clr\\), pour centered log-ratio) \\[clr_i = log \\left( \\frac{x_i}{g \\left( x \\right)} \\right)\\] En R, cen_lr &lt;- clr(comp) Avec des CLRs, les distances sont valides. Mais… nous restons avec le problème de la redondance d’information. En fait, la somme de chacunes des lignes d’une matrice de clr est de 0. Pas très pratique lorsque l’on effectue des statistiques incluant une inversion de la matrice de covariance (distance de Mahalanobis, géostatistiques, etc.) cen_lr %&gt;% cov() %&gt;% solve() Error in solve.default(.) : le système est numériquement singulier : conditionnement de la réciproque = 4.44407e-17 Enfin, une autre méthode de transformation développée par Egoscue et al. (2003), les log-ratios isométriques (ou isometric log-ratios, ilr) projette les compositions comprenant D composantes dans un espace restreint de D-1 dimensions orthonormées. Ces dimensions doivent doivent être préalablement établie dans un dendrogramme de bifurcation, où chaque composante ou groupe de composante est successivement divisé en deux embranchement. La manière d’arranger ces balances importe peu, mais on aura avantage à créer des balances interprétables. Le diagramme de balances peut être encodé dans une partition binaire séquentielle (ou sequential bianry partition, sbp). Une sbp est une matrice de contraste ou chaque ligne représente une partition entre deux variables ou groupes de variables. Une composante étiquettée +1 correspondra au groupe du numérateur, une composante étiquettée -1 au dénominateur et une composante étiquettée 0 sera exclue de la partition (Parent et al., 2013). J’ai reformulé la fonction CoDaDendrogram pour que l’on puisse ajouter des informations intéressantes sur les balants horizontaux. Cette fonction est disponible sur github. source(&quot;https://raw.githubusercontent.com/essicolo/AgFun/master/codadend2.R&quot;) sbp &lt;- matrix(c(1, 1,-1, 1,-1, 0), byrow = TRUE, ncol = 3) CoDaDendrogram2(comp, V = gsi.buildilrBase(t(sbp)), ylim = c(0, 1), equal.height = TRUE) Si la SBP est plus imposante, il pourrait être plus aisé de monter dans un chiffrier, puis de l’importer dans R via un fichier csv. Le calcul des ILRs est effectué comme suit. \\[ilr_j = \\sqrt{\\frac{n_j^+ n_j^-}{n_j^+ + n_j^-}} log \\left( \\frac{g \\left( c_j^+ \\right)}{g \\left( c_j^+ \\right)} \\right)\\] ou, à la ligne \\(j\\) de la SBP, \\(n_j^+\\) et \\(n_j^-\\) sont respectivement le nombre de composantes au numérateur et au dénominateur, \\(g \\left( c_j^+ \\right)\\) est la moyenne géométrique des composantes au numérateur et \\(g \\left( c_j^- \\right)\\) est la moyenne géométrique des composantes au dénominateur. Les balances sont conventionnellement notées [A,B | C,D], ou les composantes A et B au dénominateur sont balancées avec les composantes C and D au numérateur. Une balance positive signifie que la moyenne géométrique des concentrations au numérateur est supérieur à celle au dénominateur, et inversement, alors qu’une balance nulle signifie que les moyennes géométriques sont égales (équilibre). Ainsi, en modélisation linéaire, un coefficient positif sur [A,B | C,D] signifie que l’augmentation de l’importance de C et D comparativement à A et B est associé à une augmentation de la variable réponse du modèle. En R, iso_lr &lt;- ilr(comp, V = gsi.buildilrBase(t(sbp))) Notez la forme gsi.buildilrBase(t(sbp)) est une opération pour obtenir la matrice d’orthonormalité à partir de la SBP. Les ILRs sont des balances multivariées sur lesquelles on pourra effectuer des statistiques linéaries. Bien que l’interprétation des résultats comme collection d’interprétations sur des balances univariées pourra être affectée par la structure de la SBP, ni les statistiques linéaires multivariées, ni la distance entre les points ne seront affectés. En effet, chaque variante de la SBP est une rotation (d’un facteur de 60°) par rapport à l’origine: source(&quot;lib/ilr-rotation-sbp.R&quot;) Pour les transformations inverses, vous pourrez utiliser les fonctions alrInv, clrInv et ilrInv. Dans tous les cas, si vous tenez à garder la trace de vos données dans leur format original, vous aurez avantage à ajouter à votre vecteur compositionnel la valeur de remplissage, constitué d’un amalgame des composantes non mesurées. Par exemple, pourc &lt;- c(N = 0.03, P = 0.001, K = 0.01) acomp(pourc) # vous perdez la trace des proportions originales ## N P K ## 0.73170732 0.02439024 0.24390244 ## attr(,&quot;class&quot;) ## [1] acomp pourc &lt;- c(N = 0.03, P = 0.001, K = 0.01) Fv &lt;- 1 - sum(pourc) comp &lt;- acomp(c(pourc, Fv = Fv)) comp ## N P K Fv ## 0.030 0.001 0.010 0.959 ## attr(,&quot;class&quot;) ## [1] acomp iso_lr &lt;- ilr(comp) # avec une sbp par défaut ilrInv(iso_lr) ## 1 2 3 4 ## [1,] 0.03 0.001 0.01 0.959 ## attr(,&quot;class&quot;) ## [1] acomp Si vos données font partie d’un tout, je vous recommande chaudement d’utiliser des méthodes compositionnelles autant pour l’analyse que la modélisation. Pour en savoir davantage, le livre Compositional data analysis with R, de van den Boogart et Tolosana-Delgado, est disponible en format électronique à la bibliothèque de l’Université Laval. Pour aller plus loin, j’ai écri un billet à ce sujet (auquel à ce jour il manque toujours un cas d’étude): We should use balances and machine learning to diagnose ionomes. 7.3.2 Acquérir des données météo Une tâche commune en écologie est de lier des observations à la météo… qui sont rarement collectés lors d’expériences. Environnement Canada possède sont réseau de stations. Les données sont disponibles sur internet en libre accès. Vous pouvez chercher des stations, effectuer des requêtes et télécharger des fichiers csv. Pour un petit tableau, la tâche est plutôt triviale. Mais ça devient rapidement laborieux à mesure que l’on doit rechercher de nombreuses données. Le module weathercan, développé par Steffi LaZerte, permet d’effectuer des requêtes rapidement à partir des coordonnées de votre site expérimental. Par exemple, si je cherche une station météo sfournissant des données horaires situé à moins de 20 km du sommet du Mont-Bellevue, à Sherbrooke, aux coordonnées [latitude 45.35, longitude -71.90], library(&quot;weathercan&quot;) station_site &lt;- stations_search(coords = c(45.35, -71.90), dist = 20, interval = &quot;hour&quot;) station_site ## # A tibble: 4 x 14 ## prov station_name station_id climate_id WMO_id TC_id lat lon elev ## &lt;fct&gt; &lt;chr&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 QC LENNOXVILLE 5397 7024280 71611 WQH 45.4 -71.8 181 ## 2 QC SHERBROOKE 48371 7028123 71610 YSC 45.4 -71.7 241. ## 3 QC SHERBROOKE A 5530 7028124 71610 YSC 45.4 -71.7 241. ## 4 QC SHERBROOKE A 30171 7028126 &lt;NA&gt; GSC 45.4 -71.7 241. ## # … with 5 more variables: tz &lt;chr&gt;, interval &lt;chr&gt;, start &lt;int&gt;, ## # end &lt;int&gt;, distance &lt;dbl&gt; Je prends en note l’identifiant de la station désirée (ou des stations, disons 5397 et 48371), puis je lance une requête pour obtenir la météo horaire entre les dates désirées. mont_bellevue &lt;- weather_dl(station_ids = c(5397, 48371), start = &quot;2019-02-01&quot;, end = &quot;2019-02-07&quot;, interval = &quot;hour&quot;, verbose = TRUE, tz_disp = &quot;Etc/GMT+5&quot;) ## Getting station: 5397 ## Formatting station data: 5397 ## Adding header data: 5397 ## Getting station: 48371 ## Formatting station data: 48371 ## Adding header data: 48371 ## Trimming missing values before and after mont_bellevue %&gt;% head(5) ## # A tibble: 5 x 35 ## station_name station_id station_operator prov lat lon elev ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 LENNOXVILLE 5397 Environment and… QC 45.4 -71.8 181 ## 2 LENNOXVILLE 5397 Environment and… QC 45.4 -71.8 181 ## 3 LENNOXVILLE 5397 Environment and… QC 45.4 -71.8 181 ## 4 LENNOXVILLE 5397 Environment and… QC 45.4 -71.8 181 ## 5 LENNOXVILLE 5397 Environment and… QC 45.4 -71.8 181 ## # … with 28 more variables: climate_id &lt;chr&gt;, WMO_id &lt;chr&gt;, TC_id &lt;chr&gt;, ## # date &lt;date&gt;, time &lt;dttm&gt;, year &lt;chr&gt;, month &lt;chr&gt;, day &lt;chr&gt;, ## # hour &lt;chr&gt;, weather &lt;chr&gt;, hmdx &lt;dbl&gt;, hmdx_flag &lt;chr&gt;, ## # pressure &lt;dbl&gt;, pressure_flag &lt;chr&gt;, rel_hum &lt;dbl&gt;, ## # rel_hum_flag &lt;chr&gt;, temp &lt;dbl&gt;, temp_dew &lt;dbl&gt;, temp_dew_flag &lt;chr&gt;, ## # temp_flag &lt;chr&gt;, visib &lt;dbl&gt;, visib_flag &lt;chr&gt;, wind_chill &lt;dbl&gt;, ## # wind_chill_flag &lt;chr&gt;, wind_dir &lt;dbl&gt;, wind_dir_flag &lt;chr&gt;, ## # wind_spd &lt;dbl&gt;, wind_spd_flag &lt;chr&gt; Et voilà. mont_bellevue %&gt;% ggplot(aes(x = time, y = temp)) + geom_line(aes(colour = station_name)) 7.3.3 Pédométrie avec R Cette section a été écrite par Michael Leblanc. Plusieurs fonctionnalités ont été développées sur R afin d’aider les pédométriciens à visualiser, explorer et traiter les données numériques en science des sols. Voici quelques exemples. 7.3.3.1 Texture du sol La texture du sol est définie par sa composition granulométrique, habituellement représentée par trois fractions (sable, limon, argile), laquelle peut être généralisée en classe texturale. La définition des classes texturales diffère d’un système ou d’un pays à l’autre comme en témoigne l’article Perdus dans le triangle des textures (Richer de Forges et al. 2008). La définition des fractions granulométriques peut également différer selon le domaine d’étude (ingénierie, pédologie) ou le pays. Par exemple, le diamètre du limon est de 0,002 mm à 0,05 mm dans le système canadien, américain et français alors qu’il est de 0,002 mm à 0,02 mm dans le système australien et de 0,002 mm à 0,063 mm dans le système allemand. Il est donc important de vérifier la méthodologie et le système de classification utilisés pour interpréter les données de texture du sol. Le module soilTexture propose des fonctions permettant d’aborder ces multiples définitions. library(&quot;soiltexture&quot;) ## soiltexture 1.5.1 (git revision: 4b25ba2). For help type: help(pack=&#39;soiltexture&#39;) 7.3.3.1.1 Les triangles texturaux Avec la fonction TT.plot, vous pouvez présenter vos données granulométriques dans un triangle textural tel que défini par les différents systèmes nationaux. Auparavant, créons un objet comprenant des textures aléatoires. set.seed(848341) # random.org rand_text &lt;- TT.dataset(n=100, seed.val=29) head(rand_text) ## CLAY SILT SAND Z ## 1 54.650857 40.37101 4.978129 13.2477582 ## 2 44.745954 40.81782 14.436221 20.8433109 ## 3 18.192509 48.26752 33.539970 7.1814626 ## 4 17.750492 40.14405 42.105458 -0.2077358 ## 5 65.518360 23.36110 11.120538 10.8656027 ## 6 6.610293 22.45353 70.936173 3.7108567 Avec le module soiltexture, les tableaux de texture doivent inclure les intitullés exactes CLAY, SILT et SAND (notez les majuscules). Les points des textures générées peuvent être portés dans des diagrammes ternaires texturaux de différents systèmes de classification, par exemple le système canadioen et le système USDA. par(mfrow=c(1, 2)) TT.plot(class.sys = &quot;CA.FR.TT&quot;, tri.data = rand_text, col = &quot;blue&quot;) TT.plot(class.sys = &quot;USDA.TT&quot;, tri.data = rand_text, col = &quot;blue&quot;) Les paramètres de la figure (titres, polices, style de la grille, etc.) peuvent être personnalisés avec les arguments TT.plot. 7.3.3.1.2 Les classes texturales La fonction TT.points.in.classes est utile pour désigner la classe texturale à partir des données granulométriques, en spécifiant bien le système de classification désiré. TT.points.in.classes( tri.data = rand_text[1:10, ], # class.sys = &quot;CA.FR.TT&quot;, PiC.type = &quot;t&quot; ) ## [1] &quot;ALi&quot; &quot;ALi&quot; &quot;L&quot; &quot;L&quot; &quot;ALo&quot; &quot;LS&quot; &quot;ALo&quot; &quot;A&quot; &quot;LLi&quot; &quot;LSA&quot; Plusieurs autres fonctions sont proposées par soiltexture afin de visualiser, classifier et transformer les données de texture du sol : Functions in soiltexture. Julien Moeys (2018) propose également le tutoriel The soil texture wizard: a tutorial. 7.3.3.2 Profils de sols Le profil de sols est une entité décrite par une séquence de couches ou d’horizons avec différentes caractéristiques morphologiques. Le module AQP, pour Algorithms for Quantitative Pedology, propose des fonctions de visualisation, d’agrégation et de classification permettant d’aborder la complexité inhérente aux informations pédologiques. 7.3.3.2.1 La visualisation de profils Vous devez d’abord structurer vos données dans un tableau (data.frame) incluant minimalement ces trois colonnes : Identifiant unique du profil (groupes d’horizons) (id) Limites supérieures de l’horizon (top) Limites inférieures de l’horizon (down) Vos données morphologiques, physico-chimiques, etc., sont incluses dans les autres colonnes. Chargeons un fichier pédologique à titre d’exemple. profils &lt;- read_csv(&quot;data/06_pedometric-profile.csv&quot;) ## Parsed with column specification: ## cols( ## id = col_double(), ## horizon = col_character(), ## top = col_double(), ## bottom = col_double(), ## hue = col_character(), ## value = col_double(), ## chroma = col_double(), ## pH.CaCl2 = col_double(), ## C.CNS.pc = col_double() ## ) head(profils) ## # A tibble: 6 x 9 ## id horizon top bottom hue value chroma pH.CaCl2 C.CNS.pc ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Ap1 0 23 10YR 2 3 4.78 2.71 ## 2 1 Ap2 23 34 10YR 2 2 4.74 2.2 ## 3 1 Bfcj 34 46 7.5YR 4 5 4.79 2.4 ## 4 1 BC 46 83 2.5Y 4 5 4.93 0.22 ## 5 1 C 83 100 2.5Y 5 4 4.82 0.18 ## 6 2 Ap 0 29 10YR 2 2 4.6 4.22 La fonction munsell2rgb permet de convertir le code de couleur Munsell en format RGB. library(&quot;aqp&quot;) ## This is aqp 1.17 ## ## Attaching package: &#39;aqp&#39; ## The following object is masked from &#39;package:plotly&#39;: ## ## slice ## The following objects are masked from &#39;package:dplyr&#39;: ## ## slice, union ## The following object is masked from &#39;package:base&#39;: ## ## union profils$soil_color &lt;- with(profils, munsell2rgb(hue, value, chroma)) Préalablement à la visualisation, le tableau est transformé en objet SoilProfileCollection par la fonction depths. Pour ce faire, le tableau doit être un pur data.frame, non pas un tibble. profils &lt;- profils %&gt;% as.data.frame() depths(profils) &lt;- id ~ top + bottom La fonction plot détectera le type d’objet et appellera la fonction de visualisation en conséquence. par(mfrow = c(1, 3)) plot(profils, name=&quot;horizon&quot;) title(&#39;Couleur des horizons&#39;, cex.main=1) plot(profils, name=&quot;horizon&quot;, color=&#39;C.CNS.pc&#39;, col.label=&#39;C total (%)&#39;) plot(profils, name=&quot;horizon&quot;, color=&#39;pH.CaCl2&#39;, col.label=&#39;pH CaCl2&#39;) De multiples figures thématiques peuvent être générées afin de représenter les particuliarités des profils. Pour aller plus loin, consultez les guides Introduction to SoilProfileCollection Objects et Generating Sketches from SPC Objects. 7.3.3.2.2 Les plans verticaux (depth functions) Les plans verticaux sont des diagrammes qui permettent d’interpréter les données en fonction de la profondeur. La fonction slab permet le calcul de statistiques descriptives par intervalles de profondeur réguliers, lesquelles permettent de visualiser la variabilité verticale des propriétés des sols. agg &lt;- slab(profils, fm = ~ C.CNS.pc + pH.CaCl2) La visualisation est générée par le module graphique ggplot2 agg %&gt;% ggplot(mapping = aes(x = -top, y = p.q50)) + facet_grid(. ~ variable, scale = &quot;free&quot;) + geom_ribbon(aes(ymin = p.q25, ymax = p.q75), fill = &quot;grey75&quot;, alpha = 0.5) + geom_path() + labs(x = &quot;Profondeur (cm)&quot;, y = &quot;Médiane bordée des 25e and 75e percentiles&quot;) + coord_flip() 7.3.3.2.3 Le regroupement de profils Le calcul des distances de dissimilarité entre les profils avec profile_compare permet la construction de dendrogramme et le regroupement des profils. Notez que nous survolerons au chapitre 8 les concepts de dissimilarité et de partitionnement. library(&quot;cluster&quot;) library(&quot;mvtnorm&quot;) library(&quot;sharpshootR&quot;) d &lt;- profile_compare(profils, vars=c(&#39;C.CNS.pc&#39;, &#39;pH.CaCl2&#39;), k=0, max_d=40) ## Computing dissimilarity matrices from 10 profiles [0.08 Mb] d_diana &lt;- diana(d) plotProfileDendrogram(profils, name=&quot;horizon&quot;, d_diana, scaling.factor = 0.3, y.offset = 5, color=&#39;pH.CaCl2&#39;, col.label=&#39;pH CaCl2&#39;) 7.3.3.2.4 Diagramme de relations entre les horizons Il est possible de visualiser les transitions d’horizon les plus probables dans un groupe de profils de sols. tp &lt;- hzTransitionProbabilities(profils, name=&quot;horizon&quot;) ## Warning: ties in transition probability matrix par(mar = c(0, 0, 0, 0), mfcol = c(1, 2)) plot(profils, name=&quot;horizon&quot;) plotSoilRelationGraph(tp, graph.mode = &quot;directed&quot;, edge.arrow.size = 0.5, edge.scaling.factor = 2, vertex.label.cex = 0.75, vertex.label.family = &quot;sans&quot;) Consultez AQP project pour des présentations, des tutoriels et des exemples de figures qui montrent les nombreuses possibilités du package AQP. 7.3.4 Méta-analyses en R Je conseille les livres Introduction to Meta-Analysis, Meta-analysis with R et Handbook of Meta-analysis in Ecology and Evolution pour les méta-analyses sur des écosystèmes. Le module metafor est un ioncournable pour effectuer des métaanalyses en R. On ne passe pas tout à fait à côté si l’on utilise le module meta, lui-même basé en partie sur metafor. Le module meta a touttefois l’avantage d’être simple d’utilisation. Par exemple, pour une méta-analyse d’une réponse continue, library(&quot;meta&quot;) ## Loading &#39;meta&#39; package (version 4.9-4). ## Type &#39;help(meta)&#39; for a brief overview. ## ## Attaching package: &#39;meta&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## ci meta_data &lt;- read_csv(&quot;https://portal.uni-freiburg.de/imbi/_SUPPRESS_ACCESSRULE/lehre/lehrbuecher/meta-analysis-with-r/dataset02.csv&quot;) ## Parsed with column specification: ## cols( ## author = col_character(), ## Ne = col_double(), ## Me = col_double(), ## Se = col_double(), ## Nc = col_double(), ## Mc = col_double(), ## Sc = col_double() ## ) meta_analyse &lt;- metacont(n.e = Ne, mean.e = Me, sd.e = Se, n.c = Nc, mean.c = Mc, sd.c = Sc, data = meta_data, sm = &quot;SMD&quot;) meta_analyse ## SMD 95%-CI %W(fixed) %W(random) ## 1 -0.5990 [-1.3300; 0.1320] 3.5 5.7 ## 2 -0.9518 [-1.6770; -0.2266] 3.6 5.7 ## 3 -0.5909 [-1.6301; 0.4483] 1.7 4.1 ## 4 -0.7064 [-1.7986; 0.3858] 1.6 3.9 ## 5 -0.2815 [-0.6076; 0.0445] 17.6 8.1 ## 6 -0.5375 [-1.0816; 0.0065] 6.3 6.8 ## 7 -1.3204 [-2.1896; -0.4513] 2.5 4.9 ## 8 -0.4800 [-1.3514; 0.3914] 2.5 4.9 ## 9 0.0918 [-0.2549; 0.4385] 15.6 8.0 ## 10 -3.2433 [-4.2035; -2.2831] 2.0 4.5 ## 11 0.0000 [-0.7427; 0.7427] 3.4 5.6 ## 12 -0.7061 [-1.2020; -0.2102] 7.6 7.1 ## 13 -0.4724 [-1.2537; 0.3089] 3.1 5.4 ## 14 -0.1849 [-0.5071; 0.1373] 18.0 8.2 ## 15 -0.0265 [-0.6045; 0.5515] 5.6 6.6 ## 16 -1.1648 [-2.0828; -0.2468] 2.2 4.7 ## 17 -0.2127 [-0.9651; 0.5397] 3.3 5.6 ## ## Number of studies combined: k = 17 ## ## SMD 95%-CI z p-value ## Fixed effect model -0.3915 [-0.5283; -0.2548] -5.61 &lt; 0.0001 ## Random effects model -0.5858 [-0.8703; -0.3013] -4.04 &lt; 0.0001 ## ## Quantifying heterogeneity: ## tau^2 = 0.2309; H = 1.91 [1.50; 2.43]; I^2 = 72.5% [55.4%; 83.1%] ## ## Test of heterogeneity: ## Q d.f. p-value ## 58.27 16 &lt; 0.0001 ## ## Details on meta-analytical method: ## - Inverse variance method ## - DerSimonian-Laird estimator for tau^2 ## - Hedges&#39; g (bias corrected standardised mean difference) Et pour effectuer un forest plot, forest(meta_analyse) 7.3.5 Créer des applications avec R RStudio vous permet de déployer vos résultats sous forme d’applications web grâce à son module shiny. Pour ce faire, le seul préalable est de savoir programmer en R. En agençant une interface avec des inputs (listes de sélection, des boîtes de dialogue, des sélecteurs, des boutons, etc.) avec des modèles que vous développez, vous pourrez créer des interfaces intéractives. Pour créer une application shiny, vous devez créer une partie pour l’interface (ui) et une autre pour le calcul (server). Je n’irai pas dans les détails, étant donnée qu’il s’agit d’un sujet à part entière. Pour aller plus loin, visitez le site du projet shiny. library(&quot;shiny&quot;) ui &lt;- basicPage( sliderInput(&quot;A&quot;, &quot;Asymptote:&quot;, min = 0, max = 100, value = 50), sliderInput(&quot;E&quot;, &quot;Environnement:&quot;, min = -10, max = 100, value = 20), sliderInput(&quot;R&quot;, &quot;Taux:&quot;, min = 0, max = 0.1, value = 0.035), sliderInput(&quot;prix_dose&quot;, &quot;Prix dose:&quot;, min = 0, max = 5, value = 1), sliderInput(&quot;prix_vente&quot;, &quot;Prix vente:&quot;, min = 0, max = 200, value = 100), sliderInput(&quot;dose&quot;, &quot;Dose:&quot;, min = 0, max = 300, value = c(0, 200)), plotOutput(&quot;distPlot&quot;) ) server &lt;- function(input, output) { mitsch_f &lt;- reactive({ input$A * (1 - exp(-input$R * (seq(input$dose[1], input$dose[2], length = 100) + input$E))) }) mitsch_opt &lt;- reactive({ (log((input$A * input$R * input$prix_vente) / input$prix_dose - input$E * input$R) / input$R ) }) output$distPlot &lt;- renderPlot({ plot(seq(input$dose[1], input$dose[2], length = 100), mitsch_f(), type = &quot;l&quot;, ylim = c(0, 100)) abline(v = mitsch_opt() ) text(mitsch_opt(), 2, paste(&quot;Dose optimale:&quot;, round(mitsch_opt(), 0))) }) } shinyApp(ui, server) Une fois l’application créée, il est possible de la déployer sur le site shninyapps.io. D’abord créer une application shiny dans RStudio: File &gt; New File &gt; Shiny Web App. Écrivez votre code dans le fichier app.R (dans ce cas, ce peut être un copier-coller), puis cliquez sur Run App en haut à droite de la fenêtre d’édition du code. Lorsque l’application fonctionne, vous pourrez la publier via RStudio en cliquant sur le bouton Publish dans la fenêtre Viewer (vous devez au préalable avoir un comte sur shinyapp.io). Une application sera publique et sera ouverte. https://essicolo.shinyapps.io/Mitscherlich/ Pour déployer en mode privé, vous devrez débourser pour un forfait ou installer votre propre serveur. "],
["chapitre-ordination.html", "8 Association, partitionnement et ordination 8.1 Espaces d’analyse 8.2 Analyse d’association 8.3 Partitionnement 8.4 Ordination", " 8 Association, partitionnement et ordination ️ Objectifs spécifiques: À la fin de ce chapitre, vous serez en mesure d’effectuer des calculs permettant de mesurer des différence entre des observations, des groupes d’observation ou des variables observées serez en mesure d’effection des analyses de partitionnement hiérarchiques et non-hiérarchiques serez en mesure d’effectuer des calculs d’ordination à l’aide des techniques de réduction d’axe communes: analyse en composante principale, l’analyse de correspondance, l’analyse en coordonnées principales, analyse discriminante linéaire, l’analyse de redondance et l’analyse canonique des correspondances. Les données écologiques incluent généralement plusieurs variables qui doivent être analysées conjointement. Les techniques pour l’analyse multivariée de données écologiques ont grandi en nombre et en complexité, laissant émerger l’écologie numérique, un nouveau domaine d’étude scientifique initié par Pierre Legendre et Louis Legendre dont l’ouvrage Numerical Ecology, aujourd’hui à sa troisième édition, reste un incontournable pour qui s’intéresse aux mathématiques sous-jacentes au domaine. Pour la rédaction de ces notes, c’est toutefois le livre Numerical ecology with R, écrit par Borcard et al. (2011) pour offrir un guide à qui voudrait une approche plus appliquée. L’écologie numérique sera effleurée dans ce chapitre, qui introduit à trois concepts. Les associations permettent de quantifier la ressemblance ou la différence entre deux observation (échantillons) ou variables (descripteurs). Lorsque l’on a plus de deux variables ou plus de deux site, nous obtenons des matrices d’association. Le partitionnement permet de regrouper des observations ou des variables selon des métriques d’association. L’ordination vise par l’intermédiaire de techniques de réduction d’axe à mettre de l’ordre dans des données dont le nombre élevé de variables peut amener à des difficultés d’appréciation et d’interprétaion. library(&quot;tidyverse&quot;) 8.1 Espaces d’analyse 8.1.1 Abondance et occurence L’abondance est le décompte d’espèces observées, tandis que l’occurence est la présence ou l’absence d’une espèce. Le tableau suivant contient des données d’abondance. abundance &lt;- tibble(&#39;Bruant familier&#39; = c(1, 0, 0, 3), &#39;Citelle à poitrine rousse&#39; = c(1, 0, 0, 0), &#39;Colibri à gorge rubis&#39; = c(0, 1, 0, 0), &#39;Geai bleu&#39; = c(3, 2, 0, 0), &#39;Bruant chanteur&#39; = c(1, 0, 5, 2), &#39;Chardonneret&#39; = c(0, 9, 6, 0), &#39;Bruant à gorge blanche&#39; = c(1, 0, 0, 0), &#39;Mésange à tête noire&#39; = c(20, 1, 1, 0), &#39;Jaseur boréal&#39; = c(66, 0, 0, 0)) Ce tableau peut être rapidement transformé en données d’occurence, qui ne comprennent que l’information booléenne de présence (noté 1) et d’absence (noté 0). occurence &lt;- abundance %&gt;% transmute_all(~if_else(. &gt; 0, 1, 0)) L’espace des espèces (ou des variables ou descripteurs) est celui où les espèces forment les axes et où les sites sont positionnés dans cet espace. Il s’agit d’une perspective en mode R, qui permet principalement d’identifier quels espèces se retrouvent plus courrament ensemble. abundance %&gt;% select(&quot;Bruant chanteur&quot;, &quot;Chardonneret&quot;, &quot;Mésange à tête noire&quot;) ## # A tibble: 4 x 3 ## `Bruant chanteur` Chardonneret `Mésange à tête noire` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 20 ## 2 0 9 1 ## 3 5 6 1 ## 4 2 0 0 Dans l’espace des sites (ou les échantillons ou objets), on transpose la matrice d’abondance. On passe ici en mode Q, où chaque point est une espèce, et où l’on peut observer quels échantillons sont similaires. abundance %&gt;% t() ## [,1] [,2] [,3] [,4] ## Bruant familier 1 0 0 3 ## Citelle à poitrine rousse 1 0 0 0 ## Colibri à gorge rubis 0 1 0 0 ## Geai bleu 3 2 0 0 ## Bruant chanteur 1 0 5 2 ## Chardonneret 0 9 6 0 ## Bruant à gorge blanche 1 0 0 0 ## Mésange à tête noire 20 1 1 0 ## Jaseur boréal 66 0 0 0 8.1.2 Environnement L’espace de l’environnement comprend souvent un autre tableau contenant l’information sur l’environnement où se trouve les espèces: les coordonnées et l’élévation, la pente, le pH du sol, la pluviométrie, etc. 8.2 Analyse d’association Nous utiliserons le terme association comme une mesure pour quantifier la ressemblance ou la différence entre deux objets (échantillons) ou variables (descripteurs). Alors que la corrélation et la covariance sont des mesures d’association entre des variables (analyse en mode R), la similarité et la distance sont deux types de une mesure d’association entre des objets (analyse en mode Q). Une distance de 0 est mesurée chez deux objets identiques. La distance augmente au fur et à mesure que les objets sont dissociés. Une similarité ayant une valeur de 0 indique aucune association, tandis qu’une valeur de 1 indique une association parfaite. À l’opposé, la dissimilarité est égale à 1-similarité. La distance peut être liée à la similarité par la relation: \\[distance=\\sqrt{1-similarité}\\] ou \\[distance=\\sqrt{dissimilarité}\\] La racine carrée permet, pour certains indices de similarité, d’obtenir des propriétés euclédiennes. Pour plus de détails, voyez le tableau 7.2 de Legendre et Legendre (2012). Les matrices d’association sont généralement présentées comme des matrices carrées, dont les dimensions sont égales au nombre d’objets (mode Q) ou de vrariables (mode R) dans le tableau. Chaque élément (“cellule”) de la matrice est un indice d’association entre un objet (ou une variable) et un autre. Ainsi, la diagonale de la matrice est un vecteur nul (distance ou dissimilarité) ou unitaire (similarité), car elle correspond à l’association entre un objet et lui-même. Puisque l’association entre A et B est la même qu’entre B et A, et puisque la diagonale retourne une valeur convenue, il est possible d’exprimer une matrice d’association en mode “compact”, sous forme de vecteur. Le vecteur d’association entre des objets A, B et C contiendra toute l’information nécessaire en un vecteur de trois chiffres, [AB, AC, BC], plutôt qu’une matrice de dimension \\(3 \\times 3\\). L’impact sur la mémoire vive peut être considérable pour les calculs comprenant de nombreuses dimensions. En R, les calculs de similarité et de distances peuvent être effectués avec le module vegan. La fonction vegdist permet de calculer les indices d’association en forme carrée. Nous verons plus tard les méthodes de mesure de similarité et de distance plus loin. Pour l’instant, utilisons la méthode de Jaccard pour une démonstration sur des données d’occurence. library(&quot;vegan&quot;) vegdist(occurence, method = &quot;jaccard&quot;, diag = TRUE, upper = TRUE) ## 1 2 3 4 ## 1 0.0000000 0.7777778 0.7500000 0.7142857 ## 2 0.7777778 0.0000000 0.6000000 1.0000000 ## 3 0.7500000 0.6000000 0.0000000 0.7500000 ## 4 0.7142857 1.0000000 0.7500000 0.0000000 Remarquez que vegdist retourne une matrice dont la diagonale est de 0 (on l’affiche en spécifiant diag = TRUE). La diagonale est l’association d’un objet avec lui-même. Or la similarité d’un objet avec lui-même devrait être de 1! En fait, par convention vegdist retourne des dissimilarités, non pas des similarités. La matrice de distance serait donc calculée en extrayant la racine carrée des éléments de la matrice de dissimilarité: dissimilarity &lt;- vegdist(occurence, method = &quot;jaccard&quot;, diag = TRUE, upper = TRUE) distance &lt;- sqrt(dissimilarity) distance ## 1 2 3 4 ## 1 0.0000000 0.8819171 0.8660254 0.8451543 ## 2 0.8819171 0.0000000 0.7745967 1.0000000 ## 3 0.8660254 0.7745967 0.0000000 0.8660254 ## 4 0.8451543 1.0000000 0.8660254 0.0000000 Dans le chapitre sur l’analyse compositionnelle, nous avons abordé les significations différentes que peuvent prendre le zéro. L’information fournie par un zéro peut être différente selon les circonstances. Dans le cas d’une variable continue, un zéro signifie généralement une mesure sous le seuil de détection. Deux tissus dont la concentration en cuivre est nulle ont une afinité sous la perspective de la concentration en cuivre. Dans le cas de mesures d’abondance (décompte) ou d’occurence (présence-absence), on pourra décrire comme similaires deux niches écologiques où l’on retrouve une espèce en particulier. Mais deux sites où l’on de retouve pas d’ours polaires ne correspondent pas nécessairement à des niches similaires! En effet, il peut exister de nombreuses raisons écologiques et méthodologiques pour lesquelles l’espèces ou les espèces n’ont pas été observées. C’est le problème des double-zéros (espèces non observées à deux sites), problème qui est amplifié avec les grilles comprenant des espèces rares. La ressemblance entre des objets comprenant des données continues devrait être calculée grâce à des indicateurs symétriques. Inversement, les affinités entre les objets décrits par des données d’abondance ou d’occurence susceptibles de générer des problèmes de double-zéros devraient être évaluées grâce à des indicateurs asymétriques. Un défi supplémentaire arrive lorsque les données sont de type mixte. Nous utiliserons la convention de vegan et nous calculerons la dissimilarité, non pas la similarité. Les mesures de dissimilarité sont calculées sur des données d’abondance ou des données d’occurence. Notons qu’il existe beaucoup de confusion dans la littérature sur la manière de nommer les dissimilarités (ce qui n’est pas le cas des distances, dont les noms sont reconnus). Dans les sections suivantes, nous noterons la dissimilarité avec un \\(d\\) minuscule et la distance avec un \\(D\\) majuscule. 8.2.1 Association entre objets (mode Q) 8.2.1.1 Objets: Abondance La dissimilarité de Bray-Curtis est asymétrique. Elle est aussi appelée l’indice de Steinhaus, de Czekanowski ou de Sørensen. Il est important de s’assurer de bien s’entendre la méthode à laquelle on fait référence. L’équation enlève toute ambiguité. La dissimilarité de Bray-Curtis entre les points A et B est calculée comme suit. \\[d_{AB} = \\frac {\\sum \\left| A_{i} - B_{i} \\right| }{\\sum \\left(A_{i}+B_{i}\\right)}\\] Utilisons vegdist pour générer les matrices d’association. Le format “liste” de R est pratique pour enregistrer la collection d’objets, dont les matrice d’association que nous allons créer dans cette section. associations_abund &lt;- list() associations_abund[[&#39;BrayCurtis&#39;]] &lt;- vegdist(abundance, method = &quot;bray&quot;) associations_abund[[&#39;BrayCurtis&#39;]] ## 1 2 3 ## 2 0.9433962 ## 3 0.9619048 0.4400000 ## 4 0.9591837 1.0000000 0.7647059 La dissimilarité de Bray-Curtis est souvent utilisée dans la littérature. Toutefois, la version originale de Bray-Curtis n’est pas tout à fait métrique (semimétrique). Conséquemment, la dissimilarité de Ruzicka (une variante de la dissimilarité de Jaccard pour les données d’abondance) est métrique, et devrait probablement être préféré à Bary-Curtis (Oksanen, 2006). \\[d_{AB, Ruzicka} = \\frac { 2 \\times d_{AB, Bray-Curtis} }{1 + d_{AB, Bray-Curtis}}\\] associations_abund[[&#39;Ruzicka&#39;]] &lt;- associations_abund[[&#39;BrayCurtis&#39;]] * 2 / (1 + associations_abund[[&#39;BrayCurtis&#39;]]) La dissimilarité de Kulczynski (aussi écrit Kulsinski) est asymétrique et semimétrique, tout comme celle de Bray-Curtis. Elle est calculée comme suit. \\[d_{AB} = 1-\\frac{1}{2} \\times \\left[ \\frac{\\sum min(A_i, B_i)}{\\sum A_i} + \\frac{\\sum min(A_i, B_i)}{\\sum B_i} \\right]\\] associations_abund[[&#39;Kulczynski&#39;]] &lt;- vegdist(abundance, method = &quot;kulczynski&quot;) Une approche commune pour mesurer l’association entre sites décrits par des données d’abondance est la distance de Hellinger. Notez qu’il s’agit ici d’une distance, non pas d’une dissimilarité. Pour l’obtenir, on doit d’abord diviser chaque donnée d’abondance par l’abondance totale pour chaque site pour obtenir les espèces en tant que proportions, puis on extrait la racine carrée de chaque élément. Enfin, on calcule la distance euclidienne entre les proportions de chaque site. Pour rappel, une distance euclidienne est la généralisation en plusieurs dimensions du théorème de Pythagore, \\(c = \\sqrt{a^2 + b^2}\\). \\[D_{AB} = \\sqrt {\\sum \\left( \\frac{A_i}{\\sum A_i} - \\frac{B_i}{\\sum B_i} \\right)^2}\\] 😱 Attention La distance d’Hellinger hérite des biais liées aux données compositionnelles. Elle peut être substitiée par une matrice de distances d’Aitchison. associations_abund[[&#39;Hellinger&#39;]] &lt;- dist(decostand(abundance, method=&quot;hellinger&quot;)) Toute comme la distance d’Hellinger, la distance de chord est calculée par une distance euclidienne sur des données d’abondance transformées de sorte que chaque ligne ait une longueur (norme) de 1. associations_abund[[&#39;Chord&#39;]] &lt;- dist(decostand(abundance, method=&quot;normalize&quot;)) La métrique du chi-carré, ou \\(\\chi\\)-carré, ou chi-square, donne davantage de poids aux espèces rares qu’aux espèces communes. Son utilisation est recommandée lorsque les espèces rares sont de bons indicateurs de conditions écologiques particulières (Legendre et Legendre, 2012, p. 308). \\[ d_{AB} = \\sqrt{\\sum _j \\frac{1}{\\sum y_j} \\left( \\frac{A_j}{\\sum A} - \\frac{B_j}{\\sum B} \\right)^2 } \\] La métrique peut être transformée en distance en la multipliant par la racine carrée de la somme totale des espèces dans la matric d’abondance (\\(X\\)). \\[ D_{AB} = \\sqrt{\\sum X} \\times d_{AB} \\] associations_abund[[&#39;ChiSquare&#39;]] &lt;- dist(decostand(abundance, method=&quot;chi.square&quot;)) Une mannière visuellement plus intéressante de présenter une matrice d’association est un graphique de type heatmap. associations_abund_df &lt;- list() for (i in 1:length(associations_abund)) { associations_abund_df[[i]] &lt;- data.frame(as.matrix(associations_abund[[i]])) colnames(associations_abund_df[[i]]) &lt;- rownames(associations_abund_df[[i]]) associations_abund_df[[i]]$row &lt;- rownames(associations_abund_df[[i]]) associations_abund_df[[i]] &lt;- associations_abund_df[[i]] %&gt;% gather(key=row) associations_abund_df[[i]]$column = rep(1:4, 4) associations_abund_df[[i]]$dist &lt;- names(associations_abund)[i] } associations_abund_df &lt;- do.call(rbind, associations_abund_df) ggplot(associations_abund_df, aes(x=row, y=column)) + facet_wrap(. ~ dist, nrow = 2) + geom_tile(aes(fill = value)) + geom_text(aes(label = round(value, 2))) + scale_fill_gradient2(low = &quot;#00ccff&quot;, mid = &quot;#aad400&quot;, high = &quot;#ff0066&quot;, midpoint = 2) + labs(x=&quot;Site&quot;, y=&quot;Site&quot;) Peu importe le type d’association utilisée, les heatmaps montrent les mêmes tendances. Les assocaitions de dissimilarité (Bray-Curtis, Kulczynski et Ruzicka) s’étalent de 0 à 1, tandis que les distances (Chi-Square, Chord et Hellinger) partent de zéro, mais n’ont pas de limite supérieure. On note les plus grandes différences entre les sites 2 et 4, tandis que les sites 2 et 3 sont les plus semblables pour toutes les mesures d’association à l’exception de la dissimilarité de Kulczynski. 8.2.1.2 Objets: Occurence (présence-absence) Des indices d’association différents devraient être utilisés lorsque des données sont compilées sous forme booléenne. En général, les tableaux de données d’occurence seront compilés avec des 1 (présence) et des 0 (absence). La similarité de Jaccard entre le site A et le site B est la proportion de double 1 (présences de 1 dans A et B) parmi les espèces. La dissimilarié est la proportion complémentaire (comprenant [1, 0], [0, 1] et [0, 0]). La distance de Jaccard est la racine carrée de la dissimilarité. associations_occ &lt;- list() associations_occ[[&#39;Jaccard&#39;]] &lt;- vegdist(occurence, method = &quot;jaccard&quot;) Les distances d’Hellinger, de chord et de chi-carré sont aussi appropriées pour les calculs de distances sur des tableaux d’occurence. associations_occ[[&#39;Hellinger&#39;]] &lt;- dist(decostand(occurence, method=&quot;hellinger&quot;)) associations_occ[[&#39;Chord&#39;]] &lt;- dist(decostand(occurence, method=&quot;normalize&quot;)) associations_occ[[&#39;ChiSquare&#39;]] &lt;- dist(decostand(occurence, method=&quot;chi.square&quot;)) Graphiquement, associations_occ_df &lt;- list() for (i in 1:length(associations_occ)) { associations_occ_df[[i]] &lt;- data.frame(as.matrix(associations_occ[[i]])) colnames(associations_occ_df[[i]]) &lt;- rownames(associations_occ_df[[i]]) associations_occ_df[[i]]$row &lt;- rownames(associations_occ_df[[i]]) associations_occ_df[[i]] &lt;- associations_occ_df[[i]] %&gt;% gather(key=row) associations_occ_df[[i]]$column = rep(1:4, 4) associations_occ_df[[i]]$dist &lt;- names(associations_occ)[i] } associations_occ_df &lt;- do.call(rbind, associations_occ_df) ggplot(associations_occ_df, aes(x=row, y=column)) + facet_wrap(. ~ dist) + geom_tile(aes(fill = value)) + geom_text(aes(label = round(value, 2))) + scale_fill_gradient2(low = &quot;#00ccff&quot;, mid = &quot;#aad400&quot;, high = &quot;#ff0066&quot;, midpoint = 1) + labs(x=&quot;Site&quot;, y=&quot;Site&quot;) Il est attendu que les matrices d’association sur l’occurence sont semblables à celles sur l’abondance. Dans ce cas-ci, la distance d’Hellinger donne des résultats semblables à la dissimilarité de Jaccard. 8.2.1.3 Objets: Données quantitatives Les données quantitative en écologie peuvent décrire l’état de l’environnement: le climat, l’hydrologie, l’hydrogéochimie, la pédologie, etc. En règle générale, les coordonnées des sites ne sot pas des variables environnementales, à que l’on soupçonne la coordonnée elle-même d’être responsable d’effets sur notre système: mais il s’agira la plupart du temps d’effets confondants (par exemple, on peut mesurer un effet de lattitude sur le rendement des agrumes, mais il s’agira probablement avant tout d’effets dus aux conditions climatiques, qui elles changent en fonction de la lattitude). D’autre types de données quantitative pouvant être appréhendées par des distances sont les traits phénologiques, les ionomes, les génomes, etc. La distance euclidienne est la racine carrée de la somme des carrés des distances sur tous les axes. Il s’agit d’une application multidimensionnelle du théorème de Pythagore. La distance d’Aitchison, couverte dans le chapitre 7, est une distance euclidienne calculée sur des données compositionnelles préalablement transformées. La distance euclidienne est sensible aux unités utilisés: utiliser des milimètres plutôt que des mètres enflera la distance euclidienne. Il est recommandé de porter une attention particulière aux unités, et de standardiser les données au besoin (par exemple, en centrant la moyenne à zéro et en fixant l’écart-type à 1). On pourrait, par exemple, mesurer la distance entre des observations des dimensions de différentes espèces d’iris. Ce tableau est inclu dans R par défaut. data(iris) iris %&gt;% sample_n(5) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 6.2 2.2 4.5 1.5 versicolor ## 2 5.9 3.2 4.8 1.8 versicolor ## 3 7.7 2.6 6.9 2.3 virginica ## 4 5.5 3.5 1.3 0.2 setosa ## 5 4.4 2.9 1.4 0.2 setosa Les mesures du tableau sont en centimètres. Pour éviter de donner davantage de poids aux longueur des sépales et en même temps de négliger la largeur des pétales, nous allons standardiser le tableau. iris_sc &lt;- iris %&gt;% select(-Species) %&gt;% scale(.)%&gt;% as_tibble(.) %&gt;% mutate(Species = iris$Species) iris_sc ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 -0.898 1.02 -1.34 -1.31 setosa ## 2 -1.14 -0.132 -1.34 -1.31 setosa ## 3 -1.38 0.327 -1.39 -1.31 setosa ## 4 -1.50 0.0979 -1.28 -1.31 setosa ## 5 -1.02 1.25 -1.34 -1.31 setosa ## 6 -0.535 1.93 -1.17 -1.05 setosa ## 7 -1.50 0.786 -1.34 -1.18 setosa ## 8 -1.02 0.786 -1.28 -1.31 setosa ## 9 -1.74 -0.361 -1.34 -1.31 setosa ## 10 -1.14 0.0979 -1.28 -1.44 setosa ## # … with 140 more rows Pour les comparaisons des dimensions, prenons la moyenne des dimensions (mises à l’échelle) par espèce. iris_means &lt;- iris_sc %&gt;% group_by(Species) %&gt;% summarise_all(mean) %&gt;% select(-Species) iris_means ## # A tibble: 3 x 4 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -1.01 0.850 -1.30 -1.25 ## 2 0.112 -0.659 0.284 0.166 ## 3 0.899 -0.191 1.02 1.08 Nous pouvons utiliser la distance euclidienne, commune en géométrie, pour comparer les espèces. La distance euclidienne est calculée comme suit. \\[ \\mathcal{E} = \\sqrt{\\Sigma_i \\left( A_i - B_i \\right) ^2 } \\] associations_cont = list() associations_cont[[&#39;Euclidean&#39;]] &lt;- dist(iris_sc %&gt;% select(-Species), method=&quot;euclidean&quot;) La distance de Mahalanobis est semblable à la distance euclidienne, mais qui tient compte de la covariance de la matrice des objets. Cette covariance peut être utilisée pour décrire la structure d’un nuage de points. La diastance de Mahalanobis se calcule comme suit. \\[\\mathcal{M} = \\sqrt{(A - B)^T S^{-1} (A-B)}\\] Notez qu’il s’agit d’une généralisation de la distance euclidienne, qui équivaut à une distance de Mahalanobis dont la matrice de covariance est une matrice identité. La distance de Mahalanobis permet de représenter des distances dans un espace fortement corrélé. Elle est courramment utilisée pour détecter les valeurs aberrantes selon des critères de distance à partir du centre d’un jeu de données multivariées. associations_cont[[&#39;Mahalanobis&#39;]] &lt;- vegdist(iris_sc %&gt;% select(-Species), &#39;mahalanobis&#39;) La distance de Manhattan porte aussi le nom de distance de cityblock ou de taxi. C’est la distance que vous devrez parcourir pour vous rendre du point A au point B à Manhattan, c’est-à-dire selon une séquence de tronçons perpendiculaires. \\[ D_{AB} = \\sum _i \\left| A_i - B_i \\right| \\] La distance de Manhattan est appropriée lorsque les gradients (changements d’un état à l’autre ou d’une région à l’autre) ne permettent pas des changements simultanés. Mieux vaut standardiser les variables pour éviter qu’une dimension soit prépondérante. associations_cont[[&#39;Manhattan&#39;]] &lt;- vegdist(iris_sc %&gt;% select(-Species), &#39;manhattan&#39;) Avant de présenter les résultats des espèces d’iris, voici une représentation des distances euclidiennes (rouge), de Mahalanobis (bleu) et de Manhattan (vert), chacune de 1 et 2 unités à partir du centre et, pour ce qui est de la distance de Mahalanobis, selon la covariance. library(&quot;car&quot;) ## Loading required package: carData ## ## Attaching package: &#39;car&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## recode ## The following object is masked from &#39;package:purrr&#39;: ## ## some library(&quot;MASS&quot;) ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:plotly&#39;: ## ## select ## The following object is masked from &#39;package:dplyr&#39;: ## ## select select &lt;- dplyr::select # éviter les conflits de fonctions entre MASS et dplyr filter &lt;- dplyr::filter sigma &lt;- matrix(c(1, 0.6, 0.6, 1), ncol = 2) # matrice de covariance mu &lt;- c(0, 0) # centre data &lt;- mvrnorm(n = 100, mu, sigma) # générer des données plot(data, ylim = c(-2, 2), xlim = c(-2, 2), asp = 1) ## cercles t &lt;- seq(0,2*pi,length=100) c1 &lt;- t(rbind(mu[2] + sin(t)*1, mu[1] + cos(t)*1)) c2 &lt;- t(rbind(mu[2] + sin(t)*2, mu[1] + cos(t)*2)) lines(c1, lwd = 2, col = &quot;red&quot;) lines(c2, lwd = 2, col = &quot;red&quot;) ## ellipses e1 &lt;- ellipse(mu, sigma, radius=1, add=TRUE) e2 &lt;- ellipse(mu, sigma, radius=2, add=TRUE) ## carrés lines(c(1, 0, -1, 0, 1), c(0, 1, 0, -1, 0), lwd = 2, col = &quot;green&quot;) lines(c(2, 0, -2, 0, 2), c(0, 2, 0, -2, 0), lwd = 2, col = &quot;green&quot;) Et, graphiquement, les résultats des distances des iris. associations_cont_df &lt;- list() for (i in 1:length(associations_cont)) { associations_cont_df[[i]] &lt;- data.frame(as.matrix(associations_cont[[i]])) colnames(associations_cont_df[[i]]) &lt;- rownames(associations_cont_df[[i]]) associations_cont_df[[i]]$row &lt;- rownames(associations_cont_df[[i]]) associations_cont_df[[i]] &lt;- associations_cont_df[[i]] %&gt;% gather(key=row) associations_cont_df[[i]]$column = rep(1:nrow(iris), nrow(iris)) associations_cont_df[[i]]$dist &lt;- names(associations_cont)[i] } associations_cont_df &lt;- do.call(rbind, associations_cont_df) ggplot(associations_cont_df, aes(x=row, y=column)) + facet_wrap(. ~ dist) + geom_tile(aes(fill = value), colour = NA) + #geom_text(aes(label = round(value, 2))) + scale_fill_gradient2(low = &quot;#00ccff&quot;, mid = &quot;#aad400&quot;, high = &quot;#ff0066&quot;, midpoint = 5) + labs(x=&quot;Site&quot;, y=&quot;Site&quot;) Le tableau iris est ordonné par espèce. Les distances euclidienne et de Manhattan permettent aisément de distinguer les espèces selon les dimensions des pétales et des sépales. Toutefois, l’utilsation de la covariance avec la distance de Mahalanobis crée des distinction moins tranchées. 8.2.1.4 Objets: Données mixtes Les données catégorielles ordinales peuvent être transformées en données continues par gradations linéaires ou quadratiques. Les données catégorielles nominales, quant à elles, peuvent être encodées (encodage catégoriel) en données similaires à des occurences. Attention toutefois: contrairement à la régression linéaire qui demande d’exclure une catégorie, l’encodage catégoriel doit inclure toutes les catégories. Le comportement par défaut de la fonction model.matrix est d’exclure la catégorie de référence: on doit spécifier que l’intercept est de zéro, c’est-à-dire model.matrix(~ + categorie). La similarité de Gower a été développée pour mesurer des associations entre des objets dont les données sont mixtes: booléennes, catégorielles et continues. La similarité de Gower est calculée en additionnant les distances calculées par colonne, individuellement. Si la colonne est booléenne, on utilise les distances de Jaccard (qui exclue les double-zéro) de manière univariée: une variable à la fois. Pour les variables continues, on utilise la distance de Manhattan divisée par la plage de valeurs de la variable (pour fin de standardisation). Puisqu’elle hérite de la particularité de la distance de Manhattan et de la similarité de Jaccard univariée, la similarité de Gower reste une combinaison linéaire de distances univariées. X &lt;- tibble(ID = 1:8, age = c(21, 21, 19, 30, 21, 21, 19, 30), gender = c(&#39;M&#39;,&#39;M&#39;,&#39;N&#39;,&#39;M&#39;,&#39;F&#39;,&#39;F&#39;,&#39;F&#39;,&#39;F&#39;), civil_status = c(&#39;MARRIED&#39;,&#39;SINGLE&#39;,&#39;SINGLE&#39;,&#39;SINGLE&#39;,&#39;MARRIED&#39;,&#39;SINGLE&#39;,&#39;WIDOW&#39;,&#39;DIVORCED&#39;), salary = c(3000.0,1200.0 ,32000.0,1800.0 ,2900.0 ,1100.0 ,10000.0,1500.0), children = c(TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE), available_credit = c(2200,100,22000,1100,2000,100,6000,2200)) X ## # A tibble: 8 x 7 ## ID age gender civil_status salary children available_credit ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 1 21 M MARRIED 3000 TRUE 2200 ## 2 2 21 M SINGLE 1200 FALSE 100 ## 3 3 19 N SINGLE 32000 TRUE 22000 ## 4 4 30 M SINGLE 1800 TRUE 1100 ## 5 5 21 F MARRIED 2900 TRUE 2000 ## 6 6 21 F SINGLE 1100 TRUE 100 ## 7 7 19 F WIDOW 10000 FALSE 6000 ## 8 8 30 F DIVORCED 1500 TRUE 2200 Il faut préalablement procéder à l’encodage catégoriel pour les variables catégorielles nominales. X_dum &lt;- model.matrix(~ 0 + ., X[, -1]) X_dum ## age genderF genderM genderN civil_statusMARRIED civil_statusSINGLE ## 1 21 0 1 0 1 0 ## 2 21 0 1 0 0 1 ## 3 19 0 0 1 0 1 ## 4 30 0 1 0 0 1 ## 5 21 1 0 0 1 0 ## 6 21 1 0 0 0 1 ## 7 19 1 0 0 0 0 ## 8 30 1 0 0 0 0 ## civil_statusWIDOW salary childrenTRUE available_credit ## 1 0 3000 1 2200 ## 2 0 1200 0 100 ## 3 0 32000 1 22000 ## 4 0 1800 1 1100 ## 5 0 2900 1 2000 ## 6 0 1100 1 100 ## 7 1 10000 0 6000 ## 8 0 1500 1 2200 ## attr(,&quot;assign&quot;) ## [1] 1 2 2 2 3 3 3 4 5 6 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$gender ## [1] &quot;contr.treatment&quot; ## ## attr(,&quot;contrasts&quot;)$civil_status ## [1] &quot;contr.treatment&quot; ## ## attr(,&quot;contrasts&quot;)$children ## [1] &quot;contr.treatment&quot; Calculons la dissimilarité de Gower (cette fois le graphique est fait avec pheatmap). library(&quot;pheatmap&quot;) d_gow &lt;- as.matrix(vegdist(X_dum, &#39;gower&#39;)) colnames(d_gow) &lt;- rownames(d_gow) &lt;- X$ID pheatmap(d_gow) Les dendrogrammes apparaissants sur les axes du graphique sont issus d’un processus de partitionnement basé sur la distance, que nous verrons plus loin dans ce chapiter. Les profils des clients 4 et 7, ainsi que ceux des clients 3 et 7 diffèrent le plus. Les profils 3 et 4 sont néanmoins plutôt différents. 8.2.2 Associations entre variables (mode R) Il existe de nombreuses approches pour mesurer les associations entre variables. La plus connue est la corrélation. Mais les données d’abondance et d’occurence demandent des approches différentes. 8.2.2.1 Variables: Abondance La distance du chi-carré est suggérée par Borcard et al. (2011). abundance_r &lt;- t(abundance) D_chisq_R &lt;- as.matrix(dist(decostand(abundance_r, method=&quot;chi.square&quot;))) pheatmap(D_chisq_R, display_numbers = round(D_chisq_R, 2)) Des coabondances sont notables pour la mésange à tête noire, le jaseur boréal, la citelle à poitrine rousse et le bruant à gorge blanche (tache bleu au centre). 8.2.2.2 Variables: Occurence La dissimilarité de Jaccard peut être utilisée. occurence_r &lt;- t(occurence) D_jacc_R &lt;- as.matrix(vegdist(occurence_r, method = &quot;jaccard&quot;)) pheatmap(D_jacc_R, display_numbers = round(D_jacc_R, 2)) Des cooccurences sont notables pour le jaseur boréal, la citelle à poitrine rousse et le bruant à gorge blanche (tache bleu au centre). 8.2.2.3 Variables: Quantités La matrice des corrélations de Pearson peut être utilisée pour les données continues. Quant aux variables ordinales, elles devraient idéalement être liées linéairement ou quadratiquement. Si ce n’est pas le cas, c’est-à-dire que les catégories sont ordonnées par rang seulement, vous pourrez avoir recours aux coefficients de corrélation de Spearman ou de Kendall. iris_cor &lt;- iris %&gt;% select(-Species) %&gt;% cor(.) pheatmap(cor(iris[, -5]), cluster_rows = FALSE, cluster_cols = FALSE, display_numbers = round(iris_cor, 2)) 8.2.3 Conclusion sur les associations Il n’existe pas de règle claire pour déterminer quelle technique d’association utiliser. Cela dépend en premier lieu de vos données. Vous sélectionnerez votre méthode d’association selon le type de données que vous abordez, la question à laquelle vous désirez répondre ainsi l’expérience dans la littérature comme celle de vos collègues scientifiques. S’il n’existe pas de règle clair, c’est qu’il existe des dizaines de méthodes différentes, et la plupart d’entre elles vous donneront une perspective juste et valide. Il faut néanmoins faire attention pour éviter de sélectionner les méthodes qui ne sont pas appropriées. 8.3 Partitionnement Les données suivantes ont été générées par Leland McInnes (Tutte institute of mathematics, Ottawa). Êtes-vous en mesure d’identifier des groupes? Combien en trouvez-vous? df_mcinnes &lt;- read_csv(&quot;data/clusterable_data.csv&quot;, col_names = c(&quot;x&quot;, &quot;y&quot;), skip = 1) ## Parsed with column specification: ## cols( ## x = col_double(), ## y = col_double() ## ) ggplot(df_mcinnes, aes(x=x, y=y)) + geom_point() + coord_fixed() En 2D, l’oeil humain peut facilement détecter les groupes. En 3D, c’est toujours possible, mais au-delà de 3D, le partitionnement cognitive devient rapidement maladroite. Les algorithmes sont alors d’une aide précieuse. Mais ils transportent en pratique tout un baggage de limitations. Quel est le critère d’association entre les groupes? Combien de groupe devrions-nous créer? Comment distinguer une donnée trop bruitée pour être classifiée? Le partitionnement de données (clustering en anglais), et inversement leur regroupement, permet de créer des ensembles selon des critères d’association. On suppose donc que Le partitionnement permet de créer des groupes selon l’information que l’on fait émerger des données. Il est conséquemment entendu que les données ne sont pas catégorisées à priori: il ne s’agit pas de prédire la catégorie d’un objet, mais bien de créer des catégories à partir des objets par exemple selon leurs dimensions, leurs couleurs, leurs signature chimique, leurs comportements, leurs gènes, etc. Plusieurs méthodes sont aujourd’hui offertes aux analystes pour partitionner leurs données. Dans le cadre de ce manuel, nous couvrirons ici deux grandes tendances dans les algorithmes. Méthodes hiérarchique et non hiérarchiques. Dans un partitionnement hiérarchique, l’ensemble des objets forme un groupe, comprenant des sous-regroupements, des sous-sous-regroupements, etc., dont les objets forment l’ultime partitionnement. On pourra alors identifier comment se décline un partitionnement. À l’inverse, un partitionnement non-hiérarchique des algorhitmes permettent de créer les groupes non hiérarchisés les plus différents que possible. Membership exclusif ou flou. Certaines techniques attribuent à chaque objet une classe unique: l’appartenance sera indiquée par un 1 et la non appartenance par un 0. D’autres techniques vont attribuer un membership flou où le degré d’appartenance est une variable continue de 0 à 1. Parmi les méthodes floues, on retrouve les méthodes probabilistes. 8.3.1 Évaluation d’un partitionnement Le choix d’une technique de partitionnement parmi de nombreuses disponibles, ainsi que le choix des paramètres gouvernant chacune d’entre elles, est avant tout basé sur ce que l’on désire définir comme étant un groupe, ainsi que la manière d’interpréter les groupes. En outre, le nombre de groupe à départager est toujours une décision de l’analyste. Néanmoins, on peut se fier des indicateurs de performance de partitionnement. Parmis ceux-ci, retenons le score silouhette ainsi que l’indice de Calinski-Harabaz. 8.3.1.1 Score silouhette En anglais, le h dans silouhette se trouve après le l: on parle donc de silhouette coefficient pour désigner le score de chacun des objets dans le partitionnement. Pour chaque objet, on calcule la distance moyenne qui le sépare des autres points de son groupe (\\(a\\)) ainsi que la distance moyenne qui le sépare des points du groupe le plus rapproché. \\[s = \\frac{b-a}{max \\left(a, b \\right)}\\] Un coefficient de -1 indique le pire classement, tandis qu’un coefficient de 1 indique le meilleur classement. La moyenne des coefficients silouhette est le score silouhette. 8.3.1.2 Indice de Calinski-Harabaz L’indice de Calinski-Harabaz est proportionnel au ratio des dispersions intra-groupe et la moyenne des dispersions inter-groupes. Plus l’indice est élevé, mieux les groupes sont définis. La mathématique est décrite dans la documentation de scikit-learn, un module d’analyse et autoapprentissage sur Python. Note. Les coefficients silouhette et l’indice de Calinski-Harabaz sont plus appropriés pour les formes de groupes convexes (cercles, sphères, hypersphères) que pour les formes irrégulières (notamment celles obtenues par la DBSCAN, discutée ci-desssous). 8.3.2 Partitionnement non hiérarchique Il peut arriver que vous n’ayez pas besoin de comprendre la structure d’agglomération des objets (ou variables). Plusieurs techniques de partitionnement non hiérarchique sont disponibles sur R. On s’intéressera en particulier aux k-means et au dbscan. 8.3.2.1 Kmeans L’objectif des kmeans est de minimiser la distance euclédienne entre un nombre prédéfini de k groupes exclusifs. L’algorhitme commence par placer une nombre k de centroides au hasard dans l’espace d’un nombre p de variables (vous devez fixer k, et p est le nombre de colonnes de vos données). Ensuite, chaque objet est étiquetté comme appartenant au groupe du centroid le plus près. La position du centroide est déplacée à la moyenne de chaque groupe. Recommencer à partir de l’étape 2 jusqu’à ce que l’assignation des objets aux groupes ne change plus. Source: David Sheehan La technique des kmeans suppose que les groupes ont des distributions multinormales - représentées par des cercles en 2D, des sphères en 3D, des hypersphères en plus de 3D. Cette limitation est problématique lorsque les groupes se présentent sous des formes irrégulières, comme celles du nuage de points de Leland McInnes, présenté plus haut. De plus, la technique classique des kmeans est basée sur des distances euclidiennes: l’utilisation des kmeans n’est appropriée pour les données comprenant beaucoup de zéros, comme les données d’abondance, qui devraient préalablement être transformées en variables centrées et réduites (Legendre et Legendre, 2012). La technique des mixtures gaussiennes (gaussian mixtures) est une généralisation des kmeans permettant d’intégrer la covariance des groupes. Les groupes ne sont plus des hyper-sphères, mais des hyper-ellipsoïdes. 8.3.2.1.1 Application Nous pouvons utilisé la fonction kmeans de R. Toutefois, puisque l’on désire ici effectuer des tests de partitionnement pour plusieurs nombres de groupes, nous utiliserons cascadeKM, du module vegan. Notez que de nombreux paramètres par défaut sont utilisés dans les exécutions ci-dessous. Ces notes de cours ne forment pas un travail de recherche scientifique. Lors de travaux de recherche, l’utilsation d’un argument ou d’un autre dans une fonction doit être justifié: qu’un paramètre soit utilisé par défaut dans une fonction n’est a priori pas une justification convainquante. Pour les kmeans, on doit fixer le nombre de groupes. Le graphique des données de Leland McInnes montrent 6 groupes. Toutefois, il est rare que l’on puisse visualiser des démarquations aussi tranchées que celles de l’exemple, qui plus est dans des cas où l’on doit traiter de plus de deux dimensions. Je vais donc lancer le partitionnement en boucle pour plusieurs nombres de groupes, de 3 à 10 et pour chaque groupe, évaluer le score silouhette et de Calinski-Habaraz. J’utilise un argument random_state pour m’assurer que les groupes seront les mêmes à chaque fois que la cellule sera lancée. library(&quot;vegan&quot;) mcinnes_kmeans &lt;- cascadeKM(df_mcinnes, inf.gr = 3, sup.gr = 10, criterion = &quot;calinski&quot;) str(mcinnes_kmeans) ## List of 4 ## $ partition: int [1:2309, 1:8] 1 1 1 1 1 1 1 1 1 1 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:2309] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. ..$ : chr [1:8] &quot;3 groups&quot; &quot;4 groups&quot; &quot;5 groups&quot; &quot;6 groups&quot; ... ## $ results : num [1:2, 1:8] 85.1 2164.5 61.4 2294.6 51.4 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:2] &quot;SSE&quot; &quot;calinski&quot; ## .. ..$ : chr [1:8] &quot;3 groups&quot; &quot;4 groups&quot; &quot;5 groups&quot; &quot;6 groups&quot; ... ## $ criterion: chr &quot;calinski&quot; ## $ size : int [1:10, 1:8] 1243 505 561 NA NA NA NA NA NA NA ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:10] &quot;Group 1&quot; &quot;Group 2&quot; &quot;Group 3&quot; &quot;Group 4&quot; ... ## .. ..$ : chr [1:8] &quot;3 groups&quot; &quot;4 groups&quot; &quot;5 groups&quot; &quot;6 groups&quot; ... ## - attr(*, &quot;class&quot;)= chr &quot;cascadeKM&quot; L’objet mcinnes_kmeans, de type cascadeKM, peut être visualisé directement avec la fonction plot. plot(mcinnes_kmeans) On obtient un maximum de Calinski à 4 groupes, qui correspons à la deuxième simulation effectuée de 3 à 10. Examinons les scores silouhette (module: cluster). library(&quot;cluster&quot;) asw &lt;- c() for (i in 1:ncol(mcinnes_kmeans$partition)) { mcinnes_kmeans_silhouette &lt;- silhouette(mcinnes_kmeans$partition[, i], dist = vegdist(df_mcinnes, method = &quot;euclidean&quot;)) asw[i] &lt;- summary(mcinnes_kmeans_silhouette)$avg.width } plot(3:10, asw, type = &#39;b&#39;) Le score silouhette maximum est à 3 groupes. La forme des groupes n’étant pas convexe, il fallait s’attendre à ce que indicateurs maximaux pour les deux indicateurs soient différents. C’est d’ailleurs souvent le cas. Cet exemple supporte que le choix du nombre de groupe à départager repose sur l’analyste, non pas uniquement sur les indicateurs de performance. Choisissons 6 groupes, puisque que c’est visuellement ce que l’on devrait chercher pour ce cas d’étude. kmeans_group &lt;- mcinnes_kmeans$partition[, 4] mcinnes_kmeans$partition %&gt;% head(3) ## 3 groups 4 groups 5 groups 6 groups 7 groups 8 groups 9 groups 10 groups ## 1 1 3 2 6 2 3 9 1 ## 2 1 3 5 4 2 8 9 1 ## 3 1 4 2 6 5 3 4 4 df_mcinnes %&gt;% mutate(kmeans_group = kmeans_group) %&gt;% # ajouter une colonne de regoupement ggplot(aes(x=x, y=y)) + geom_point(aes(colour = factor(kmeans_group))) + coord_fixed() L’algorithme kmeans est loin d’être statisfaisant. Cela est attendu, puisque les kmeans recherchent des distribution gaussiennes sur des groupes vraisemblablement non-gaussiens. Nous pouvons créer un graphique silouhette pour nos 6 groupes. Notez qu’à cause d’un bogue, il n’est pas possible de présenter les données clairement lorsqu’elles sont nombreuses. sil &lt;- silhouette(mcinnes_kmeans$partition[, 6], dist = vegdist(df_mcinnes[, ], method = &quot;euclidean&quot;)) sil &lt;- sortSilhouette(sil) plot(sil, col = &#39;black&#39;) 8.3.2.2 DBSCAN La technique DBSCAN (* Density-Based Spatial Clustering of Applications with Noise) sousentend que les groupes sont composés de zones où l’on retrouve plus de points (zones denses) séparées par des zones de faible densité. Pour lancer l’algorithme, nous devons spécifier une mesure d’association critique (distance ou dissimilarité) d* ainsi qu’un nombre de point critique k dans le voisinage de cette distance. L’algorithme commence par étiqueter chaque point selon l’une de ces catégories: Noyau: le point a au moins k points dans son voisinage, c’est-à-dire à une distance inférieure ou égale à d. Bordure: le point a moins de k points dans son voisinage, mais l’un de des points voisins est un noyau. Bruit: le cas échéant. Ces points sont considérés comme des outliers. Les noyaux distancés de d ou moins sont connectés entre eux en englobant les bordures. Le nombre de groupes est prescrit par l’algorithme DBSCAN, qui permet du coup de détecter des données trop bruitées pour être classées. Damiani et al. (2014) a développé une approche utilisant la technique DBSCAN pour partitionner des zones d’escale pour les flux de populations migratoires. 8.3.2.2.1 Application La technique DBSCAN n’est pas basée sur le nombre de groupe, mais sur la densité des points. L’argument x ne constitue pas les données, mais une matrice d’association. L’argument minPts spécifie le nombre minimal de points qui l’on doit retrouver à une distance critique d* pour la formation des *noyaux et la propagation des groupes, spécifiée dans l’argument eps. La distance d peut être estimée en prenant une fraction de la moyenne, mais on aura volontiers recours à sont bon jugement. library(&quot;dbscan&quot;) mcinnes_dbscan &lt;- dbscan(x = vegdist(df_mcinnes[, ], method = &quot;euclidean&quot;), eps = 0.03, minPts = 10) dbscan_group &lt;- mcinnes_dbscan$cluster unique(dbscan_group) ## [1] 1 0 2 6 3 4 5 Les paramètres spécifiés donnent 5 groupes (1, 2, ..., 5) et des points trop bruités pour être classifiés (étiquetés 0). Voyons comment les groupes ont été formés. df_mcinnes %&gt;% mutate(dbscan_group = dbscan_group) %&gt;% # ajouter une colonne de regoupement ggplot(aes(x=x, y=y)) + geom_point(aes(colour = factor(dbscan_group))) + coord_fixed() Le partitionnement semble plus conforme à ce que l’on recherche. Néanmoins, DBSCAN cré quelques petits groupes indésirables (groupe 6, en rose) ainsi qu’un grand groupe (violet) qui auraient lieu d’être partitionné. Ces défaut pourraient être réglés en jouant sur les paramètres eps et minPts. 8.3.3 Partitionnement hiérarchique Les techniques de partitionnement hiérarchique sont basées sur les matrices d’association. La technique pour mesurer l’association (entre objets ou variables) déterminera en grande partie le paritionnement des données. Les partitionnements hiérarchiques ont l’avantage de pouvoir être représentés sous forme de dendrogramme (ou arbre) de partition. Un tel dendrogramme présente des sous-groupes qui se joignent en groupes jusqu’à former un seul ensemble. Le partitionnement hiérarchique est abondamment utilisé en phylogénie, pour étudier les relations de parenté entre organismes vivants, populations d’organismes et espèces. La phénétique, branche empirique de la phylogénèse interspécifique, fait usage du partitionnement hiérarchique à partir d’associations génétiques entre unités taxonomiques. On retrouve de nombreuses ressources académiques en phylogénétique ainsi que des outils pour R et Python. Toutefois, la phylogénétique en particulier ne fait pas partie de la présente ittération de ce manuel. 8.3.3.1 Techniques de partitionnement hiérarchique Le partitionnement hiérarchique est typiquement effectué avec une des quatres méthodes suivantes, dont chacune possède ses particularités, mais sont toutes agglomératives: à chaque étape d’agglomération, on fusionne les deux groupes ayant le plus d’affinité sur la base des deux sous-groupes les plus rapprochés. Single link (single). Les groupes sont agglomérés sur la base des deux points parmi les groupes, qui sont les plus proches. Complete link (complete). À la différence de la méthode single, on considère comme critère d’agglomération les éléments les plus éloignés de chaque groupe. Agglomération centrale. Il s’agit d’une fammlle de méthode basées sur les différences entre les tendances centrales des objets ou des groupes. Average (average). Appelée UPGMA (Unweighted Pair-Group Method unsing Average), les groupes sont agglomérés selon un centre calculés par la moyenne et le nombre d’objet pondère l’agglomération (le poids des groupes est retiré). Cette technique est historiquement utilisée en bioinformatique pour partitionner des groupes phylogénétiques (Sneath et Sokal, 1973). Weighted (weighted). La version de average, mais non pondérée (WPGMA). Centroid (centroid). Tout comme average, mais le centroïde (centre géométrique) est utilisé au lieu de la moyenne. Accronyme: UPGMC. Median (median). Appelée WPGMC. Devinez! ;) Ward (ward). L’optimisation vise à minimiser les sommes des carrés par regroupement. 8.3.3.2 Quel outil de partitionnement hiérarchique utiliser? Alors que le choix de la matrice d’association dépend des données et de leur contexte, la technique de partitionnement hiérarchique peut, quant à elle, être basée sur un critère numérique. Il en existe plusieurs, mais le critère recommandé pour le choix d’une technique de partitionnement hiérarchique est la corrélation cophénétique. La distance cophénétique est la distance à laquelle deux objets ou deux sous-groupes deviennent membres d’un même groupe. La corrélation cophénétique est la corrélation de Pearson entre le vecteur d’association des objets et le vecteur de distances cophénétiques. 8.3.3.3 Application Les techniques de partitionnement hiérarchique présentées ci-dessus sont disponibles dans le module stats de R, qui est chargé automatiquement lors de l’ouversture de R. Nous allons classifier les dimensions des iris grâce à la distance de Manhattan. mcinnes_hclust_distmat &lt;- vegdist(df_mcinnes, method = &quot;manhattan&quot;) clustering_methods &lt;- c(&#39;single&#39;, &#39;complete&#39;, &#39;average&#39;, &#39;centroid&#39;, &#39;ward&#39;) clust_l &lt;- list() coph_corr_l &lt;- c() for (i in seq_along(clustering_methods)) { clust_l[[i]] &lt;- hclust(mcinnes_hclust_distmat, method = clustering_methods[i]) coph_corr_l[i] &lt;- cor(mcinnes_hclust_distmat, cophenetic(clust_l[[i]])) } ## The &quot;ward&quot; method has been renamed to &quot;ward.D&quot;; note new &quot;ward.D2&quot; tibble(clustering_methods, coph_corr = coph_corr_l) %&gt;% ggplot(aes(x = fct_reorder(clustering_methods, -coph_corr), y = coph_corr)) + geom_col() + labs(x = &quot;Méthode de partitionnement&quot;, y = &quot;Corrélation cophénétique&quot;) La méthode average retourne la corrélation la plus élevée. Pour plus de flexibilité, enchâssons le nom de la méthode dans une variable. Ainsi, en chageant le nom de cette variable, le reste du code sera conséquent. names(clust_l) &lt;- clustering_methods best_method &lt;- &quot;average&quot; Le partitionnement hiérarchique peut être visualisé par un dendrogramme. plot(clust_l[[best_method]]) 8.3.3.4 Combien de groupes utiliser? La longueur des lignes verticales est la distance séparant les groupes enfants. Bien que la sélection du nombre de groupe soit avant tout basée sur les besoins du problème, nous pouvons nous appuyer sur certains outils. La hauteur totale peut servir de critère pour définir un nombre de groupes adéquat. On pourra sélectionner le nombre de groupe où la hauteur se stabilise en fonction du nombre de groupe. On pourra aussi utiliser le graphique silhouette, comprenant une collection de largeurs de silouhette, représentant le degré d’appartenance à son groupe. La fonction sklearn.metrics.silhouette_score, du module scikit-learn, s’en occupe. asw &lt;- c() num_groups &lt;- 3:10 for(i in seq_along(num_groups)) { sil &lt;- silhouette(cutree(clust_l[[best_method]], k = num_groups[i]), mcinnes_hclust_distmat) asw[i] &lt;- summary(sil)$avg.width } plot(num_groups, asw, type = &quot;b&quot;) Le nombre optimal de groupes serait de 5. Coupons le dendrorgamme à la hauteur correspondant à 5 groupes avec la fonction cutree. k_opt &lt;- num_groups[which.max(asw)] hclust_group &lt;- cutree(clust_l[[best_method]], k = k_opt) plot(clust_l[[best_method]]) rect.hclust(clust_l[[best_method]], k = k_opt) La classification hiérarchique, uniquement basée sur la distance, peut être inappropriée pour définir des formes complexes. df_mcinnes %&gt;% mutate(hclust_group = hclust_group) %&gt;% # ajouter une colonne de regoupement ggplot(aes(x=x, y=y)) + geom_point(aes(colour = factor(hclust_group))) + coord_fixed() 8.3.4 Partitionnement hiérarchique basée sur la densité des points La tecchinque HDBSCAN, dont l’algorithme est relativement récent (Campello et al., 2013), permet une partitionnement hiérarchique sur le même principe des zones de densité de la technique DBSCAN. Le HDBSCAN a été utilisée pour partitionner les lieux d’escale d’oiseaux migrateurs en Chine (Xu et al., 2013). Avec DBSCAN, un rayon est fixé dans une métrique appropriée. Pour chaque point, on compte le nombre de point voisins, c’est à dire le nombre de point se situant à une distance (ou une dissimilarité) égale ou inférieure au rayon fixé. Avec HDBSCAN, on spécifie le nombre de points devant être recouverts et on calcule le rayon nécessaire pour les recouvrir. Ainsi, chaque point est associé à un rayon critique que l’on nommera \\(d_{noyau}\\). La métrique initiale est ensuite altérée: on remplace les associations entre deux objets A et B par la valeur maximale entre cette association, le rayon critique de A et le rayon critique de B. Cette nouvelle distance est appelée la distance d’atteinte mutuelle: elle accentue les distances pour les points se trouvant dans des zones peu denses. On applique par la suite un algorithme semblable à la partition hiérarchique single link: En s’élargissant, les rayons se superposent, chaque superposition de rayon forment graduellement des groupes qui s’agglomèrent ainsi de manière hiérarchique. Au lieu d’effectuer une tranche à une hauteur donnée dans un dendrogramme de partitionnement, la technique HDBSCAN se base sur un dendrogramme condensé qui discarte les sous-groupes comprenant moins de n objets (\\(n_{gr min}\\)). Dans nouveau dendrogramme, on recherche des groupes qui occupent bien l’espace d’analyse. Pour ce faitre, on utilise l’inverse de la distance pour créer un indicateur de persistance (semblable à la similarité), \\(\\lambda\\). Pour chaque groupe hiérarchique dans le dendrogramme condensé, on peut calculer la persistance où le groupe prend naissance. De plus, pour chaque objet d’un groupe, on peut aussi calculer une distance à laquelle il quitte le groupe. La stabilité d’un groupe est la domme des différences de persistance entre la persistance à la naissance et les persistances des objets. On descend dans le dendrogramme. Si la somme des stabilité des groupes enfants est plus grande que la stabilité du groupe parent, on accepte la division. Sinon, le parent forme le groupe. La documentation du module hdbscan pour Python offre une description intuitive et plus exhaustive des principes et algorithme de HDBSCAN. 8.3.4.1 Paramètres Outre la métrique d’association dont nous avons discuté, HDBSCAN demande d’être nourri avec quelques paramètres importants. En particulier, le nombre minimum d’objets par groupe, \\(n_{gr min}\\) dépend de la quantité de données que vous avez à votre disposition, ainsi que de la quantité d’objets que vous jugez suffisante pour créer des groupes. Nous utiliserons l’implémentation de HDBSCAN du module dbscan. Si vous désirez davantage d’options, vous préférerez probablement l’implémentation du module largeVis. mcinnes_hdbscan &lt;- hdbscan(x = vegdist(df_mcinnes, method = &quot;euclidean&quot;), minPts = 20, gen_hdbscan_tree = TRUE, gen_simplified_tree = FALSE) hdbscan_group &lt;- mcinnes_hdbscan$cluster unique(hdbscan_group) ## [1] 6 0 4 3 5 1 2 Nous avons 6 groupes, numérotés de 1 à 6, ainsi que des étiquettes identifiant des objets désignés comme étant du bruit de fond, numéroté 0. Le dendrogramme non condensé peu être produit. plot(mcinnes_hdbscan$hdbscan_tree) Difficile d’y voir clair avec autant d’objets. L’objet mcinnes_hdbscan a un nombre minimum d’objets par groupe de 20. Ce qui permet de présenter le dendrogramme de manière condensée. plot(mcinnes_hdbscan) Enfin, un aperçu des stratégies de partitionnement utilisés jusqu’ici. clustering_group &lt;- df_mcinnes %&gt;% mutate(kmeans_group, hclust_group, dbscan_group, hdbscan_group) %&gt;% gather(-x, -y, key = &quot;method&quot;, value = &quot;cluster&quot;) ## Warning: attributes are not identical across measure variables; ## they will be dropped clustering_group$cluster &lt;- factor(clustering_group$cluster) clustering_group %&gt;% ggplot(aes(x = x, y = y)) + geom_point(aes(colour = cluster)) + facet_wrap(~method, ncol = 2) + coord_equal() + theme_bw() Clairement, le partitionnement avec HDBSCAN donne les meilleurs résultats. 8.3.5 Conclusion sur le partitionnement Au chapitre 4, nous avons vu avec le jeu de données “datasaurus” que la visualisation peut permettre de détecter des structures en segmentant les données selon des groupes. Or, si les données n’étaient pas étiquetées, leur structure serait indétectable avec les algorithmes disponibles actuellement. Le partitionnement permet d’explorer des données, de détecter des tendances et de dégager des groupes permettant la prise de décision. Plusieurs techniques de partitionnement ont été présentées. Le choix de la technique sera déterminante sur la manière dont les groupes seront partitionnés. La définition d’un groupe variant d’un cas à l’autre, il n’existe pas de règle pour prescrire une méthode ou une autre. La partitionnement hiérarchique a l’avantage de permetre de visualiser comment les groupes s’agglomèrent. Parmi les méthodes de partitionnement hiérarchique disponibles, les méthodes basées sur la densité permettent une grande flexibilité, ainsi qu’une détection d’observations ne faisant partie d’aucun goupe. 8.4 Ordination En écologie, biologie, agronommie comme en foresterie, la plupart des tableaux de données comprennent de nombreuses variables: pH, nutriments, climat, espèces ou cultivars, etc. L’ordination vise à mettre de l’ordre dans des données dont le nombre élevé de variables peut amener à des difficultés d’appréciation et d’interprétaion (Legendre et Legendre, 2012). Plus précisément, le terme ordination est utilisé en écologie pour désigner les techniques de réduction d’axe. L’analyse en composante principale est probablement la plus connue de ces techniques. Mais de nombreuses techniques d’ordination ont été développées au cours des dernières années, chacune ayant ses domaines d’application. Les techniques de réduction d’axe permettent de dégager l’information la plus importante en projetant une synthèse des relations entre les observations et entre les variables. Les techniques ne supposant aucune structure a priori sont dites non-contraignantes: elles ne comprennent pas de tests statistiques. À l’inverse, les ordinations contraignantes lient des variables descriptives avec une ou plusieurs variables prédictives. La référence en la matière est indiscutablement (Legendre et Legendre, 2012). Cette section en couvrira quelques unes et vous guidera vers la technique la plus appropriée pour vos données. 8.4.1 Ordination non contraignante Cette section couvrira l’analyse en composantes principales (ACP), l’analyse de correspondance (AC), l’analyse factorielle (AF) ainsi que l’analyse en coordonnées principales (ACoP). Méthode Distance préservée Variables Analyse en composantes principales (ACP) Distance euclidienne Données quantitatives, relations linéaires (attention aux double-zéros) Analyse de correspondance (AC) Distance de \\(\\chi^2\\) Données non-négatives, dimentionnellement homogènes ou binaires, abondance ou occurence Positionnement multidimensionnel (PoMd) Toute mesure de dissimilarité Données quantitatives, qualitatives nominales/ordinales ou mixtes Source: Adapté de (Legendre et Legendre, 2012, chapitre 9) 8.4.1.1 Analyse en composantes principales L’objectif d’une ACP est de représenter les données dans un nombre réduit de dimensions représentant le plus possible la variation d’un tableau de données: elle permet de projetter les données dans un espace où les variables sont combinées en axes orthogonaux dont le premier axe capte le maximum de variance. L’ACP peut par exemple être utilisée pour analyser des corrélations entre variables ou dégager l’information la plus pertinente d’un tableau de données météo ou de signal en un nombre plus retreint de variables. L’ACP effectue une rotation des axes à partir du centre (moyenne) du nuage de points effectuée de manière à ce que le premier axe définisse la direction où l’on retrouve la variance maximale. Ce premier axe est une combinaison linéaire des variables et forme la première composante principale. Une fois cet axe définit, on trouve de deuxième axe, orthogonal au premier, où l’on retouve la variance maximale - cet axe forme la deuxième composante principale, et ainsi de suite jusqu’à ce que le nombre d’axe corresponde au nombre de variables. Les projections des observations sur ces axes principaux sont appelés les scores. Les projections des variables sur les axes principaux sont les vecteurs propres (eigenvectors, ou loadings). La variance des composantes principales diminue de la première à la dernière, et peut être calculée comme une proportion de la variance totale: c’est le pourcentage d’inertie. Par convention, on utilise les valeurs propres (eigenvalues) pour mesurer l’importance des axes. Si la première composante principale a une inertie de 50% et la deuxième a une intertie de 30%, la représentation en 2D des projection représentera 80% de la variance du nuage de points. L’hétérogénéité des échelles de mesure peut avoir une grande importance sur les résultats d’une ACP (les données doivent être dimensionnellement homogènes). En effet, la hauteur d’un ceriser aura une variance plus grande que le diamètre d’une cerise exprimé dans les mêmes unités, et cette dernière aura plus de variance que la teneur en cuivre d’une feuille. Il est conséquemment avisé de mettre les données à l’échelle en centrant la moyenne à zéro et l’écart-type à 1 avant de procéder à une ACP. L’ACP a été conçue pour projetter en un nombre moindre de dimensions des observations dont les distributions sont multinormales. Bien que l’ACP soit une technique robuste, il est préférable de transformer préalablement les variables dont la distribution est particulièrement asymétriques (Legendre et Legendre, 2012, p. 450). Le cas échéant, les valeurs extrêmes pourraient faire dévier les vecteurs propres et biaiser l’analyse. En particulier, les données ACP menées sur des données compositionnelles sont réputées pour générer des analyses biaisées (Pawlowsky-Glahn and Egozcue, 2006). Le test de Mardia (Korkmaz, 2014) peut être utilisé pour tester la multinormalité. Une distribution multinormale devrait générer des scores en forme d’hypersphère (en forme de cercle sur un biplot: voir plus loin). 8.4.1.1.1 Vecteurs propres et valeurs propres Une matrice carrée (comme une matrice de covariance \\(\\Sigma\\)) multipliée par un vecteur propre \\(e\\) est égale aux valeurs propres \\(\\lambda\\) multipliées par les vecteurs propres \\(e\\). \\[ \\Sigma e = \\lambda e \\] De manière intuitive, les vecteurs propres indiquent l’orientation de la covariance, et les valeurs propres indique la longueur associée à cette direction. L’ACP est basée sur le calcul des vecteurs propres et des valeurs propres de la matrice de covariance des variables. Pour d’abord obtenir les valeurs propres \\(\\lambda\\), il faut résoudre l’équation \\[ det(cov(X) - \\lambda I) = 0 \\], où \\(det\\) est l’opération permettant de calculer le déterminant, \\(cov\\) est l’opération pour calculer la covariance, \\(X\\) est la matrice de données, \\(\\lambda\\) sont les valeurs propres et \\(I\\) est une matrice d’identité. Pour \\(p\\) variables dans votre tableau \\(X\\), vous obtiendrex \\(p\\) valeurs propres. Ensuite, on trouve les vecteurs propres en résolvant l’équation $ e = e $. Bien qu’il soit possible d’effectuer cette opération à la main pour des cas très simples, vous aurez avantage à utiliser un langage de programmation. Chargeons les données d’iris, puis isolons seulement les deux dimensions des sépales l’espèce setosa. data(&quot;iris&quot;) setosa_sepal &lt;- iris %&gt;% filter(Species == &quot;setosa&quot;) %&gt;% select(starts_with(&quot;Sepal&quot;)) setosa_sepal ## Sepal.Length Sepal.Width ## 1 5.1 3.5 ## 2 4.9 3.0 ## 3 4.7 3.2 ## 4 4.6 3.1 ## 5 5.0 3.6 ## 6 5.4 3.9 ## 7 4.6 3.4 ## 8 5.0 3.4 ## 9 4.4 2.9 ## 10 4.9 3.1 ## 11 5.4 3.7 ## 12 4.8 3.4 ## 13 4.8 3.0 ## 14 4.3 3.0 ## 15 5.8 4.0 ## 16 5.7 4.4 ## 17 5.4 3.9 ## 18 5.1 3.5 ## 19 5.7 3.8 ## 20 5.1 3.8 ## 21 5.4 3.4 ## 22 5.1 3.7 ## 23 4.6 3.6 ## 24 5.1 3.3 ## 25 4.8 3.4 ## 26 5.0 3.0 ## 27 5.0 3.4 ## 28 5.2 3.5 ## 29 5.2 3.4 ## 30 4.7 3.2 ## 31 4.8 3.1 ## 32 5.4 3.4 ## 33 5.2 4.1 ## 34 5.5 4.2 ## 35 4.9 3.1 ## 36 5.0 3.2 ## 37 5.5 3.5 ## 38 4.9 3.6 ## 39 4.4 3.0 ## 40 5.1 3.4 ## 41 5.0 3.5 ## 42 4.5 2.3 ## 43 4.4 3.2 ## 44 5.0 3.5 ## 45 5.1 3.8 ## 46 4.8 3.0 ## 47 5.1 3.8 ## 48 4.6 3.2 ## 49 5.3 3.7 ## 50 5.0 3.3 library(&quot;MVN&quot;) ## sROC 0.1-2 loaded setosa_sepal_mvn &lt;- mvn(setosa_sepal, mvnTest = &quot;mardia&quot;) setosa_sepal_mvn$multivariateNormality ## Test Statistic p value Result ## 1 Mardia Skewness 0.759503524380438 0.943793240544741 YES ## 2 Mardia Kurtosis 0.0934600553610254 0.925538081956867 YES ## 3 MVN &lt;NA&gt; &lt;NA&gt; YES Pour considérer la distribution comme multinormale, la p-value de la distortion (Mardia Skewness) et la statistique de Kurtosis (Mardia Kurtosis) doit être égale ou plus élevée que 0.05 (Kormaz, 2019, fiche d’aide de la fonction mvn de R). C’est bien le cas pour les données du tableau setosa_sepal. Retirons de la matrice de covariance les valeurs et vecteurs propres avec la fonction eigen. setosa_eigen &lt;- eigen(cov(setosa_sepal)) setosa_eigenval &lt;- setosa_eigen$values setosa_eigenvec &lt;- setosa_eigen$vectors Le premier vecteur propre correspond à la première colonne, et le second à la deuxième. Les coordonnées x et y sont les premières et deuxièmes lignes. Les vecteurs propres ont une longueur unitaire (norme de 1). Ils peuvent être mis à l’échelles à la racine carrée des valeurs propres. setosa_eigenvec_sc &lt;- setosa_eigenvec %*% diag(sqrt(setosa_eigen$values)) Pour effectuer une translation des vecteurs propres au centre du nuage de point, nous avons besoin du centroïde. centroid &lt;- setosa_sepal %&gt;% apply(., 2, mean) plot(setosa_sepal, asp = 1) # vecteurs propres brutes lines(x=c(centroid[1], centroid[1] + setosa_eigenvec[1, 1]), y=c(centroid[2], centroid[2] + setosa_eigenvec[2, 1]), col = &quot;green&quot;, lwd = 3) # vecteur propre 1 lines(x=c(centroid[1], centroid[1] + setosa_eigenvec[1, 2]), y=c(centroid[2], centroid[2] + setosa_eigenvec[2, 2]), col = &quot;green&quot;, lwd = 3) # vecteur propre 1 # vecteurs propres à l&#39;échelle lines(x=c(centroid[1], centroid[1] + setosa_eigenvec_sc[1, 1]), y=c(centroid[2], centroid[2] + setosa_eigenvec_sc[2, 1]), col = &quot;red&quot;, lwd = 4) # vecteur propre 1 lines(x=c(centroid[1], centroid[1] + setosa_eigenvec_sc[1, 2]), y=c(centroid[2], centroid[2] + setosa_eigenvec_sc[2, 2]), col = &quot;red&quot;, lwd = 4) # vecteur propre 1 points(x=centroid[1], y=centroid[2], pch = 16, cex = 2, col =&quot;blue&quot;) # centroid On peut observer que, comme je l’ai mentionné plus haut, les vecteurs propres indiquent l’orientation de la covariance, et les valeurs propres indique la longueur associée à cette direction. 8.4.1.1.2 Biplot Imaginez un nuage de points en 3D, axes y compris. Vous tournez votre nuage de points pour trouver la perspective en 2D qui fera en sorte que vos données soient les plus dispersées possibles. Avec une lampe de poche, vous illuminez votre nuage de points dans l’axe de cette perspective: vous venez d’effectuer une analyse en composantes principales, et l’ombre des points et des axes sur le mur formera votre biplot. Pour créer un biplot, on juxtapose les descripteurs (variables) en tant que vecteurs propres, représentés par des flèches, et les objets (observations) en tant que scores, représentés par des points. Les résultats d’une ordination peuvent être présentés selon deux types de biplots (Legendre et Legendre, 2012). Biplot de corrélation permettant de visualiser les corrélations entre des variables météorologiques. Source: Parent, 2017 Deux types de projection sont courramment utilisés. Biplot de distance. Ce type de projection permet de visualiser la position des objets entre eux et par rapport aux descripteurs et d’apprécier la contribution des descripteurs pour créer les composantes principales. Pour créer un biplot de distance, on projette directement les vecteurs propres (\\(U\\)) en guise de descripteurs. Pour ce qui est des objets, on utilise les scores de l’ACP (\\(F\\)). De cette manière, les distances euclidiennes entre les scores sont des approximations des distances euclidiennes dans l’espace multidimentionnel, la projection d’un objet sur un descripteur perpendiculairement à ce dernier est une approximation de la position de l’objet sur le descripteur et la projection d’un descripteur sur un axe principal est proportionnelle à sa contribution pour générer l’axe. Biplot de corrélation. Cette projection permet d’apprécier les corrélations entre les descripteurs. Pour ce faire, les objets et les valeurs propres doivent être transformés. Pour générer les descripteurs, les vecteurs propres (\\(U\\)) doivent être multipliés par la matrice diagonalisée de la racine carrée des valeurs propres (\\(\\Lambda\\)), c’est-à-dire \\(U \\Lambda ^{\\frac{1}{2}}\\). En ce qui a trait aux objets, on multiplie les scores par (\\(F\\)) par la racine carrée négative des valeurs propres diagonalisées, c’est-à-dire \\(F \\Lambda ^{- \\frac{1}{2}}\\). De cette manière, tout comme c’est le cas pour le biplot de distance, la projection d’un objet sur un descripteur perpendiculairement à ce dernier est une approximation de la position de l’objet sur le descripteur, la projection d’un descripteur sur un axe principal est proportionnelle à son écart-type et les angles entre les descripteurs sont proportionnelles à leur corrélation (et non pas leur proximité). En d’autres mots, le bilot de distances devrait être utilisé pour apprécier la distance entre les objets et le biplot de corrélation devrait être utilisé pour apprécier les corrélations entre les descripteurs. Mais dans tous les cas, le type de biplot utilisé doit être indiqué. Le triplot est une forme apparentée au biplot, auquel on ajoute des variables prédictives. Le triplot est utile pour représenter les résultats des ordinations contraignantes comme les analyses de redondance et les analyse de correspondance canoniques. 8.4.1.1.3 Application Bien que l’ACP puisse être effectuée grâce à des modules de base de R, nous utiliserons le module vegan. Le tableau varechem comprend des données issues d’analyse de sols identifiés par leur composition chimique, leur pH, leur profondeur totale et la profondeur de l’humus publiées dans Väre et al. (1995) et exportées du module vegan. library(&quot;vegan&quot;) data(&quot;varechem&quot;) varechem %&gt;% sample_n(5) ## N P K Ca Mg S Al Fe Mn Zn Mo Baresoil ## 1 22.8 40.9 171.4 691.8 151.4 40.8 104.8 17.6 43.6 9.1 0.40 40.5 ## 2 16.7 55.8 205.3 1169.7 126.3 35.9 253.6 96.4 25.1 8.2 0.05 7.6 ## 3 30.5 24.6 78.7 188.5 55.5 25.3 294.9 123.8 10.1 3.0 0.40 18.6 ## 4 19.8 42.1 139.9 519.4 90.0 32.3 39.0 40.9 58.1 4.5 0.30 43.9 ## 5 20.2 67.7 207.1 973.3 209.1 58.1 138.0 35.4 32.1 16.8 0.80 21.2 ## Humdepth pH ## 1 3.8 2.7 ## 2 1.1 3.6 ## 3 1.7 3.1 ## 4 2.2 2.7 ## 5 2.0 3.0 Comme nous l’avons vu précdemment, les données de concentration sont de type compositionnelles. Les données compositionnelles du tableau varechem mériteraient d’être transformées (Aitchison et Greenacre, 2002). Utilisons les log-ratios centrés (clr). library(&quot;compositions&quot;) varecomp &lt;- varechem %&gt;% select(-Baresoil, -Humdepth, -pH) %&gt;% mutate(Fv = apply(., 1, function(x) 1e6 - sum(x))) vareclr &lt;- varecomp %&gt;% acomp(.) %&gt;% clr(.) %&gt;% as_tibble() %&gt;% bind_cols(varechem %&gt;% select(Baresoil, Humdepth, pH)) ## Warning: Calling `as_tibble()` on a vector is discouraged, because the behavior is likely to change in the future. Use `tibble::enframe(name = NULL)` instead. ## This warning is displayed once per session. vareclr %&gt;% sample_n(5) ## # A tibble: 5 x 15 ## N P K Ca Mg S Al Fe Mn Zn Mo ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -1.13 -0.568 0.781 2.03 0.109 -0.608 0.317 -2.68 -0.841 -2.12 -4.90 ## 2 -1.37 -0.655 0.697 2.01 -0.0269 -0.705 0.547 -1.89 -0.869 -2.35 -5.06 ## 3 -0.773 -0.988 0.175 1.05 -0.174 -0.960 1.50 0.628 -1.88 -3.09 -5.11 ## 4 -1.69 -0.624 0.830 1.59 -0.0312 -0.729 0.189 -0.626 -0.331 -2.60 -5.49 ## 5 -1.38 -0.525 0.659 1.96 -0.193 -0.767 0.172 -1.49 -0.102 -2.33 -5.16 ## # … with 4 more variables: Fv &lt;dbl&gt;, Baresoil &lt;dbl&gt;, Humdepth &lt;dbl&gt;, ## # pH &lt;dbl&gt; Effectuons l’ACP. Pour cet exemple, nous standardiserons les données étant données que les colonnes Baresoil, Humedepth et pH ne sont pas à la même échelle que les colonnes des clr. vareclr_sc &lt;- scale(vareclr) vare_pca &lt;- rda(vareclr_sc) # ou bien rda(vareclr, scale = TRUE, mais la mise à l&#39;échelle préalable est plus explicite) L’objet vareclr_pca contient l’information nécessaire pour mener notre ACP. summary(vare_pca, scaling = 2) # scaling = 2 pour obtenir les infos pour les biplots de corrélation ## ## Call: ## rda(X = vareclr_sc) ## ## Partitioning of variance: ## Inertia Proportion ## Total 15 1 ## Unconstrained 15 1 ## ## Eigenvalues, and their contribution to the variance ## ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Eigenvalue 7.1523 2.4763 2.1122 0.93015 0.57977 0.48786 0.36646 ## Proportion Explained 0.4768 0.1651 0.1408 0.06201 0.03865 0.03252 0.02443 ## Cumulative Proportion 0.4768 0.6419 0.7827 0.84473 0.88338 0.91590 0.94034 ## PC8 PC9 PC10 PC11 PC12 PC13 ## Eigenvalue 0.29432 0.19686 0.15434 0.107357 0.095635 0.042245 ## Proportion Explained 0.01962 0.01312 0.01029 0.007157 0.006376 0.002816 ## Cumulative Proportion 0.95996 0.97308 0.98337 0.990527 0.996902 0.999719 ## PC14 ## Eigenvalue 0.0042200 ## Proportion Explained 0.0002813 ## Cumulative Proportion 1.0000000 ## ## Scaling 2 for species and site scores ## * Species are scaled proportional to eigenvalues ## * Sites are unscaled: weighted dispersion equal on all dimensions ## * General scaling constant of scores: 4.309777 ## ## ## Species scores ## ## PC1 PC2 PC3 PC4 PC5 PC6 ## N 0.1437 0.7606 -0.6792 0.19837 0.1128526 -0.050149 ## P 0.8670 -0.3214 -0.2950 -0.22940 0.1437960 -0.042884 ## K 0.9122 -0.3857 0.2357 0.03469 0.2737020 0.075717 ## Ca 0.9649 -0.3362 -0.2147 0.17757 -0.2188717 0.008051 ## Mg 0.8263 -0.2723 0.1035 0.52135 -0.1495399 -0.342214 ## S 0.8825 -0.3169 0.3539 -0.21216 0.1176279 -0.191386 ## Al -1.0105 -0.2442 0.2146 0.02674 -0.1005560 -0.043569 ## Fe -1.0338 -0.2464 0.1492 0.13162 0.1512218 0.081571 ## Mn 0.9556 0.1041 -0.1256 -0.21300 0.2565831 0.275275 ## Zn 0.7763 -0.1031 -0.3123 -0.36493 -0.5665691 0.153089 ## Mo -0.2152 0.8717 0.4065 -0.33643 -0.2134335 -0.167725 ## Fv 0.2360 0.5776 -0.8112 0.12736 0.1280097 -0.109737 ## Baresoil 0.5147 0.4210 0.4472 0.54980 -0.1438570 0.463148 ## Humdepth 0.7455 0.4379 0.5194 0.16493 0.0004757 -0.273056 ## pH -0.5754 -0.5864 -0.5957 0.23408 -0.1517661 -0.056641 ## ## ## Site scores (weighted sums of species scores) ## ## PC1 PC2 PC3 PC4 PC5 PC6 ## sit1 0.16862 0.423777 0.46731 0.91175 1.10380 1.06421 ## sit2 -0.09705 -0.097482 0.61143 -0.29049 1.14916 0.40622 ## sit3 0.02831 -0.795737 0.74176 -0.19097 -2.43337 -0.81762 ## sit4 1.39081 -0.354376 -0.19377 -0.45160 0.46020 -0.31446 ## sit5 1.30346 0.357866 0.29887 0.76856 0.20913 -0.64145 ## sit6 0.43636 0.495037 1.21722 1.18128 -0.98242 -0.74474 ## sit7 1.07306 0.467575 -0.32245 0.03717 0.13956 -0.64972 ## sit8 0.02545 0.659714 -0.28861 -0.01424 0.47105 0.45173 ## sit9 1.42005 0.007356 -0.29000 -0.78474 0.97592 -0.80263 ## sit10 -0.50638 -0.220909 1.52981 0.26289 0.42135 0.94054 ## sit11 0.45392 0.649297 0.44573 -0.26620 -0.74522 -0.53228 ## sit12 0.18623 0.259640 0.89112 0.21096 -0.51393 2.24361 ## sit13 1.26264 0.225744 -0.96668 -0.69334 0.61990 0.43312 ## sit14 -1.48685 0.739545 -0.20926 1.09256 0.61856 -0.87999 ## sit15 -0.50622 1.108685 -2.61287 -1.00433 -1.35383 1.21964 ## sit16 -1.28653 0.898663 -0.38778 -0.47556 -0.02449 -0.29419 ## sit17 -1.72773 0.476962 -0.48878 0.71156 1.06398 -1.33473 ## sit18 -0.82844 -0.296515 1.20315 -1.49821 -0.18330 1.05231 ## sit19 -1.00247 -0.609253 0.25185 -0.85420 0.71031 0.14854 ## sit20 -0.43405 -0.338912 0.55348 -1.35776 -0.81986 -1.02468 ## sit21 -0.05083 0.122645 -0.04611 -0.56047 -0.26151 -0.98053 ## sit22 0.17891 -2.315489 -0.69084 -0.19547 0.80628 0.04291 ## sit23 -0.46443 -2.592018 -1.21615 1.56359 -0.62334 0.28748 ## sit24 0.46316 0.728185 -0.49843 1.89726 -0.80791 0.72671 La deuxième ligne de Importance of components, Proportion Explained, indique la proportion de la variance totale captée successivement par les axes principaux. Le premier axe principal comporte 47.68% de la variance. Le deuxième axe principal ajoutant une proportion de 16,51%, une représentation en deux axes principaux présentent 64.19 % de la variance. prop_expl &lt;- vare_pca$CA$eig / sum(vare_pca$CA$eig) prop_expl ## PC1 PC2 PC3 PC4 PC5 ## 0.4768180610 0.1650859388 0.1408156459 0.0620101490 0.0386511040 ## PC6 PC7 PC8 PC9 PC10 ## 0.0325238535 0.0244303815 0.0196215021 0.0131238464 0.0102890284 ## PC11 PC12 PC13 PC14 ## 0.0071571089 0.0063756951 0.0028163495 0.0002813359 La décision du nombre d’axes principaux à retenir est arbitraire. Elle peut dépendre d’un nombre maximal de paramètre à retenir pour éviter de surdimensionner un modèle (curse of dimensionality, section 11) ou d’un seuil de pourcentage de variance minimal à retenir, par exemple 75%. Ou bien, vous retiendrez deux composantes principales si vous désirez présenter un seul biplot. L’approche de Kaiser-Guttmann (Borcard et al., 2011) consiste à sélectionner les composantes principales dont la valeur propre est supérieure à leur moyenne. plot(x = 1:length(vare_pca$CA$eig), y = vare_pca$CA$eig, type = &quot;b&quot;, xlab = &quot;Rang de la valeur propre&quot;, ylab = &quot;Valeur propre&quot;) abline(h = mean(vare_pca$CA$eig), col = &quot;red&quot;, lty = 2) L’approche du broken stick consiste à couper un bâton d’une longueur de 1 en n tranches. La première tranche est de longueur \\(\\frac{1}{n}\\). La tranche suivante est d’une longueur de la tranche précédente à laquelle on aditionne \\(\\frac{1}{longueur~restante}\\). Puis on place les longueurs en ordre décroissant. On retient les composantes principales dont les valeurs propres cumulées sont plus grandes que le broken stick. broken_stick &lt;- function(x) { bsm &lt;- vector(&quot;numeric&quot;, length = x) bsm[1] &lt;- 1/x for (i in 2:x) { bsm[i] &lt;- bsm[i-1] + 1/(x+1-i) } bsm &lt;- rev(bsm/x) return(bsm) } Le graphique du broken stick: plot(x = 1:length(vare_pca$CA$eig), y = prop_expl, type = &quot;b&quot;, xlab = &quot;Rang de la valeur propre&quot;, ylab = &quot;Valeur propre&quot;) lines(x = 1:length(vare_pca$CA$eig), y = broken_stick(length(vare_pca$CA$eig)), col = &quot;red&quot;, lty = 2) Les approches Kaiser-Guttmann et broken stick suggèrent que les trois premières composantes sont suffisantes pour décrire la dispersion des données. Examinons les loadings (vecteurs propres) plus en particulier. Dans le langage du module vegan, les vecteurs propres sont les espèces (species) et les scores sont les sites. vare_eigenvec &lt;- vegan::scores(vare_pca, scaling = 2, display = &quot;species&quot;, choices = 1:(ncol(vareclr)-1)) vare_eigenvec ## PC1 PC2 PC3 PC4 PC5 ## N 0.1437343 0.7606006 -0.6792046 0.1983670 0.1128526122 ## P 0.8669892 -0.3213683 -0.2949864 -0.2294036 0.1437959857 ## K 0.9122089 -0.3857245 0.2356904 0.0346904 0.2737019601 ## Ca 0.9648855 -0.3361651 -0.2147486 0.1775746 -0.2188716732 ## Mg 0.8263327 -0.2723055 0.1035276 0.5213484 -0.1495399242 ## S 0.8824519 -0.3169039 0.3538854 -0.2121562 0.1176278503 ## Al -1.0105173 -0.2441785 0.2145614 0.0267422 -0.1005559874 ## Fe -1.0337676 -0.2463987 0.1491865 0.1316173 0.1512218115 ## Mn 0.9555632 0.1041030 -0.1256178 -0.2130047 0.2565830557 ## Zn 0.7763480 -0.1030878 -0.3122919 -0.3649341 -0.5665691228 ## Mo -0.2152399 0.8717229 0.4064967 -0.3364279 -0.2134335302 ## Fv 0.2360040 0.5775863 -0.8111953 0.1273582 0.1280096553 ## Baresoil 0.5147445 0.4209983 0.4472351 0.5497950 -0.1438569673 ## Humdepth 0.7455213 0.4379436 0.5193895 0.1649306 0.0004756685 ## pH -0.5753858 -0.5863743 -0.5957495 0.2340826 -0.1517660977 ## PC6 PC7 PC8 PC9 PC10 ## N -0.050148980 -0.09111164 -0.06122008 0.315645453 0.08090232 ## P -0.042883754 0.26894062 0.34111276 0.021124287 0.08756299 ## K 0.075717162 -0.21662612 -0.01641260 0.143099440 -0.08737113 ## Ca 0.008050762 0.03630015 0.04775616 -0.073609828 -0.10601799 ## Mg -0.342213793 0.04617838 -0.12098602 -0.051599273 0.18373857 ## S -0.191386377 -0.26825994 0.15822845 0.038378858 0.05100717 ## Al -0.043569364 -0.22737412 0.10598673 0.040586196 -0.14473132 ## Fe 0.081571443 0.10553041 -0.09254655 -0.079426433 0.09908706 ## Mn 0.275275174 0.20224538 -0.19347804 -0.038859808 -0.07637994 ## Zn 0.153089144 -0.12332232 -0.14862229 0.024026151 0.02643462 ## Mo -0.167725160 0.13788948 0.17165900 0.032981311 0.01419924 ## Fv -0.109737235 -0.20911147 0.11289753 -0.281443886 -0.08391004 ## Baresoil 0.463148072 -0.02103009 0.23028292 0.004554036 0.02604286 ## Humdepth -0.273056212 0.17061078 -0.11310394 0.027515405 -0.23068827 ## pH -0.056640816 0.19890884 0.12152266 0.150118818 -0.15240317 ## PC11 PC12 PC13 PC14 ## N -0.019251478 0.045420621 0.05020956 0.002340519 ## P -0.045741546 0.145128883 -0.03337551 -0.010109130 ## K 0.183005607 0.002260341 -0.10566808 0.001169065 ## Ca 0.161460554 0.041210064 0.14341793 0.007419161 ## Mg -0.009862571 -0.063493608 -0.03782662 -0.023575986 ## S -0.138785063 -0.117144869 0.06075094 0.025874035 ## Al -0.089462074 0.058212507 0.01983102 -0.037901576 ## Fe -0.006376211 0.049837173 -0.01169516 0.036048221 ## Mn -0.083300112 -0.133353213 0.02679781 -0.021373612 ## Zn -0.064973307 0.051057277 -0.06538348 0.010896560 ## Mo 0.128814989 -0.114803631 -0.01989539 -0.001335923 ## Fv -0.012456867 -0.020157331 -0.05448619 0.005707928 ## Baresoil -0.061147847 -0.019696758 -0.01640490 0.003823725 ## Humdepth -0.102189307 0.109293684 -0.02485030 0.016559206 ## pH -0.037691048 -0.153813168 -0.04523353 0.014193061 ## attr(,&quot;const&quot;) ## [1] 4.309777 L’ordre d’importance des vecteurs propres est établi en ordre croissant des élément des vecteurs propres associées. Un vecteur propre est une combinaison linéaire des variables. Par exemple, le premier vecteur propre pointe surtout dans la direction du Fe (-1.497) et de l’Al (-1.463). Le deuxième pointe surtout vers le Mo (2.145). Les vecteurs (loadings) d’un biplot de distance présentant les des deux premières composantes principales prendront les coordonnées des deux premières colonnes. Le vecteur Al aura la coordonnée [-1.463 ; -0.601], le vecteur de Fe sera placé à [-1.497 ; -0.606] et le vecteur Mo à [-0.312 ; 2.145]. Il existe différentes fonctions d’affichage des biplots. Notez que leur longueur peut être magnifiée pour améliorer la visualisation. Lançons la fonction biplot pour créer un biplot de distance et un autre de corrélation. par(mfrow = c(1, 2)) biplot(vare_pca, scaling = 1, main = &quot;Biplot de distance&quot;) biplot(vare_pca, scaling = 2, main = &quot;Biplot de corrélation&quot;) Le biplot de distance permet de dégager les variables qui expliquent davantage la variabilité dans notre tableau: les clr du Fe et de l’Al forment en grande partie le premier axe principal, alors que le clr du Mo forme en grande partie le second axe. Le biplot de corrélation montre que les clr du Fe et du Al sont corrélés dans le même sens, mais das le sens contraire du clr du Mn. L’information sur la teneur en Fe et celle de l’Al est en grande partie redondante. Toutefois, le clr du Mo est presque indépendant du clr du Fe, ceux-ci étant à angle presque droit (~90°). Ces relations peuvent être explorées directement. par(mfrow = c(1, 2)) plot(vareclr$Al, vareclr$Fe) plot(vareclr$Mo, vareclr$Fe) Nous avons mentionné que l’ACP est une rotation. Prenons un second exemple pour bien en saisir les tenants et aboutissants. Le tableau de données que nous chargerons provient d’un infographie d’un dauphin, intitullée Bottlenose Dolphin, conçu par l’artiste Tarnyloo. Les points correspondent à la surface d’un dauphin. J’ai ajouté une colonne anatomy, qui indique à quelle partie anatomique le point appartient. dolphin &lt;- read_csv(&quot;data/07_dolphin.csv&quot;) ## Parsed with column specification: ## cols( ## x = col_double(), ## y = col_double(), ## z = col_double(), ## anatomy = col_character() ## ) dolphin %&gt;% sample_n(5) ## # A tibble: 5 x 4 ## x y z anatomy ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.0734 -0.591 -0.00311 Head ## 2 -0.392 1.03 1.27 Caudal fin ## 3 0.0825 -0.461 -0.0983 Head ## 4 0.0628 -0.108 -0.196 Body ## 5 0.0249 -0.468 -0.186 Head Voici en vue isométrique ce en quoi consiste ce nuage de points. library(&quot;scatterplot3d&quot;) scatterplot3d(x = dolphin$x, y = dolphin$y, z = dolphin$z, pch = 16, cex.symbols = 0.2) Effectuons l’ACP sur le dauphin. dolph_pca &lt;- rda(dolphin %&gt;% select(x, y, z), scale = FALSE) biplot(dolph_pca, scaling = 2) On n’y voit pas grand chose, mais si l’on extrait les scores et que l’on raccourcit les vecteurs: dolph_scores &lt;- vegan::scores(dolph_pca, display = &quot;sites&quot;) dolph_loads &lt;- vegan::scores(dolph_pca, display = &quot;species&quot;) dolph_loads ## PC1 PC2 ## x -0.02990131 0.01608095 ## y -7.13731672 -1.43221776 ## z -4.56612084 2.23859843 ## attr(,&quot;const&quot;) ## [1] 9.089026 plot(dolph_scores, pch = 16, cex = 0.24, asp = 1, col = factor(dolphin$anatomy)) segments(x0 = rep(0, 3), y0 = rep(0, 3), x = dolph_loads[, 1]/50, y = dolph_loads[, 2]/50, col = &quot;chocolate&quot;, lwd = 4) La meilleure représentation du dauphin en 2D, selon la variance, est son profil - en effet, il est plus long et haut que large. Note. Une ACP effectue seulement une rotation des points. Les distances euclidiennes entre les points sont maintenues. Note. L’ACP a été conçue pour projetter en un nombre moindre de dimensions des observations dont les distributions sont multinormales (ce n’est évidemment pas le cas du dauphin). Note. Les axes principaux d’une ACP sont des variables aléatoires. Elles peuvent être assujetties à des tests ststistiques, des modèles, du partitionnement de données, etc. Excercice. Effectuez maintenant une ACP avec les données d’iris. 8.4.1.2 Analyse de correspondance (AC) L’analyse de correspondance (AC) est particulièrement appropriée pour traiter des données d’abondance et d’occurence. Tout comme l’analyse en composantes principales, les données apportés vers une AC doivent être dimensionnellement homogènes, c’est-à-dire que chaque variable doit être de même métrique: pour des données d’abondance, cela signifie que les décomptes réfèrent tous au même concept: individus, colonies, surfaces occupées, etc. Alors que la distance euclidienne est préservée avec l’ACP, l’AC préserve la distance du \\(\\chi^2\\), qui est insensible aux double-zéros. L’AC produit \\(min(n,p)-1\\) axes principaux orthogonaux qui captent non pas le maximum de variance, mais la proportion de mesures aux carré par rapport à la somme des carrés de la matrice. Le biplot obtenu peut être présenté sous forme de biplot de site (scaling 1), où la distance du \\(\\chi^2\\) est préservée entre les sites ou biplot d’espèces (scaling 2), ou la distance du \\(\\chi^2\\) est préservée entre les espèces. L’AC hérite du coup une propriété importate de la distance du \\(\\chi^2\\), qui accorde davantage de distance entre un compte de 0 et de 1 qu’entre 1 et 2, et davantage entre 1 et 2 qu’entre 2 et 3. Par exemple, sur ces trois sites, on a compté un individu A de moins que d’individu B. abundance_0123 = tibble(Site = c(&quot;Site 1&quot;, &quot;Site 2&quot;, &quot;Site 3&quot;), A = c(0, 1, 9), B = c(1, 2, 10)) abundance_0123 ## # A tibble: 3 x 3 ## Site A B ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Site 1 0 1 ## 2 Site 2 1 2 ## 3 Site 3 9 10 Pourtant, la distance du \\(\\chi^2\\) est plus élevée entre le site 1 et le site 2 qu’entre le site 2 et le site 3. dist(decostand(abundance_0123 %&gt;% select(-Site), method=&quot;chi.square&quot;)) ## 1 2 ## 2 0.6724111 ## 3 0.9555316 0.2831205 La distance du \\(\\chi^2\\) donne davantage d’importance aux espèces rares, ce dont une analyse doit tenir compte. Il pourrait être envisageable de retirer d’un tableau des espèces rare, ou bien prétransformer des données d’abondance par une transformation de chord ou de Hellinger (tel que discuté au chapitre 6), puis procéder à une ACP sur ces données (Legendre et Gallagher, 2001). 8.4.1.2.1 Application Le tableau varespec comprend des données de surface de couverture de 44 espèces de plantes en lien avec les données environnementales du tableau varechem. Ces données ont été publiées dans Väre et al. (1995) et exportées du module vegan. data(&quot;varespec&quot;) varespec %&gt;%sample_n(5) ## Callvulg Empenigr Rhodtome Vaccmyrt Vaccviti Pinusylv Descflex Betupube ## 1 0.00 15.13 2.42 5.92 15.97 0.00 3.7 0 ## 2 0.02 6.45 0.00 0.00 14.13 0.07 0.0 0 ## 3 0.67 0.17 0.00 0.35 12.13 0.12 0.0 0 ## 4 0.00 3.47 0.00 0.25 20.50 0.25 0.0 0 ## 5 0.05 9.30 0.00 0.00 8.50 0.03 0.0 0 ## Vacculig Diphcomp Dicrsp Dicrfusc Dicrpoly Hylosple Pleuschr Polypili ## 1 1.12 0.00 0.00 3.63 0.00 6.70 58.07 0.00 ## 2 0.47 0.00 0.85 1.87 0.08 1.35 13.73 0.07 ## 3 0.00 0.00 0.33 10.92 0.02 0.00 37.75 0.02 ## 4 0.00 0.25 0.00 0.38 0.25 0.00 4.07 0.00 ## 5 0.00 0.00 0.00 0.03 0.00 0.00 0.75 0.00 ## Polyjuni Polycomm Pohlnuta Ptilcili Barbhatc Cladarbu Cladrang Cladstel ## 1 0.00 0.13 0.02 0.08 0.08 1.42 7.63 2.55 ## 2 0.05 0.00 0.12 0.00 0.00 6.80 11.22 0.05 ## 3 0.23 0.00 0.03 0.02 0.00 12.05 8.13 0.18 ## 4 0.25 0.00 0.25 0.25 0.00 0.46 4.00 84.30 ## 5 0.03 0.00 0.00 0.03 0.00 0.48 24.50 75.00 ## Cladunci Cladcocc Cladcorn Cladgrac Cladfimb Cladcris Cladchlo Cladbotr ## 1 0.15 0.00 0.38 0.12 0.10 0.03 0.00 0.02 ## 2 4.75 0.03 0.12 0.22 0.18 0.07 0.00 0.02 ## 3 2.65 0.13 0.18 0.23 0.25 1.23 0.00 0.00 ## 4 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.00 ## 5 0.20 0.00 0.03 0.03 0.05 0.03 0.03 0.00 ## Cladamau Cladsp Cetreric Cetrisla Flavniva Nepharct Stersp Peltapht ## 1 0 0.02 0.00 0.00 0.00 0.00 0.00 0.07 ## 2 0 0.02 0.15 0.00 0.00 0.02 0.00 0.00 ## 3 0 0.00 0.15 0.03 0.00 0.00 0.85 0.00 ## 4 0 0.25 0.25 0.25 0.67 0.00 0.00 0.00 ## 5 0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## Icmaeric Cladcerv Claddefo Cladphyl ## 1 0 0.00 0.15 0.00 ## 2 0 0.00 1.20 0.00 ## 3 0 0.00 1.00 0.00 ## 4 0 0.00 0.25 0.25 ## 5 0 0.03 0.03 0.00 Pour effectuer l’AC, nous utiliserons, comme pour l’ACP, le module vegan mais cette fois-ci avec la fonction cca. L’AC en scaling 1 est effectuée sur le tableau des abondances avec les espèces comme colonnes et les sites comme lignes. Les matrices d’abondance transposées indique les sites où chque espèce ont été dénombrées: pour une analyse en scaling 2, on effectue une analyse de correspondance sur la matrice d’abondance (ou d’occurence) transposée. Pour chacune des AC, je filtre pour m’assurer que toutes les lignes contiennent au moins une observation. Ce n’est pas nécessaire dans notre cas, mais je le laisse pour l’exemple. vare_cca &lt;- cca(varespec %&gt;% filter(rowSums(.) &gt; 0)) summary(vare_cca, scaling = 1) ## ## Call: ## cca(X = varespec %&gt;% filter(rowSums(.) &gt; 0)) ## ## Partitioning of scaled Chi-square: ## Inertia Proportion ## Total 2.083 1 ## Unconstrained 2.083 1 ## ## Eigenvalues, and their contribution to the scaled Chi-square ## ## Importance of components: ## CA1 CA2 CA3 CA4 CA5 CA6 CA7 ## Eigenvalue 0.5249 0.3568 0.2344 0.19546 0.17762 0.12156 0.11549 ## Proportion Explained 0.2520 0.1713 0.1125 0.09383 0.08526 0.05835 0.05544 ## Cumulative Proportion 0.2520 0.4233 0.5358 0.62962 0.71489 0.77324 0.82868 ## CA8 CA9 CA10 CA11 CA12 CA13 ## Eigenvalue 0.08894 0.07318 0.05752 0.04434 0.02546 0.01710 ## Proportion Explained 0.04269 0.03513 0.02761 0.02129 0.01222 0.00821 ## Cumulative Proportion 0.87137 0.90650 0.93411 0.95539 0.96762 0.97583 ## CA14 CA15 CA16 CA17 CA18 ## Eigenvalue 0.014896 0.010160 0.007830 0.006032 0.004008 ## Proportion Explained 0.007151 0.004877 0.003759 0.002896 0.001924 ## Cumulative Proportion 0.982978 0.987855 0.991614 0.994510 0.996434 ## CA19 CA20 CA21 CA22 CA23 ## Eigenvalue 0.002865 0.0019275 0.0018074 0.0005864 0.0002434 ## Proportion Explained 0.001375 0.0009253 0.0008676 0.0002815 0.0001168 ## Cumulative Proportion 0.997809 0.9987341 0.9996017 0.9998832 1.0000000 ## ## Scaling 1 for species and site scores ## * Sites are scaled proportional to eigenvalues ## * Species are unscaled: weighted dispersion equal on all dimensions ## ## ## Species scores ## ## CA1 CA2 CA3 CA4 CA5 CA6 ## Callvulg 0.0303167 -1.597460 0.11455 -2.894569 0.1376073 2.291129 ## Empenigr 0.0751030 0.379305 0.39303 0.023675 0.8568729 -0.400964 ## Rhodtome 1.1052309 1.499299 3.04284 0.120106 3.2324306 -0.283510 ## Vaccmyrt 1.4614812 1.622935 2.72375 0.231688 0.4604556 0.712538 ## Vaccviti 0.1468014 0.313436 0.14696 0.243505 0.6868371 -0.147815 ## Pinusylv -0.4820096 0.588517 -0.36020 -0.127094 0.4064754 0.386604 ## Descflex 1.5348239 1.218806 1.87562 -0.001340 -1.3136979 -0.070731 ## Betupube 0.6694503 1.951826 3.84017 1.389423 7.5959115 -0.244478 ## Vacculig -0.0830789 -1.629259 1.05063 0.802648 -0.3058811 -1.625341 ## Diphcomp -0.5446464 -1.037570 0.52282 0.940275 0.3682126 -1.082929 ## Dicrsp 1.8120408 0.360290 -4.92082 3.088562 1.3867372 0.157815 ## Dicrfusc 1.2704743 -0.562978 -0.39718 -2.929542 0.3848272 -2.408710 ## Dicrpoly 0.7248118 1.409347 0.80341 1.915549 4.5674148 1.295447 ## Hylosple 2.0062408 1.743883 2.27549 0.928884 -3.7648428 2.254851 ## Pleuschr 1.3102086 0.583036 -0.01004 0.137298 -1.1216144 0.200422 ## Polypili -0.3805097 -1.243904 0.54593 1.477188 -0.7276341 -0.387641 ## Polyjuni 1.0133795 0.099043 -2.24697 1.510641 0.7729714 -3.062378 ## Polycomm 0.8468241 1.321773 1.13585 1.140723 2.6836594 -0.605038 ## Pohlnuta -0.0136453 0.589290 -0.35542 0.135481 0.9369707 0.397246 ## Ptilcili 0.4223631 1.598584 3.43474 1.400065 6.3209491 0.198935 ## Barbhatc 0.5018348 2.119334 4.57303 1.693188 8.1101807 0.645995 ## Cladarbu -0.1531729 -1.483884 0.20024 0.193680 0.0734141 0.358926 ## Cladrang -0.5502561 -1.084008 0.40552 0.724060 -0.3357992 -0.335924 ## Cladstel -1.4373146 1.077753 -0.44397 -0.375926 -0.2421525 0.004212 ## Cladunci 0.8151727 -1.006186 -1.82587 -1.389523 1.6046713 3.675908 ## Cladcocc -0.2133215 -0.584429 -0.21434 -0.567886 -0.0003788 -0.145303 ## Cladcorn 0.2631227 -0.177858 -0.44464 0.272422 0.3992282 -0.306738 ## Cladgrac 0.1956947 -0.311167 -0.23894 0.379013 0.4933026 0.037581 ## Cladfimb 0.0009213 -0.161418 0.18463 -0.435908 0.4831233 -0.143751 ## Cladcris 0.3373031 -0.470369 -0.05093 -0.823855 0.7182250 0.636140 ## Cladchlo -0.6200021 1.207278 0.21889 0.426447 1.9506082 0.120722 ## Cladbotr 0.5647242 1.047333 2.65330 0.907734 4.4946805 1.201655 ## Cladamau -0.6598144 -1.512880 0.83251 1.577699 -0.0407227 -1.419139 ## Cladsp -0.8209003 0.476164 -0.49752 -0.998241 -0.2393208 0.390785 ## Cetreric 0.2458192 -0.689228 -1.68427 -0.131681 0.7439412 2.374535 ## Cetrisla -0.3465221 1.362693 0.85897 0.396752 2.7526968 0.396591 ## Flavniva -1.4391907 -0.833589 -0.12919 0.007071 -1.4841375 2.956977 ## Nepharct 1.6813309 0.199484 -4.33509 2.229917 0.9561223 -5.472858 ## Stersp -0.5172793 -2.280900 0.99775 2.377013 -0.8892757 -1.441228 ## Peltapht 0.4035858 -0.043265 0.04538 0.711040 0.1824679 -0.841227 ## Icmaeric 0.0378754 -2.419595 0.72135 0.361302 -0.3736424 -2.092136 ## Cladcerv -0.9232858 -0.005233 -1.22058 0.305290 -0.8142627 0.414135 ## Claddefo 0.5190399 -0.496632 -0.15271 -0.695927 0.9042143 0.909191 ## Cladphyl -1.2836161 1.155872 -0.79912 -0.741170 -0.1608002 0.490526 ## ## ## Site scores (weighted averages of species scores) ## ## CA1 CA2 CA3 CA4 CA5 CA6 ## sit1 -0.108122 -0.53705 0.229574 0.24412 0.1405624 -0.14253 ## sit2 0.697118 -0.14441 -0.031788 -0.21743 -0.2738522 -0.08146 ## sit3 0.987603 0.15042 -1.348447 0.80472 0.3095168 0.46773 ## sit4 0.851765 0.49901 0.443559 0.12277 -0.4814871 0.07589 ## sit5 0.359881 -0.05608 0.145813 0.15087 0.2405263 -0.17770 ## sit6 0.003545 0.37017 0.027760 0.06168 -0.1158930 -0.03413 ## sit7 0.860732 -0.11504 0.110869 -1.02169 0.0772348 -0.60530 ## sit8 0.636936 -0.33250 0.001120 -0.79797 0.0130769 -0.54049 ## sit9 1.279352 0.81557 0.670053 0.23137 -0.8929976 0.41783 ## sit10 -0.195009 -0.80564 0.117686 -0.58286 -0.0007212 0.53071 ## sit11 0.528532 -0.70420 -0.517771 -0.86836 0.5713441 0.91671 ## sit12 0.382866 -0.18686 -0.004789 0.10156 0.0458125 0.21087 ## sit13 0.990715 0.11967 -1.110040 0.44929 0.1885902 -0.70694 ## sit14 -0.264704 -1.06013 0.334900 0.45973 -0.0326631 -0.19945 ## sit15 -0.428410 -1.20765 0.374344 0.74970 -0.2596294 -0.30467 ## sit16 -0.330534 -0.77498 0.130760 0.22391 0.0632686 0.09060 ## sit17 -0.899601 0.12075 -0.075742 0.03842 -0.1489585 -0.12031 ## sit18 -0.770294 -0.35351 -0.033779 -0.01795 -0.3007839 0.44303 ## sit19 -0.992193 0.50319 -0.157505 -0.07070 -0.1065172 -0.09928 ## sit20 -0.937173 0.78688 -0.258119 -0.19377 -0.0343535 -0.01259 ## sit21 -0.726413 0.49163 -0.157235 -0.08698 -0.0105774 -0.02801 ## sit22 -1.002083 0.71239 -0.236526 -0.18643 -0.0231666 -0.04928 ## sit23 -0.322647 -0.03871 -0.001297 0.09029 -0.1481448 0.06934 ## sit24 0.259527 0.80746 1.124258 0.36083 1.5437866 0.07051 varespec_eigenval &lt;- eigenvals(vare_cca, scaling = 1) prop_expl &lt;- varespec_eigenval / sum(varespec_eigenval) par(mfrow = c(1, 2)) plot(x = 1:length(varespec_eigenval), y = vare_cca$CA$eig, type = &quot;b&quot;, xlab = &quot;Rang de la valeur propre&quot;, ylab = &quot;Valeur propre&quot;) abline(h = mean(varespec_eigenval), col = &quot;red&quot;, lty = 2) plot(x = 1:length(varespec_eigenval), y = prop_expl, type = &quot;b&quot;, xlab = &quot;Rang de la valeur propre&quot;, ylab = &quot;Valeur propre&quot;) lines(x = 1:length(varespec_eigenval), y = broken_stick(length(varespec_eigenval)), col = &quot;red&quot;, lty = 2) Créons les biplots. par(mfrow = c(1, 2)) plot(vare_cca, scaling = 1, main = &quot;Biplot des espèces&quot;) plot(vare_cca, scaling = 2, main = &quot;Biplot des sites&quot;) Le biplot des espèces, à gauche (scaling = 1), montre la distribution des sites selon les espèces. Les emplacements des scores (en noir) montrent les contrastes entre sites selon les espèces qui les recouvrent. Les sites 14 et 15, par exemple, contrastent les sites 19, 20, 21 et 22 selon le 2ième axe principal. Par ailleurs, les axes principaux sont formé de plusieurs espèces dont aucune ne domine clairement. Le biplot des sites, à droite (scaling = 2), montre la distribution des recouvrements d’espèces selon les sites. Par exemple, les espèces Betupube (Betula pubescens) et Barbhatc (Barbilophozia hatcheri ) se recouvrent en particulier le site 24. Le site 1 est difficile à identifier, car il est couvert par plusieurs noms d’espèces, au bas au centre. Les sites 3 et 13 se confondent avec Dicrsp (une espèce de Dicranum) qui le recouvre amplement. Pour les deux types de biplot, les sites où les espèces situés près de l’origine, car ils peuvent être soit près de la moyenne, soit distribués uniformément. Le nombre de composantes à retenir peut être évalué par les approches Kaiser-Guttmann et broken-stick. scaling &lt;- 1 varespec_eigenval &lt;- eigenvals(vare_cca, scaling = scaling) # peut être effectué sur les deux types de scaling prop_expl &lt;- varespec_eigenval / sum(varespec_eigenval) par(mfrow = c(1, 2)) plot(x = 1:length(varespec_eigenval), y = vare_cca$CA$eig, type = &quot;b&quot;, xlab = &quot;Rang de la valeur propre&quot;, ylab = &quot;Valeur propre&quot;, main = paste(&quot;Eigenvalue - Kaiser-Guttmann, scaling =&quot;, scaling)) abline(h = mean(varespec_eigenval), col = &quot;red&quot;, lty = 2) plot(x = 1:length(varespec_eigenval), y = prop_expl, type = &quot;b&quot;, xlab = &quot;Rang de la valeur propre&quot;, ylab = &quot;Valeur propre&quot;, main = paste(&quot;Proportion - broken stick, scaling =&quot;, scaling)) lines(x = 1:length(varespec_eigenval), y = broken_stick(length(varespec_eigenval)), col = &quot;red&quot;, lty = 2) Pour les deux scalings, l’approche Kaiser-Guttmann propose 7 axes, tandis que l’approche broken-stick en propose 5. Les représentations biplot d’analyse de correspondance peuvent prendre la forme d’un boomerang, en particulier celles qui sont basées sur des données d’occurence. Le tableau suivant initialement de Chessel et al. (1987) et est distribué dans le module ade4. library(&quot;ade4&quot;) data(&quot;doubs&quot;) fish &lt;- doubs$fish doubs_cca &lt;- cca(fish %&gt;% filter(rowSums(.) &gt; 0)) plot(doubs_cca, scaling = 2) Les numéros de sites correspondent à la position dans une rivière, 1 étant en amont et 30 en aval. Le premier axe discrimine l’amont et l’aval, tandis que le deuxième montre deux niches en amont. Bien que l’on observe une discontinuité dans le cours d’eau, il y a une continuité dans les abondances. Cet effet peut être corrigé en retirant la tendance de l’analyse de correspondance par une detrended correspondance analysis. Pour cela, il faudra utiliser la fonction decorana, ce qui ne sera pas couvert ici. L’analyse des correspondances multiples (ACM) est utile pour l’ordination des données catégorielles. Le module ade4 est en mesure d’effectuer des AMC, mais n’est pas couvert dans ce manuel. Excercice. Effectuez et analysez une AC avec les données de recouvrement varespec. 8.4.1.3 Positionnement multidimensionnel (PoMd) Le positionnement multidimensionnel (PoMd), ou manifold analysis, se base sur les assiciations entre les objets (mode Q) ou les variables (mode R) pour en réduire les dimensions. Alors que l’analyse en composantes principales conserve la distance euclidienne et que l’analyse de correspondance conserve la distance du \\(\\chi^2\\), le PoMd conserve l’association que vous sélectionnerez à votre convenance. Le PoMd vise à représenter en un nombre limité de dimensions (souvent 2) la distance (ou dissimilarité) qu’ont les objets (ou des variables) les uns par rapport aux autres dans l’espace multidimensionnel. Il existe deux types d’AEM. Le PoMd-métrique (metric multidimentional scaling MMDS, parfois le metric est retiré, MDS, et parfois l’on parle de classic MDS) vise à représenter fidèlement la distance entre les objets ou les variables. Le PoMd-métrique ne devrait être utilisée que lorsque la métrique n’est ni euclidienne, ni de \\(\\chi^2\\) et que l’on désire préserver les distances entre les objets. L’PoMd-métrique aussi appelée analyse en coordonnées principales (ACoP ou de l’anglais PCoA) . Le PoMd-non-métrique (nonmetric multidimentional scaling, NMDS) vise quant à lui à représenter l’ordre des distances entre les objets ou les variables. C’est une approche par rang: le PoMd-non-métrique vise représenter les objets sont plus proches ou plus éloignées les uns des autres plutôt que de représenter leur similarité dans l’espace multidimentionnelle. L’IsoMap, pour isometric feature mapping, est une extension du PoMd qui recontruit les distances selon les points retrouvés dans le voisinage. Les isomaps sont en mesure d’applatir des données ayant des formes complexes. Nous ne traitons pour l’instant que de l’PoMd-métrique (fonction vegan::cmdscale) et des PoMd-non-métrique (fonction vegan::metaMDS). 8.4.1.3.1 Application Utilisons les données d’abondance que nous avions au tout début de ce chapitre. La matrice d’association de Bray-Curtis sera utilisée. assoc_mat &lt;- vegdist(abundance, method = &quot;bray&quot;) pheatmap(assoc_mat %&gt;% as.matrix(), cluster_rows = FALSE, cluster_cols = FALSE, display_numbers = round(assoc_mat %&gt;% as.matrix(), 2)) Les sites 2 et 3 devraient être plus près l’un et l’autre, puis les sites 3 et 4. Les autres associations sont éloignés d’environ la même distance. Lançons le calcul de la PoMd-métrique. pcoa &lt;- cmdscale(assoc_mat, k = nrow(abundance)-1, eig = TRUE) spec_scores &lt;- wascores(pcoa$points, abundance) ordiplot(vegan::scores(pcoa), type = &#39;t&#39;, cex = 1.5) ## species scores not available text(spec_scores, row.names(spec_scores), col = &quot;red&quot;, cex = 0.75) On observe en effet que les sites 2 et 3 sont les plus près. Les sites 3 et 4sont plus éloignés. Les sites 1, 2 et 4 font à peu près un triangle équilatéral, ce qui correspond à ce à quoi on devrait s’attendre. Les wa-scores permettent de juxtaposer les espèces sur les sites, pour référence. Le colibri n’est présent que sur le site 2. Le site 1 est populé par des jaseurs et des mésanges, et c’est le seul site où l’on a observé une citelle. On a observé des chardonnerets sur les sites 2 et 3. Sur le site 4, on n’a observé que des bruants, que l’on a aussi observé ailleurs, sauf au site 2. Le PoMd-non-métrique (non metric dimensional scaling, NMDS) fonctionne de la même manière que la PoMd-métrique, à la différence que la distance est basée sur les rangs. À cet égard, le site 4 à une distance de 0.76 du site 3, mais plutôt le deuxième plus loin, après le site 2 et avant le site 1. Utilisons la fonction metaMDS. nmds &lt;- metaMDS(assoc_mat, k = nrow(abundance)-1, eig = TRUE) ## Run 0 stress 0 ## Run 1 stress 0 ## ... Procrustes: rmse 0.132002 max resid 0.1663687 ## Run 2 stress 0 ## ... Procrustes: rmse 0.1221812 max resid 0.1810138 ## Run 3 stress 0 ## ... Procrustes: rmse 0.1456404 max resid 0.1951647 ## Run 4 stress 0 ## ... Procrustes: rmse 0.1241779 max resid 0.1611525 ## Run 5 stress 0 ## ... Procrustes: rmse 0.1545408 max resid 0.2045281 ## Run 6 stress 0 ## ... Procrustes: rmse 0.1737747 max resid 0.2306282 ## Run 7 stress 0 ## ... Procrustes: rmse 0.05097609 max resid 0.07169513 ## Run 8 stress 0 ## ... Procrustes: rmse 0.149294 max resid 0.2006741 ## Run 9 stress 0 ## ... Procrustes: rmse 0.1469209 max resid 0.1920626 ## Run 10 stress 0 ## ... Procrustes: rmse 0.1049836 max resid 0.1441087 ## Run 11 stress 0 ## ... Procrustes: rmse 0.1205633 max resid 0.1552559 ## Run 12 stress 0 ## ... Procrustes: rmse 0.1038427 max resid 0.1335605 ## Run 13 stress 0 ## ... Procrustes: rmse 0.150115 max resid 0.1996867 ## Run 14 stress 0 ## ... Procrustes: rmse 0.09195021 max resid 0.1300376 ## Run 15 stress 0 ## ... Procrustes: rmse 0.1661097 max resid 0.225884 ## Run 16 stress 0 ## ... Procrustes: rmse 0.09325455 max resid 0.1118114 ## Run 17 stress 0 ## ... Procrustes: rmse 0.07740563 max resid 0.1076017 ## Run 18 stress 5.723476e-05 ## ... Procrustes: rmse 0.1505751 max resid 0.1911405 ## Run 19 stress 0 ## ... Procrustes: rmse 0.0626549 max resid 0.09201548 ## Run 20 stress 0 ## ... Procrustes: rmse 0.1501717 max resid 0.1926781 ## *** No convergence -- monoMDS stopping criteria: ## 20: stress &lt; smin ## Warning in metaMDS(assoc_mat, k = nrow(abundance) - 1, eig = TRUE): stress ## is (nearly) zero: you may have insufficient data spec_scores &lt;- wascores(nmds$points, abundance) ordiplot(vegan::scores(nmds), type = &#39;t&#39;, cex = 1.5) ## species scores not available text(spec_scores, row.names(spec_scores), col = &quot;red&quot;, cex = 0.75) Dans ce cas, entre PoMd-métrique et non-métrique, les résultats peuvent être interprétés de manière similaire. En ce qui a trait au dauphin, Pour plus de détails, je vous invite à vous référer à Borcard et al. (2011)) ou de consulter l’excellent site GUSTA ME. 8.4.1.4 Conclusion sur l’ordination non contraignante Lorsque les données sont euclidiennes, l’analyse en composantes principales (ACP) dervait être utilisée. Lorsque la métrique est celle du \\(\\chi^2\\), on préférera l’analyse de correspondance (AC). Si la métrique est autre, le positionnement multidimensionel (PoMd) est préférable. Dans ce dernier cas, si l’on recherche une représentation simplifiée de la distance entre les objets ou variables, on utilisera un PoMd-métrique. À l’inverse, si l’on désire une représentation plus fidèle au rang des distances, on préférera l’PoMd-non-métrique. 8.4.2 Ordination contraignante Alors que l’ordination non contraignante vous permet de dresser un protrait de vos variables, l’ordination contraignante (ou canonique) permet de tester statistiquement ainsi que de représenter la relation entre plusieurs variables explicatives (par exemple, des conditions environnementales) et une ou plusieurs variables réponses (par exemple, les espèces observées). L’analyse discriminante n’a fondamentalement qu’une seulement variable réponse, et celle-ci doit décrire l’appartenance à une catégorie. L’analyse de redondance sera préférée lorsque le nombre de variable est plus restreint (variables ionomiques et indicateurs de performance des cultures). Les détails, ainsi que les tenants et aboutissants de ces méthodes, sont présentés dans Numerical Ecology (Legendre et Legendre, 2012). L’analyse canonique des corrélations sera préférée lorsque les variables sont parsemées (beaucoup de colonnes avec beaucoup de zéros, comme les variables d’abondance). 8.4.2.1 Analyse discriminante Alors que l’analyse en composante principale vise à présenter la perspective (les axes) selon laquelle les points sont les plus éclatées, l’analyse discriminante, le plus souvent utilisé dans sa forme linéaire (ADL) et quadratique (ADQ), vise à présenter la perspective selon laquelle les groupes sont les plus éclatés, les groupes formant la variable contraignante. Ces groupes peuvent être connus (e.g. cultivar, région géographique) ou attribués (exemple: par partitionnement). L’ADL est parfois nommée analyse canonique de la variance. L’AD vise à représenter des différences entre des groupes aux moyens de combinaisons linéaires (ADL) ou quadratique (ADQ) de variables mesurées. Sa représentation sous forme de biplot permet d’apprécier les différences entre les groupes d’identifier les variables qui sont responsables de la discrimination. Biplot de distance de l’analyse discriminante des ionomes d’espèces de plantes à fruits cultivées sauvages et domestiquées, Source: Parent et al. (2013) L’ADL a été développée par Fisher (1936), qui à titre d’exemple d’application a utilisé un jeu de données de dimensions d’iris collectées par Edgar Anderson, du Jardin botanique du Missouri, sur 150 spécimens d’iris collectés en Gaspésie (Est du Québec), ma région natale (suis-je assez chauvin?). Ce jeu de données est amplement utilisé à titre d’exemple en analyse multivariée. Williams (1983) a présenté les tenants et aboutissants de l’ADL en écologie. Tout comme les données passant pas une ACP doivent suivre une distribution multinormale pour être statistiquement valide, les distributions des groupes dans une ADL doivent être multinormales et les variances des points par groupe doivent être homogènes… ce qui est rarement le cas en science. Néanmoins: Heureusement, il y a des évidences dans la littérature que certaines d’entre [ces règles] peuvent être transgressées modérément sans de grands changement dans les taux de classification. Cette conclusion dépends, toutefois, de la sévérité des transgressions, et de facteurs structueaux comme la position relative des moyennes des populations et de la nature des dispersions. - Williams (1983) L’ADL peut servir autant d’outil d’interprétation que d’outil de classification, c’est à dire de prédire une catégorie selon les variables (chapitre 12). Dans les deux cas, lorsque le nombre de variables approchent le nombre d’observation, les résultats d’une ADL risque d’être difficilement interprétables. Le test approprié pour évaluer l’homodénéité de la covariance est le M-test de Box. Ce test est peu documenté dans la littérature, est rarement utilisé mais a la réputation d’être particulièrement sévère. Il est rare que des données écologiques aient des dispersions (covariances) homogènes. Contrairement à l’ADL, l’ADQ ne demande pas à ce que les dispersions (covariances) soient homogènes. Néanmoins, l’ADQ ne génère ni de scores, ni de loadings: il s’agit d’un outil pour prédire des catégories (classification), non pas d’un outil d’ordination. 8.4.2.1.1 Application Utilisons les données d’iris. data(&quot;iris&quot;) Testons la multinormalité par groupe. Rappelons-nous que pour considérer la distribution comme multinormale, la p-value de la distortion ainsi que la statistique de Kurtosis doivent être égale ou plus élevée que 0.05. La fonction split sépare le tableau en listes et la fonction map applique la fonction spécifiée à chaque élément de la liste. Cela permet d’effectuer des tests de multinormalité sur chacune des espèces d’iris. iris %&gt;% split(.$Species) %&gt;% map(~ mvn(.x %&gt;% select(-Species), mvnTest = &quot;mardia&quot;)$multivariateNormality) ## $setosa ## Test Statistic p value Result ## 1 Mardia Skewness 25.6643445196298 0.177185884467652 YES ## 2 Mardia Kurtosis 1.29499223711605 0.195322907441935 YES ## 3 MVN &lt;NA&gt; &lt;NA&gt; YES ## ## $versicolor ## Test Statistic p value Result ## 1 Mardia Skewness 25.1850115362466 0.194444483140265 YES ## 2 Mardia Kurtosis -0.57186635893429 0.567412516528727 YES ## 3 MVN &lt;NA&gt; &lt;NA&gt; YES ## ## $virginica ## Test Statistic p value Result ## 1 Mardia Skewness 26.2705981752915 0.157059707690356 YES ## 2 Mardia Kurtosis 0.152614173978342 0.878702546726567 YES ## 3 MVN &lt;NA&gt; &lt;NA&gt; YES Le test est passé pour toutes les espèces. Voyons maintenant l’homogénéité de la covariance. Pour ce faire, nous aurons besoin de la fonction boxM, disponible avec le module biotools. Pour que les covariances soient considérées comme égales, la p-vaule doit être supérieure à 0.05. library(&quot;heplots&quot;) ## ## Attaching package: &#39;heplots&#39; ## The following object is masked from &#39;package:pls&#39;: ## ## coefplot boxM(iris %&gt;% select(-Species), group = iris$Species) ## ## Box&#39;s M-test for Homogeneity of Covariance Matrices ## ## data: iris %&gt;% select(-Species) ## Chi-Sq (approx.) = 140.94, df = 20, p-value &lt; 2.2e-16 On est loin d’un cas où les distributions sont homogènes. Nous allons néanmoins procéder à l’analyse discriminante avec le module ade4. Nous aurons d’abord besoin d’effectuer une ACP avec la fonction dudi.pca de ade4 (en spécifiant une mise à l’échelle), que nous projeterons en ADL avec discrimin. library(&quot;ade4&quot;) iris_pca &lt;- dudi.pca(df = iris %&gt;% select(-Species), scannf = FALSE, # ne pas générer de graphique scale = TRUE) iris_lda &lt;- discrimin(dudi = iris_pca, fac = iris$Species, scannf = FALSE) La visualisation peut être effectuée directement sur l’objet issu de la fonction discrimin. plot(iris_lda) Il s’agit toutefois d’une visualisation pour le diagnostic davantage que pour la publication. Si l’objectif est la pubilcation, vous pourriez utiliser la fonction plotDA que j’ai conçue à cet effet. J’ai aussi conçu une fonction similaire qui utilise le module graphique de base de R. source(&quot;https://raw.githubusercontent.com/essicolo/AgFun/master/plotDA_gg.R&quot;) plotDA(scores = iris_lda$li, loadings = iris_lda$fa, fac = iris$Species, level=0.95, facname = &quot;Species&quot;, propLoadings = 1) ## Loading required package: ellipse ## ## Attaching package: &#39;ellipse&#39; ## The following object is masked from &#39;package:car&#39;: ## ## ellipse ## The following object is masked from &#39;package:graphics&#39;: ## ## pairs ## Loading required package: grid ## Loading required package: plyr ## ------------------------------------------------------------------------- ## You have loaded plyr after dplyr - this is likely to cause problems. ## If you need functions from both plyr and dplyr, please load plyr first, then dplyr: ## library(plyr); library(dplyr) ## ------------------------------------------------------------------------- ## ## Attaching package: &#39;plyr&#39; ## The following objects are masked from &#39;package:plotly&#39;: ## ## arrange, mutate, rename, summarise ## The following objects are masked from &#39;package:dplyr&#39;: ## ## arrange, count, desc, failwith, id, mutate, rename, summarise, ## summarize ## The following object is masked from &#39;package:purrr&#39;: ## ## compact À la différence de l’ACP, l’ADL maximise la sépatation des groupes. Nous avions noté avec l’ACP que les dimensions des pétales distingaient les groupes. Puisque nous avions justement des informations sur les groupes, nous aurions pu procéder directement à un ADL pour obtenir des conclusions plus directes. Si la longueur des pétales permet de distinguer l’espèce setosa des deux autres, la largeur des pétales permet de distinguer virginica et versicolor, bien que les nuages de points se superposent. De manière bivariée, les régions de confiance des moyennes des scores discriminants (petites ellipses) montrent des différence significatives au seuil 0.05. Excercice. Si l’on effectuait l’ADL sur notre dauphin, avec la colonne anatomy comme variable de regroupement, qu’obtiendrions-nous? Si l’on consière la nageoire codale (queue) comme faisant partie du corps? Quelles sont les limitations? 8.4.2.2 Analyse de redondance (RDA) En anglais, on la nomme redundancy analysis, souvent abrégée RDA. Elle est utilisée pour résumer les relations linéaires entre des variables réponse et des variables explicatives. La “redondance” se situe dans l’utilisation de deux tableaux de données contenant de l’information concordante. L’analyse de redondance est une manière élégante d’effectuer une régresssion linéaire multiple, où la matrice de valeurs prédites par la régression est assujettie à une analyse en composantes principales. Il est ainsi possible de superposer les scores des variables explicatives à ceux des variables réponse. Plus précisément, une RDA effectue les étapes suivantes (Borcard et al. (2011)) entre une matrice de variables indépendantes (explicatives) \\(X\\) et une matrice de variables dépendantes (réponse) \\(Y\\). 8.4.2.2.1 1. Régression entre \\(Y\\) et \\(X\\) Pour chacune des variables réponse de \\(Y\\) (\\(y_1\\), \\(y_2\\), , \\(y_j\\)), effectuer une régression linéaire sur les variables explicatives \\(X\\). \\[\\hat{y}_j = b_j + m_{1, j} \\times x_1 + m_{2, j} \\times x_2 + ... + m_{i, j} \\times x_i\\] \\[\\hat{y}_j = y_j + y_{res, j}\\] Pour chaque observation (\\(n\\)), nous obtenons une série de valeurs de \\(\\hat{y}_j\\) et de \\(y_{res, j}\\). Donc chaque cellule de la matrice \\(Y\\) a ses pendant \\(\\hat{y}\\) et \\(y_{res}\\). Nous obtenons ainsi une matrice de prédiction \\(\\hat{Y}\\) et une matrice des résidus \\(Y_{res} = Y - \\hat{Y}\\). 8.4.2.2.2 2. Analyse en composantes principales Ensuite, on effectue une analyse en composantes principales (ACP) sur la matrice des prédictions \\(\\hat{Y}\\). On obtient ainsi ses valeurs et vecteurs propres. Nommons \\(U\\) ses vecteurs propres. Les fonctions de RDA mettent souvent ces veceturs à l’échelle avant de les retourner à l’utilisateur. En ordination écologique, ces vecteurs mis à l’échelle sont souvent appelés les scores des espèces, bien qu’il ne s’agisse pas nécessairement d’espèces, mais plus généralement des variables de la matrice dépendante \\(Y\\). Il est aussi possible d’effectuer une ACP sur \\(Y_{res}\\). 8.4.2.2.3 3. Calculer les scores Les vecteurs propres \\(U\\) sont utilisés pour calculer les scores des sites, \\(Y \\times U\\), ainsi que les contraintes de site \\(\\hat{Y} \\times U\\). 8.4.2.2.4 Application Nous allons utiliser la fonction rda du module vegan. En ce qui a trait aux données, utilisons les données varespec (matrice Y) et varechem (matrice X). La fonction rda peut fonctionner avec l’interface-formule de R, où à gauche du ~ on retrouve le Y (la matrice de la communauté écologique, i.e. les abondances d’espèces) contre le X (l), à gauche, ce qui peut être pratique pour l’analyse d’intéractions. Mais pour comparer deux matrices, nous pouvons définir X et Y. Ce qui est mélangeant, c’est que vegan, contrairement aux conventions, défini X comme étant la matrice réponse et Y comme étant la matrice explicative. vare_rda &lt;- rda(X = varespec, Y = vareclr, scale = FALSE) par(mfrow = c(1, 2)) ordiplot(vare_rda, scaling = 1, type = &quot;text&quot;, main = &quot;Scaling 1: triplot de distance&quot;) ordiplot(vare_rda, scaling = 2, type = &quot;text&quot;, main = &quot;Scaling 2: triplot de corrélation&quot;) La fonction ordiplot permet de créer un triplot de base. La représentation des wascores est réputée plus robuste (moins susceptible d’être bruitée), mais leur interprétation porte à confusion (Borcard et al. (2011)). Triplot de distance (scaling 1). Les angles entre les variables explicatives représentent leur corrélation (non pas les variables réponse). Triplot de corrélation (scaling 2). Les angles entre les variables représentent leurs corrélation, que les variables soient réponse ou explicative, ou entre variables réponses et variables explicatives. Les distances entre les objets sur le triplot ne sont pas des approximation de leur distance euclidienne. Les triplots montrent que les variables ont toutes un rôle important sur la dispersion des sites autours des axeds principaux. Le premier axe principal est composé de manière plus marquée par le clr de l’Al et celui du Fe. Le deuxième axe principal est composé de manière plus marquée par le clr du S, du P et du K. Le triplot de corrélation ne présente pas de tendance appréciable pour la plupart des espèces, qui ne possèdent pas de niche particulière. Toutefois, l’espèce Cladstel, présente surtout dans les sites 9 et 10, est liée à de basses teneurs en N et à de faibles valeurs de Baresoil (sol nu). L’espèce Pleuschr est liée à des sols où l’on retrouve une grande épaisseur d’humus, ainsi que des teneurs élevées en nutriment K, P, S, Ca, Mg et Zn. Elle semble apprécier les sols à bas pH, mais à faible teneur en Fe et Al. La teneur en N lui semble plus indifférente (son vecteur étant presque perpendiculaire). On pourra personnaliser les graphiques en extrayant les scores. scaling &lt;- 2 sites &lt;- vegan::scores(vare_rda, display = &quot;wa&quot;, scaling = scaling) species &lt;- vegan::scores(vare_rda, display = &quot;species&quot;, scaling = scaling) env &lt;- vegan::scores(vare_rda, display = &quot;reg&quot;, scaling = scaling) plot(0, 0, type = &quot;n&quot;, xlim = c(-3, 5), ylim = c(-3, 4), asp = 1) abline(h=0, v = 0, col = &quot;grey80&quot;) text(sites/2, labels = rownames(sites), cex = 0.7, col = &quot;grey50&quot;) text(species/2, labels = rownames(species), col = &quot;green&quot;, cex = 0.7) segments(x0 = 0, y0 = 0, x = env[, 1], y = env[, 2], col = &quot;blue&quot;) text(env, labels = rownames(env), col = &quot;blue&quot;, cex = 1) On pourra effectuer une analyse de Kaiser-Guttmann ou de broken-stick de la même manière que précédemment. Étant une collection de régressions, une RDA est en mesure d’effectuer des tests statistiques sur les coefficients de la régression en utilisant des permutations pour tester la signification des coefficients et des axes d’une RDA. On doit néanmoins obligatoirement effectuer la RDA avec l’interface formule. L’a variable de gauche’objet à gauche du ~ peut être une matrice ou un tableau, et celui de droite est défini dans data. Le . dans l’interface formule signifie “une combinaison linéaire de toutes les variables, sans intéraction”. vare_rda &lt;- rda(varespec ~ ., data = vareclr, scale = FALSE) perm_test_term &lt;- anova(vare_rda, by = &quot;term&quot;) #perm_test_axis &lt;- anova(vare_rda, by = &quot;axis&quot;) La signification des axes est difficile à interpréter. Toutefois, celui des variables présente un intérêt. perm_test_term ## Permutation test for rda under reduced model ## Terms added sequentially (first to last) ## Permutation: free ## Number of permutations: 999 ## ## Model: rda(formula = varespec ~ N + P + K + Ca + Mg + S + Al + Fe + Mn + Zn + Mo + Fv + Baresoil + Humdepth + pH, data = vareclr, scale = FALSE) ## Df Variance F Pr(&gt;F) ## N 1 216.13 4.8470 0.007 ** ## P 1 272.71 6.1159 0.003 ** ## K 1 194.97 4.3724 0.015 * ## Ca 1 24.92 0.5589 0.657 ## Mg 1 52.61 1.1799 0.300 ## S 1 100.07 2.2441 0.095 . ## Al 1 177.91 3.9900 0.014 * ## Fe 1 118.59 2.6595 0.061 . ## Mn 1 25.96 0.5822 0.643 ## Zn 1 35.81 0.8030 0.486 ## Mo 1 23.51 0.5273 0.653 ## Baresoil 1 98.64 2.2122 0.109 ## Humdepth 1 43.59 0.9777 0.428 ## pH 1 38.93 0.8730 0.446 ## Residual 9 401.31 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 La p-value est la probabilité que les pentes calculées pour les variables émergent de distributions dont la moyenne est nulle. Au seuil 0.05, les variables significatives sont (les clr de) l’azote, le phosphore, le potassium et l’aluminium. Dans le cas des matrices d’abondance (ce n’est pas le cas de varespec, constituée de données de recouvrement), il est préférable avec les RDA de les transformer préalablement avec la transformation compositionnelle, de chord ou de Hellinger (chapitre 7). Une autre option est d’effectuer une RDA sur des matrices d’association en passant par une analyse en coordonnées principales (Legendre et Anderson, 1999). Enfin, les données d’abondance à l’état brutes devraient plutôt passer utiliser une analyse canonique des corrélations. 8.4.2.3 Analyse canonique des correspondances (ACC) L’analyse canonique des correspondances (Canonical correspondance analysis), ACC, a été à l’origine conçue pour étudier les liens entre des variables environnementales et l’abondance (décompte) ou l’occurence (présence-absence) d’espèces (ter Braak, 1986). L’ACC est à la RDA ce que la CA est à l’ACP. Alors que la RDA préserve les distance euclidiennes entre variables dépendantes et indpendantes, l’ACC préserve les distances du \\(\\chi^2\\). Tout comme l’AC, elle hérite du coup une propriété importate de la distance du \\(\\chi^2\\): il y a davantage davantage d’importance aux espèces rares. L’analyse des correspondances canoniques est souvent utilisée dans la littérature, mais dans bien des cas une RDA sur des données d’abondance transformées donnera des résultats davantage intérprétables (Legendre et Gallagher, 2001). 8.4.2.3.1 Application Cet exemple d’application concerne des données d’abondance. Nous allons conséquemment utiliser une CCA avec la fonction cca, toujours avec le module vegan. Les tableaux doubs_fish et doubs_env comprennent respectivement des données d’abondance d’espèces de poissons et dans différents environnements de la rivière Doubs (Europe) publiées dans Verneaux. (1973) et exportées du module ade4. data(&quot;doubs&quot;) doubs_fish &lt;- doubs$fish doubs_env &lt;- doubs$env Sur le site no 8, aucun poisson n’a pas été observé. Les observations ne comprenant que des zéro doivent être préalablement retirées. tot_spec &lt;- doubs_fish %&gt;% transmute(tot_spec = apply(., 1, sum)) doubs_fish &lt;- doubs_fish %&gt;% filter(tot_spec != 0) doubs_env &lt;- doubs_env %&gt;% filter(tot_spec != 0) De la même manière qu’avec la fonction rda de vegan, nous utilisons cca pour l’ACC. doubs_cca &lt;- cca(doubs_fish ~ ., data = doubs_env, scale = FALSE) Comparons les résultats par(mfrow = c(1, 2)) ordiplot(doubs_cca, scaling = 1, type = &quot;text&quot;, main = &quot;CCA - Scaling 1 - Triplot de distance&quot;) ordiplot(doubs_cca, scaling = 2, type = &quot;text&quot;, main = &quot;CCA - Scaling 2 - Triplot de corrélation&quot;) Triplot de distance (scaling 1). La projection des variables réponse à angle droit sur les variables explicatives est une approximation de la réponse sur l’explication. (2) Un objet (site ou réponse) situé près d’une variable explicative est plus susceptible d’avoir le décompte 1. (3) Les distances entre les variables (réponse et explicatives) approximent la distance du \\(\\chi^2\\) (traduction adaptée de Borcard et al. (2011)). Triplot de corrélation (scaling 2). La valeur optmiale de l’espèce sur une variable environnementale quantitative peut être obtenue en projetant l’espèce à angle droit sur la variable. (2) Une espèce se trouvant près d’une variable environnementale est susceptible de se trouver en plus grande abondance aux sites de statut 1 pour cette variable. (3) Les distances n’approximent pas la distance du \\(\\chi^2\\) (traduction adaptée de Borcard et al. (2011)). "],
["chapitre-outliers.html", "9 Détection de valeurs aberrantes et imputation de données manquantes 9.1 Données manquantes, définition, origine, typologie et traitement 9.2 Valeurs et échantillons aberrants: définition, origines, méthodes de détection et traitement", " 9 Détection de valeurs aberrantes et imputation de données manquantes ️ Objectifs spécifiques: À la fin de ce chapitre, vous saurez comment procéder à l’imputation de valeurs manquantes en mode univarié et multivarié saurez comment détecter des valeurs aberrantes en mode univarié et multivarié Note. Ce chapitre a été initialement rédigé par Zonlehoua Coulibali, qui a gracieusement accepté de contribuer à ces notes de cours. Le texte a été adapté au format du manuel par Serge-Étienne Parent. Les données écologiques sont généralement recueillies à différentes échelles, concernent plusieurs sites et plusieurs variables (corrélées ou non), impliquent différents individus de différentes agences et peuvent s’étendre sur plusieurs années (Alameddine et al., 2010; Lokupitiya et al., 2006). De ce fait, la plupart de ces bases de données contiennent des valeurs manquantes et/ou aberrantes liées à différentes sources d’erreurs, pouvant parfois limiter l’utilité des inférences statistiques (Collins et al., 2001; Glasson-Cicognani et Berchtold, 2010). Il convient alors de les traiter correctement avant d’effectuer les analyses statistiques car les ignorer peut entraîner, outre une perte de précision, de forts biais dans les modèles d’analyse (Alameddine et al., 2010; Filzmoser et al., 2008; Glasson-Cicognani et Berchtold, 2010). 9.1 Données manquantes, définition, origine, typologie et traitement 9.1.1 Définition Les tableaux de données sont organisés en lignes et colonnes. Les lignes représentent les observations, les unités, les sujets ou les cas étudiés selon le contexte, et les colonnes représentent les variables mesurées pour chaque observation. Les entrées qui sont les valeurs (ou contenus) des cellules ou encore les valeurs observées, peuvent être des valeurs continues, ou des valeurs catégoriales (Little et Rubin, 2002). Considérant une variable aléatoire \\(X\\) quelconque, une donnée manquante \\(x_m\\), est une donnée pour laquelle la valeur de la variable \\(X\\) est inconnue (ou absente). En d’autres termes, on ne dispose pas de la valeur de \\(X\\) pour le sujet \\(i\\) donné. C’est une donnée non disponible qui serait utile pour l’analyse si elle était observée (Ware et al., 2012). La littérature sur les données manquantes est plus abondante dans les domaines des sciences sociales sur les données d’enquêtes, et des sciences médicales (Davey et al., 2001; Graham, 2012). Pour représenter leur répartition dans la table de données, une matrice indicatrice des valeurs manquantes \\(M = (m_{ij})\\) est généralement utilisée où \\(m_{ij}\\) est une variable binaire qui prend la valeur 1 si la valeur de la variable (\\(X\\)) est observée et 0 si \\(x\\) est absent (Collins et al., 2001; Graham, 2012; Little et Rubin, 2002). 9.1.2 Origines des données manquantes Les données manquantes ont des origines matérielles diverses. Des valeurs peuvent être absentes soit parce qu’elles n’ont pas été observées, ou qu’elles ont été perdues ou étaient incohérentes (Glasson-Cicognani et Berchtold, 2010. La donnée peut avoir été perdue lors de la collecte ou du processus d’enregistrement des données, non mesurée en raison du dysfonctionnement d’un équipement, non mesurable en raison de la disparition du sujet d’étude (mort, fugue, champ non récolté, etc.), écartée en raison d’une contamination, oubliée, non étudiée, etc. 9.1.3 Profils des données manquantes Les auteurs traitant des données manquantes distinguent des formes de répartition des données manquantes et des mécanismes conduisant à ces dernières. La répartition des données manquantes décrit les dispositions des valeurs présentes et celles qui sont manquantes dans la matrice indicatrice. Les mécanismes à l’origine des données manquantes décrivent la relation probabiliste entre les valeurs observées et les valeurs manquantes de la table de données. 9.1.3.1 Répartition des données manquantes Les données manquantes se répartissent selon différents cas de figures (Graham, 2012; Little et Rubin, 2002) dont les trois principaux sont les valeurs manquantes univariées, les valeurs manquantes monotones et celles non monotones ou arbitraires. Cette distinction est fonction de la matrice indicatrice des valeurs manquantes. Cette matrice est dite à valeurs manquantes univariées ou de non-réponse univariée, lorsque pour une variable donnée, si une observation est absente, alors toutes les observations suivantes pour cette variable sont absentes (figure 9.1a). En expérimentation agricole, ce cas de figure est qualifié de problème de la parcelle manquante où, pour une raison quelconque (par exemple : une absence de germination, une destruction accidentelle d’une parcelle ou des enregistrements incorrects), un facteur à l’étude est non disponible. Les valeurs manquantes monotones surviennent lorsque la valeur d’une variable \\(Y_j\\) manquante pour un individu \\(i\\) implique que toutes les variables suivantes \\(Y_k\\) (\\(k &gt; j\\)) sont manquantes pour cet individu (figure 9.1b). Les valeurs manquantes arbitraires ou non monotones ou encore générales, surviennent lorsque la matrice ne dessine spécifiquement aucune des formes précédentes (figure 9.1c). Figure 9.1: Exemple de profils de données manquantes Le module VIM permet de visualiser la structure des données manquantes. ## Loading required package: colorspace ## Loading required package: data.table ## data.table 1.12.0 Latest news: r-datatable.com ## ## Attaching package: &#39;data.table&#39; ## The following objects are masked from &#39;package:dplyr&#39;: ## ## between, first, last ## The following object is masked from &#39;package:purrr&#39;: ## ## transpose ## VIM is ready to use. ## Since version 4.0.0 the GUI is in its own package VIMGUI. ## ## Please use the package to use the new (and old) GUI. ## Suggestions and bug-reports can be submitted at: https://github.com/alexkowa/VIM/issues ## ## Attaching package: &#39;VIM&#39; ## The following object is masked from &#39;package:dbscan&#39;: ## ## kNN ## The following object is masked from &#39;package:datasets&#39;: ## ## sleep Pour l’exemple, prenons le tableau iris puis remplaçons au hasard des données par des valeurs manquantes (NA), puis vérifions les proportions de données manquantes et les proportions de combinaisons de données manquantes. set.seed(2868374) data(&quot;iris&quot;) iris_NA &lt;- iris n_NA &lt;- 20 row_NA &lt;- sample(1:nrow(iris), n_NA, replace = TRUE) col_NA &lt;- sample(1:ncol(iris), n_NA, replace = TRUE) for (i in 1:n_NA) iris_NA[row_NA[i], col_NA[i]] &lt;- NA summary(aggr(iris_NA, sortVar = TRUE)) ## ## Variables sorted by number of missings: ## Variable Count ## Petal.Length 0.04666667 ## Petal.Width 0.03333333 ## Sepal.Length 0.02000000 ## Sepal.Width 0.02000000 ## Species 0.01333333 ## ## Missings per variable: ## Variable Count ## Sepal.Length 3 ## Sepal.Width 3 ## Petal.Length 7 ## Petal.Width 5 ## Species 2 ## ## Missings in combinations of variables: ## Combinations Count Percent ## 0:0:0:0:0 131 87.3333333 ## 0:0:0:0:1 2 1.3333333 ## 0:0:0:1:0 4 2.6666667 ## 0:0:1:0:0 6 4.0000000 ## 0:0:1:1:0 1 0.6666667 ## 0:1:0:0:0 3 2.0000000 ## 1:0:0:0:0 3 2.0000000 Avec la fonction matrixplot, il est possible de visualiser les données manquantes en rouge, tandis que les données présentes prennent un niveau de gris selon leur valeur. matrixplot(iris_NA) 9.1.3.2 Mécanismes conduisant aux données manquantes Les mécanismes conduisant aux données manquantes décrivent la relation entre les valeurs manquantes et celles observées des variables de la table (Collins et al., 2001; Graham, 2012; Little et Rubin, 2002). En considérant la table de donnée \\(Y = \\{O,M\\}\\) où \\(O = \\left[ o_{i, j} \\right]\\) représente les données observées et \\(M = \\left[ m_{i, j} \\right]\\) la matrice indicatrice des données manquantes, le mécanisme à l’origine des données manquantes est défini par la distribution conditionnelle de \\(M\\) sachant \\(Y\\). Lorsque la probabilité qu’une valeur soit manquante ne dépend ni des valeurs observées, ni de celles manquantes, les données sont dites manquantes complètement au hasard (* MCAR, missing completely at random*). La probabilité d’absence est donc la même pour toutes les observations et elle ne dépend que de paramètres extérieurs indépendants de cette variable (Collins et al., 2001; Graham, 2012; Heitjan, 1997; Little et Rubin, 2002; Rubin, 1976). Avec de telles données (MCAR), les régressions qui n’utilisent que les enregistrements complets, les moyennes des cas disponibles, les tests non-paramétriques et les méthodes basées sur les “moments”, sont toutes valides (Heitjan, 1997). Toutefois, une perte de précision est à prévoir dans les résultats (Collins et al., 2001). Selon les mêmes auteurs, lorsque la probabilité qu’une valeur soit manquante dépend uniquement de la composante observée “O” (une ou plusieurs variables observées) mais pas des valeurs manquantes elles-mêmes, les données sont dites manquantes au hasard (* MAR: missing at random*). Dans ce cas, les méthodes du maximum de vraisemblance sont valides pour estimer les paramètres du modèle. Les procédures d’imputation multiples utilisent implicitement le mécanisme MAR (Collins et al., 2001; Heitjan, 1997). Lorsque la probabilité qu’une valeur manque dépend de la valeur non observée de la variable elle-même (\\(M\\)), les données ne manquent pas au hasard (* MNAR: missing not at random*). Ce type de données ne doit pas être ignoré dans l’ajustement de modèles car elles induisent une perte de précision (inhérente à tout cas de données manquantes) mais aussi un biais dans l’estimation des paramètres (Collins et al., 2001; Heitjan, 1997). 9.1.4 Traitement des données manquantes La présence de données manquantes dans une analyse peut conduire à des estimés de paramètres biaisés, gonfler les erreurs de type I et II, baisser les performances des intervalles de confiance (Collins et al., 2001) et entacher la généralisation des résultats (Taylor et al., 2002). Plusieurs méthodes existent pour calculer des estimés de paramètres de modèles approximativement sans biais, en présence de données manquantes. 9.1.4.1 L’analyse des cas complets Cette méthode consiste à exclure du fichier de données tous les individus ayant au moins une donnée manquante (Glasson-Cicognani et Berchtold, 2010. Elle serait la plus utilisée pour traiter les valeurs manquantes mais n’est efficace que pour les cas de données manquant complètement au hasard (MCAR) lorsque le nombre de d’observations à éliminer n’est pas trop important (Davey et al., 2001). En R, de manière générique, il est possible d’identifier une donnée manquante dans un tableau, une matrice ou un vecteur avec is.na, qui retourne un objet booléen (TRUE / FALSE). La fonction any permet d’identifier si au moins une valeur est vraie ou fausse dans un objet, alors que la fonction all permet d’identifier si toutes les valeurs sont vraies. On pourra vérifier si une ligne contient une valeur manquante avec la fonction apply, dans l’axe des lignes. Il faudra toutefois inverser le résultat booléen avec un ! pour faire en sorte que l’on écarte les valeurs manquantes. row_missing &lt;- iris_NA %&gt;% filter(apply(., 1, function(x) any(is.na(x)))) row_complete &lt;- iris_NA %&gt;% filter(!apply(., 1, function(x) any(is.na(x)))) row_missing ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 NA 3.4 1.4 0.3 setosa ## 2 4.4 NA 1.4 0.2 setosa ## 3 5.4 3.7 NA 0.2 setosa ## 4 5.7 3.8 NA NA setosa ## 5 5.1 NA 1.7 0.5 setosa ## 6 4.7 3.2 1.6 NA setosa ## 7 NA 3.4 1.5 0.4 setosa ## 8 5.0 3.5 NA 0.3 setosa ## 9 4.8 3.0 1.4 NA setosa ## 10 5.2 2.7 3.9 1.4 &lt;NA&gt; ## 11 5.6 3.0 NA 1.5 versicolor ## 12 6.1 2.8 NA 1.2 versicolor ## 13 5.4 3.0 4.5 1.5 &lt;NA&gt; ## 14 6.1 NA 4.6 1.4 versicolor ## 15 5.8 2.6 NA 1.2 versicolor ## 16 NA 2.7 5.1 1.9 virginica ## 17 6.7 2.5 5.8 NA virginica ## 18 6.3 3.4 5.6 NA virginica ## 19 6.4 3.1 NA 1.8 virginica Au lieu de apply, R fournit la fontion raccourci complete.cases. row_missing &lt;- iris_NA %&gt;% filter(complete.cases(.)) Le module tidyr (inclus dans tidyverse) nous facilite la vie avec la fonction tidyr::drop_na, qui retire toutes les lignes contenant au moins une valeur manquante. row_complete &lt;- iris_NA %&gt;% drop_na() De même, on pourra évaluer la proportion de données manquantes. nrow(row_complete) / nrow(iris) ## [1] 0.8733333 Ou bien, évaluer la proportion de donnée manquante par groupe. iris_NA %&gt;% group_by(Species) %&gt;% summarise_each(funs(sum(is.na(.))/length(.))) ## Warning: Factor `Species` contains implicit NA, consider using ## `forcats::fct_explicit_na` ## # A tibble: 4 x 5 ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 0.04 0.04 0.06 0.06 ## 2 versicolor 0 0.0208 0.0625 0 ## 3 virginica 0.02 0 0.02 0.04 ## 4 &lt;NA&gt; 0 0 0 0 Pour terminer cette section, il est possible que certaines variables soient peu mesurées dans une étude. Au jugement, on pourra sacrifier une colonne contenant plusieurs données manquantes en vue de conserver des lignes. 9.1.4.2 L’imputation L’imputation permet de créer des bases de données complètes (Donzé, 2001). Elle corrige la non-réponse partielle en substituant une “valeur artificielle” à la valeur manquante. Les auteurs distinguent l’imputation unique et l’imputation multiple. 9.1.4.2.1 L’imputation unique L’imputation unique consiste à remplacer chaque donnée manquante par une seule valeur plausible telle que la moyenne calculée sur les données réellement observées, l’imputation par le ou les plus proche(s) voisin(s) (la technique des plus proches voisins est couverte au chapitre 12). Cette dernière remplace les données manquantes par des valeurs provenant d’individus similaires pour lesquels toute l’information a été observée. L’imputation peut aussi se faire par régression en remplaçant les valeurs manquantes par des valeurs prédites selon un modèle de régression ou des méthodes bayésiennes plus sophistiquées. L’imputation unique est valide en présence de données manquantes de type MAR (Davey et al., 2001; Donzé, 2001; Glasson-Cicognani et Berchtold, 2010. Selon Heitjan (1997), il n’existe pas de règles strictes pour décider quand il faut entreprendre une imputation multiple. Néanmoins, si la fraction des observations avec des données manquantes est inférieure à par exemple 5%, et le mécanisme est ignorable (MCAR ou MAR), les analyses les plus simples sont satisfaisantes. Bien que conçu principalement pour l’imputation multiple (on y arrive bientôt), le module mice permet l’imputation univariée. Nous allons tester l’imputation par la moyenne. Voyons par exemple la moyenne des longueurs des sépales. mean(iris_NA$Sepal.Length[!complete.cases(iris_NA)], na.rm = TRUE) ## [1] 5.54375 Lançons l’imputation par la fonction mice, puis la prédiction du tableau imputé par la fonction complete. library(&quot;mice&quot;) iris_mice &lt;- mice(iris_NA, method = &quot;mean&quot;) iris_imp &lt;- complete(iris_mice) Le tableau original peut être comparé au tableau imputé. iris_NA[!complete.cases(iris_NA), ] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 7 NA 3.4 1.4 0.3 setosa ## 9 4.4 NA 1.4 0.2 setosa ## 11 5.4 3.7 NA 0.2 setosa ## 19 5.7 3.8 NA NA setosa ## 24 5.1 NA 1.7 0.5 setosa ## 30 4.7 3.2 1.6 NA setosa ## 32 NA 3.4 1.5 0.4 setosa ## 41 5.0 3.5 NA 0.3 setosa ## 46 4.8 3.0 1.4 NA setosa ## 60 5.2 2.7 3.9 1.4 &lt;NA&gt; ## 67 5.6 3.0 NA 1.5 versicolor ## 74 6.1 2.8 NA 1.2 versicolor ## 85 5.4 3.0 4.5 1.5 &lt;NA&gt; ## 92 6.1 NA 4.6 1.4 versicolor ## 93 5.8 2.6 NA 1.2 versicolor ## 102 NA 2.7 5.1 1.9 virginica ## 109 6.7 2.5 5.8 NA virginica ## 137 6.3 3.4 5.6 NA virginica ## 138 6.4 3.1 NA 1.8 virginica iris[!complete.cases(iris_NA), ] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 7 4.6 3.4 1.4 0.3 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 11 5.4 3.7 1.5 0.2 setosa ## 19 5.7 3.8 1.7 0.3 setosa ## 24 5.1 3.3 1.7 0.5 setosa ## 30 4.7 3.2 1.6 0.2 setosa ## 32 5.4 3.4 1.5 0.4 setosa ## 41 5.0 3.5 1.3 0.3 setosa ## 46 4.8 3.0 1.4 0.3 setosa ## 60 5.2 2.7 3.9 1.4 versicolor ## 67 5.6 3.0 4.5 1.5 versicolor ## 74 6.1 2.8 4.7 1.2 versicolor ## 85 5.4 3.0 4.5 1.5 versicolor ## 92 6.1 3.0 4.6 1.4 versicolor ## 93 5.8 2.6 4.0 1.2 versicolor ## 102 5.8 2.7 5.1 1.9 virginica ## 109 6.7 2.5 5.8 1.8 virginica ## 137 6.3 3.4 5.6 2.4 virginica ## 138 6.4 3.1 5.5 1.8 virginica iris_imp[!complete.cases(iris_NA), ] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 7 5.862759 3.40 1.400000 0.300000 setosa ## 9 4.400000 3.06 1.400000 0.200000 setosa ## 11 5.400000 3.70 3.773759 0.200000 setosa ## 19 5.700000 3.80 3.773759 1.202797 setosa ## 24 5.100000 3.06 1.700000 0.500000 setosa ## 30 4.700000 3.20 1.600000 1.202797 setosa ## 32 5.862759 3.40 1.500000 0.400000 setosa ## 41 5.000000 3.50 3.773759 0.300000 setosa ## 46 4.800000 3.00 1.400000 1.202797 setosa ## 60 5.200000 2.70 3.900000 1.400000 &lt;NA&gt; ## 67 5.600000 3.00 3.773759 1.500000 versicolor ## 74 6.100000 2.80 3.773759 1.200000 versicolor ## 85 5.400000 3.00 4.500000 1.500000 &lt;NA&gt; ## 92 6.100000 3.06 4.600000 1.400000 versicolor ## 93 5.800000 2.60 3.773759 1.200000 versicolor ## 102 5.862759 2.70 5.100000 1.900000 virginica ## 109 6.700000 2.50 5.800000 1.202797 virginica ## 137 6.300000 3.40 5.600000 1.202797 virginica ## 138 6.400000 3.10 3.773759 1.800000 virginica Dans la colonne Sepal.Length, toutes les valeurs manquantes ont été remplacées par ~5.862. Exercice. Pourquoi la prédiction diffère-t-elle de la moyenne? 😱 Attention. Lorsque les valeurs sont systématiquement manquantes chez une catégorie, les estimateurs seront biaisés. iris_NA_biais_1 &lt;- tibble(Sepal.Length = c(5.3, NA, 4.9, NA, 4.7, NA), Species = c(&quot;setosa&quot;, &quot;versicolor&quot;, &quot;setosa&quot;, &quot;versicolor&quot;, &quot;setosa&quot;, &quot;versicolor&quot;)) mean(iris_NA_biais_1$Sepal.Length, na.rm = TRUE) ## [1] 4.966667 iris_NA_biais_2 &lt;- tibble(Sepal.Length = c(5.3, 7.0, 4.6, 6.4, 4.8, 6.9), Species = c(&quot;setosa&quot;, &quot;versicolor&quot;, &quot;setosa&quot;, &quot;versicolor&quot;, &quot;setosa&quot;, &quot;versicolor&quot;)) mean(iris_NA_biais_2$Sepal.Length, na.rm = TRUE) ## [1] 5.833333 Dans l’exemple précédent, les données sont systématiquement manquantes chez l’espèce versicolor. La moyenne de la longueur des sépales est donc biaisée, et l’imputation par la moyenne de sera tout autant. L’imputation par la moyenne est jugée non recommandable par plusieurs statisticiens. Dans la mesure du possible, l’imputation multiple devrait être favorisée à l’imputation univariée. 9.1.4.2.2 L’imputation multiple L’imputation multiple consiste à imputer plusieurs fois les valeurs manquantes et à combiner les résultats pour diminuer l’erreur causée par la complétion (Davey et al., 2001). Les valeurs manquantes sont remplacées par \\(M\\) (\\(M &gt; 1\\)) ensembles de valeurs simulées donnant lieu à \\(M\\) versions plausibles mais différentes des données complètes (Collins et al., 2001; Taylor et al., 2002). En pratique, seulement \\(M\\) allant de 5 à 10 (imputations) est suffisant pour produire des bonnes inférences (Collins et al., 2001; Donzé, 2001). Chacun des \\(M\\) ensembles de données est analysé de la même manière par des méthodes standards d’analyse de données complètes, et les résultats sont combinés en utilisant une arithmétique simple: les moyennes des paramètres estimés sont calculées, les erreurs standards sont combinées pour refleter l’incertitude des données manquantes et l’erreur d’échantillonnage. L’imputation multiple est une procédure basée sur un modèle (model-based). L’utilisateur doit spécifier un modèle de probabilité conjointe pour les données observées et manquantes (Collins et al., 2001; Taylor et al., 2002). Le module mice donne accès à plusieurs types de modèles (argument method). Les modèles cart et rf tombent la la catégorie de l’autoapprentissage (couvert au chapitre 12). Ils ont l’avantage important d’être applicables autant pour tout type de variable. iris_mice &lt;- mice(iris_NA, method = &quot;rf&quot;) iris_imp &lt;- complete(iris_mice) De même que précédemment, le tableau original peut être comparé au tableau imputé. iris_NA[!complete.cases(iris_NA), ] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 7 NA 3.4 1.4 0.3 setosa ## 9 4.4 NA 1.4 0.2 setosa ## 11 5.4 3.7 NA 0.2 setosa ## 19 5.7 3.8 NA NA setosa ## 24 5.1 NA 1.7 0.5 setosa ## 30 4.7 3.2 1.6 NA setosa ## 32 NA 3.4 1.5 0.4 setosa ## 41 5.0 3.5 NA 0.3 setosa ## 46 4.8 3.0 1.4 NA setosa ## 60 5.2 2.7 3.9 1.4 &lt;NA&gt; ## 67 5.6 3.0 NA 1.5 versicolor ## 74 6.1 2.8 NA 1.2 versicolor ## 85 5.4 3.0 4.5 1.5 &lt;NA&gt; ## 92 6.1 NA 4.6 1.4 versicolor ## 93 5.8 2.6 NA 1.2 versicolor ## 102 NA 2.7 5.1 1.9 virginica ## 109 6.7 2.5 5.8 NA virginica ## 137 6.3 3.4 5.6 NA virginica ## 138 6.4 3.1 NA 1.8 virginica iris[!complete.cases(iris_NA), ] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 7 4.6 3.4 1.4 0.3 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 11 5.4 3.7 1.5 0.2 setosa ## 19 5.7 3.8 1.7 0.3 setosa ## 24 5.1 3.3 1.7 0.5 setosa ## 30 4.7 3.2 1.6 0.2 setosa ## 32 5.4 3.4 1.5 0.4 setosa ## 41 5.0 3.5 1.3 0.3 setosa ## 46 4.8 3.0 1.4 0.3 setosa ## 60 5.2 2.7 3.9 1.4 versicolor ## 67 5.6 3.0 4.5 1.5 versicolor ## 74 6.1 2.8 4.7 1.2 versicolor ## 85 5.4 3.0 4.5 1.5 versicolor ## 92 6.1 3.0 4.6 1.4 versicolor ## 93 5.8 2.6 4.0 1.2 versicolor ## 102 5.8 2.7 5.1 1.9 virginica ## 109 6.7 2.5 5.8 1.8 virginica ## 137 6.3 3.4 5.6 2.4 virginica ## 138 6.4 3.1 5.5 1.8 virginica iris_imp[!complete.cases(iris_NA), ] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 7 4.8 3.4 1.4 0.3 setosa ## 9 4.4 3.7 1.4 0.2 setosa ## 11 5.4 3.7 1.4 0.2 setosa ## 19 5.7 3.8 4.2 1.5 setosa ## 24 5.1 3.5 1.7 0.5 setosa ## 30 4.7 3.2 1.6 0.2 setosa ## 32 4.9 3.4 1.5 0.4 setosa ## 41 5.0 3.5 1.5 0.3 setosa ## 46 4.8 3.0 1.4 0.2 setosa ## 60 5.2 2.7 3.9 1.4 versicolor ## 67 5.6 3.0 4.2 1.5 versicolor ## 74 6.1 2.8 4.7 1.2 versicolor ## 85 5.4 3.0 4.5 1.5 versicolor ## 92 6.1 3.1 4.6 1.4 versicolor ## 93 5.8 2.6 4.2 1.2 versicolor ## 102 6.8 2.7 5.1 1.9 virginica ## 109 6.7 2.5 5.8 0.6 virginica ## 137 6.3 3.4 5.6 1.8 virginica ## 138 6.4 3.1 1.5 1.8 virginica Mieux vauit éviter d’imputer des données compositionnelles transformées (alr, clr ou ilr), car l’imputation d’une dimension transformée aura un impact sur tout le vecteur. Dans ce cas, vous pourriez préférablemen utiliser la fonction robCompositions::impCoda. 9.2 Valeurs et échantillons aberrants: définition, origines, méthodes de détection et traitement 9.2.1 Définitions En analyse univariée, une valeur aberrante est une “donnée observée” pour une variable qui semble anormale au regard des valeurs dont on dispose pour les autres observations de l’échantillon (Planchon, 2005). En analyse multivariée, l’échantillon aberrant résulte d’une erreur importante se trouvant dans un des composants du vecteur de réponse, ou de petites erreurs systématiques dans chacun de ses composants, et qui de ce fait, ne partage pas les relations entre les variables de la population (Planchon, 2005). La valeur ou l’observation aberrante est statistiquement discordante dans le contexte d’un modèle de probabilité supposé connu (Barnett et Lewis, 1994; Grubbs, 1969; Munoz-Garcia et al., 1990; Pires et Santos-Pereira, 2005). Leur présence dans les données peut conduire à des estimateurs de paramètres biaisés et, suite à la réalisation de tests statistiques, à une interprétation des résultats erronée (Planchon, 2005). 9.2.2 Origines Dans une collecte de données, plusieurs sources de variabilité peuvent mener à des données aberrantes: la variabilité inhérente mais inusitée ou erreur systématique, l’erreur de mesure et l’erreur d’exécution (figure 9.2) (Barnett et Lewis, 1994; Planchon, 2005). Figure 9.2: Schéma général de traitement des valeurs aberrantes - adapté de Barnett et Lewis, 1994 La variabilité inhérente est celle par laquelle les observations varient naturellement de manière aléatoire à travers la population. L’erreur de mesure renferme les inadéquations au niveau de la méthode de mesure, des instruments de mesure, l’arrondi des valeurs obtenues ou les erreurs d’enregistrement. Cette erreur est donc liée à des circonstances bien déterminées. Les erreurs d’exécution interviennent également dans des circonstances bien déterminées. Ce sont les erreurs de manipulation, les erreurs commises dans l’assemblage des données, ou lors du traitement informatique. L’examen des valeurs aberrantes dans une base de données a pour objectif de les identifier pour soit les supprimer, soit les conserver, ou les corriger avant d’ajuster des modèles non robustes (Filzmoser et al., 2008; Planchon, 2005). La valeur extrême peut être liée à un événement atypique, mais néanmoins connu et intéressant à étudier. Dans ce cas elle est importante à conserver. La correction (ou accommodation) évite le rejet des observations aberrantes et consiste à estimer les valeurs des paramètres de la distribution de base de façon relativement libre sans déformation des résultats liés à leur présence (Barnett et Lewis, 1994). 9.2.3 Détection et traitement des échantillons aberrants multivariés L’approche d’identification des observations aberrantes selon Davies et Gather (1993) est de supposer qu’elles ont une distribution différente de celle du reste des observations. Reimann et al. (2005) les distinguent ainsi des valeurs extrêmes qui, bien qu’éloignées du centre du nuage, appartiennent à la même distribution que les autres observations. En analyse univariée, les méthodes graphiques telles que le diagramme de dispersion des observations classées en fonction de leur rang, les boxplots, les graphiques des quantiles de valeurs brutes ou des résidus, permettent de signaler la présence de valeurs aberrantes (Planchon, 2005). En analyse multivariée, il existe deux approches fondamentales d’identification des valeurs aberrantes: celles basées sur le calcul de distances et les méthodes par projection (Filzmoser et al., 2008; Hadi et al., 2009). 9.2.3.1 Approches basées sur les distances 9.2.3.1.1 La distance de Mahalanobis Les méthodes basées sur la distance détectent les valeurs aberrantes en calculant la distance, généralement la distance de Mahalanobis (vue au chapitre 8) entre un point particulier et le centre des données (Filzmoser et al., 2008; Pires et Santos-Pereira, 2005). Pour un échantillon \\(x\\) multivarié, la distance de Mahalanobis est calculée comme: \\[ \\mathscr{M} = \\sqrt{(\\vec{x}-\\vec{\\mu})^T S^{-1} (\\vec{x}-\\vec{\\mu})}.\\ \\] où \\(\\vec{\\mu}\\) est la moyenne arithmétique multivariée (le centroïde) et \\(S\\) la matrice de variance-covariances de l’échantillon, qui doit être inversée. Cette distance indique à quel point chaque observation est éloignée du centre du nuage multivarié créé par les données (Alameddine et al., 2010; Davies et Gather, 1993). D’après Alameddine et al. (2010), lorsque les données sont supposées suivre une distribution normale, les carrés des distances \\(\\mathscr{M}\\) calculées peuvent être considérés comme suivant une distribution du \\(\\chi^2\\). Par convention, tout point qui a une dépassant un quantile donné de la distribution du \\(\\chi^2\\) (par exemple, \\(\\chi^2_{df = p ; 0.975}\\), le quantile 97,5% avec \\(p\\) (le nombre de variables) degrés de liberté), est considéré comme atypique et identifié comme une valeur aberrante (Filzmoser et al., 2005). Les observations aberrantes multivariées peuvent ainsi être définies comme des observations ayant une grande distance de Mahalanobis (\\(\\mathscr{M}^2\\)). L’inconvénient avec les méthodes basées sur les distances réside dans la difficulté d’obtenir des estimés robuste de la moyenne \\(\\mu\\) et de la matrice de variance-covariances \\(S\\), puisque la distance de Mahalanobis est elle-même sensible aux données extrêmes. De plus, il serait difficile de fixer la valeur critique idéale de \\(\\mathscr{M}\\) permettant de séparer les valeurs aberrantes des points réguliers (Filzmoser et al., 2005; Filzmoser et al., 2008). La fonction sign1 du module mvoutlier détecte les valeurs aberrantes selon un seuil du \\(\\chi^2_{df = 3 ; 0.975}\\) pour les transformations en log-ratio isométriques de Al, Fe et K dans un humus (l’inverse de la matrice de covariance des les log-ratio centrés est singulière). library(&quot;mvoutlier&quot;) library(&quot;compositions&quot;) data(&quot;humus&quot;) sbp &lt;- matrix(c(1, 1,-1,-1, 1,-1, 0, 0, 0, 0, 1,-1), ncol = 4, byrow = TRUE) ilr_elements &lt;- humus %&gt;% select(Al, Fe, K, Na) %&gt;% ilr(., V = gsi.buildilrBase(t(sbp))) %&gt;% as_tibble(.) %&gt;% dplyr::rename(AlFe_KNa = V1, Al_Fe = V2, K_Na = V3) is_out &lt;- sign1(ilr_elements, qcrit = 0.975)$wfinal01 plot(ilr_elements, col = is_out + 2) La proportion de valeurs aberrantes: sum(is_out == 0) / length(is_out) ## [1] 0.089141 Différentes méthodes robustes (qui s’accommodent de la présence de points extrêmes) de détection des valeurs aberrantes sont présentées dans la littérature telles que la méthode du volume minimum de l’ellipsoïde (MVE, minimum volume ellipsoid), du déterminant minimum de la matrice de covariance (MCD, minimum Covariance matrix determinant), et les estimateurs de type maximum de vraisemblance (M-estimators) (Alameddine et al., 2010; Filzmoser et al., 2008). Ces méthodes calculent des distances robustes similaires aux distances de Mahalanobis, mais remplacent les matrices des moyennes et des covariances respectivement par un seuil critique multivarié robuste (sur \\(\\mu\\)) et un estimateur d’échelle (sur \\(S\\)) qui ne sont pas influencés par les valeurs aberrantes (Alameddine et al., 2010). 9.2.3.1.2 La méthode du volume minimum de l’ellipsoïde (MVE) Le volume minimum de l’ellipsoïde est le plus petit ellipsoïde régulier couvrant au moins \\(h\\) éléments de l’ensemble des données \\(X = \\{x_1, x_2, ..., x_n \\}\\) où l’estimateur de localisation est le centre de cet ellipsoïde et l’estimateur de dispersion correspond à sa matrice de covariance. \\(h\\) est fixé à priori supérieur ou égal à \\(\\frac{n}{2}+1\\), où \\(n\\) est le nombre total de points du nuage de données. Le seuil de détection qui est la fraction des valeurs aberrantes qui, lorsqu’elle est dépassée entraîne des estimés totalement biaisés est de l’ordre de 50% à mesure que \\(n\\) augmente (Alameddine et al., 2010; Croux et al., 2002; Filzmoser et al., 2005; Van Aelst et Rousseeuw, 2009). L’algorithme MVE est initié en choisissant au hasard un ensemble de \\(p+1\\) points de données pour estimer le modèle majoritaire, où \\(p\\) est le nombre de variables. Cet ensemble initial est alors augmenté pour contenir les \\(h\\) points de données. L’algorithme passe par plusieurs itérations avant de converger sur l’ensemble des points les plus rapprochés qui auront le plus petit volume d’ellipsoïde (Alameddine et al., 2010). Le module MASS comprend la fonction cov.mve à cet effet. Cette fonction demande le nombre minimal de points que l’on désire conserver, en absolu. Il s’agit d’un nombre entier, alors si l’on désire en utiliser une fraction (ici, 90%), il faut l’arrondir. Parmi les sorties de la fonction cov.mve, on retrouve les numéros de ligne qui se trouvent à l’intérieur de l’ellipsoide. library(&quot;MASS&quot;) select &lt;- dplyr::select # pour éviter que la fonction select du module MASS remplace celle de dplyr min_in &lt;- round(0.9 * nrow(ilr_elements)) # le minimum de points à garder, 90% du total id_in &lt;- cov.mve(ilr_elements, quantile.used = min_in)$best is_in &lt;- 1:nrow(ilr_elements) %in% id_in plot(ilr_elements, col = is_in + 2) La proportion de valeurs aberrantes: sum(!is_in) / length(is_in) ## [1] 0.1004862 9.2.3.1.3 La méthode du déterminant minimum de la matrice de covariance (MCD) La méthode du déterminant minimum de la matrice de covariance a pour objectif de trouver \\(h\\) (\\(h &gt; n\\)) observations de l’ensemble de données \\(X = \\{x_1, x_2, ..., x_n \\}\\), dont la matrice de covariance a le plus petit déterminant. Comme avec la méthode MVE, l’estimateur de localisation est la moyenne de ces \\(h\\) points et celui de la dispersion est proportionnel à la matrice de covariance (Filzmoser et al., 2005; Hubert et al., 2018; Rousseeuw et Van Driessen, 1999). id_in &lt;- cov.mcd(ilr_elements, quantile.used = min_in)$best is_in &lt;- 1:nrow(ilr_elements) %in% id_in plot(ilr_elements, col = is_in + 2) La proportion de valeurs aberrantes: sum(!is_in) / length(is_in) ## [1] 0.1004862 Mais en cas de dissymétrie des données, ces tests (MVE, MCD) ne seraient pas applicables (Planchon, 2005). 9.2.3.2 Les méthodes par projection Ces méthodes de détection des observations aberrantes trouvent des projections appropriées des données dans lesquelles les observations aberrantes sont facilement apparentes. Ces observations sont ensuite pondérés pour produire un estimateur robuste pouvant être utilisé pour identifier les observations aberrantes (Filzmoser et al., 2008). Ces méthodes n’assument pas une distribution particulière des données mais cherchent des projections utiles. Elles ne sont donc pas affectées par la non-normalité et s’appliquent sur divers types de distributions (Filzmoser et al., 2008; Hadi et al., 2009). Le but de cette projection exploratoire est d’utiliser les données pour trouver des projections minimales (à une, deux ou trois dimensions) qui fournissent les vues les plus révélatrices des données complètes (Friedman, 1987). La méthode attribue un indice numérique à chaque projection en fonction de la densité des données projetée pour capturer le degré de structure non linéaire présent dans la distribution projetée (Friedman, 1987; Hadi et al., 2009). En R, nous revenons au module mvoutlier, mais cette fois-ci avec la fonction sign2. is_out &lt;- sign2(ilr_elements, qcrit = 0.975)$wfinal01 plot(ilr_elements, col = is_out + 2) La proportion de valeurs aberrantes: sum(is_out == 0) / length(is_out) ## [1] 0.102107 "],
["chapitre-temps.html", "10 Les séries temporelles 10.1 Opérations sur les données temporelles 10.2 Analyse de séries temporelles 10.3 Modélisation de séries temporelles 10.4 Pour terminer…", " 10 Les séries temporelles ️ Objectifs spécifiques: À la fin de ce chapitre, vous saurez comment importer et manipuler des données temporelles (utiliser le format de date, filtrer, effectuer des sommaires, agréger des données, etc.) effectuer une régression sur une série temporelle Les séries temporelles (ou chronologiques) sont des données associées à des indices temporels de tout ordre de grandeur: seconde, minute, heure, jour, mois, année, etc. En analyse de série temporelle, le temps est une variable explicative (ou dépendante) incontournable. L’émergence de cycles est une particularité des séries temporelles. Ceux-ci peuvent être analysés en vue d’en déterminer la tendance. Les séries temporelles peuvent également être modélisés en vue d’effectuer des prévisions. Source: Scène de Back to the future, Robert Zemeckis et and Bob Gale, 1985 Nous allons couvrir les concepts de base en analyse et modélisation de séries temporelles. Mais avant cela, voyons comment les données temporelles sont manipulées en R. Cette section est basée sur le livre Forecasting: Principles and Practice, de Rob J. Hyndman et George Athanasopoulos, qui peut être entièrement consulté gratuitement en ligne, ainsi que le cours associé sur la plateforme d’apprentissage DataCamp. Figure 10.1: Forecasting: Principles and Practice, de Rob J. Hyndman et George Athanasopoulos. 10.1 Opérations sur les données temporelles Le débit de la rivière Chaudière, dont l’exutoire se situe près de Québec, sur la rive Sud du fleuve Saint-Laurent, est mesuré depuis 1915. ## Parsed with column specification: ## cols( ## Station = col_double(), ## Date = col_date(format = &quot;&quot;), ## Débit = col_double(), ## Remarque = col_character() ## ) La fonction read_csv() détecte automatiquement que la colonne Date est une date. glimpse(hydro) ## Observations: 34,700 ## Variables: 4 ## $ Station &lt;dbl&gt; 23402, 23402, 23402, 23402, 23402, 23402, 23402, 23402,… ## $ Date &lt;date&gt; 1915-02-27, 1915-02-28, 1915-03-01, 1915-03-02, 1915-0… ## $ Débit &lt;dbl&gt; 538, 377, 269, 345, 269, 334, 269, 269, 269, 269, 269, … ## $ Remarque &lt;chr&gt; &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;… Le débit de la rivière Chaudière peut être exploré graphiquement. hydro %&gt;% ggplot(aes(x = Date, y = `Débit`)) + geom_line() On observe des données sont manquantes de la fin des années 1920 à la fin des années 1930. Autrement, il est difficile de visualiser la structure du débit en fonction du temps, notamment si le débit suit des cycles réguliers. On pourra isoler les données depuis 2014. hydro %&gt;% filter(Date &gt;= as.Date(&quot;2014-01-01&quot;)) %&gt;% ggplot(aes(x = Date, y = `Débit`)) + geom_line() R comprend la fonction as.Date(), où l’argument format décrit la manière avec laquelle la date est exprimée. as.Date(x = &quot;1999/03/29&quot;, format = &quot;%Y/%m/%d&quot;) ## [1] &quot;1999-03-29&quot; L’argument x peut aussi bien être une chaîne de caractères qu’un vecteur où l’on retrouve plusieurs chaînes de caractères exprimant un format de date commun. La fonction as.Date() permet ainsi de transformer des caractères en date si read_csv() ne le détecte pas automatiquement. Ce format peut prendre la forme désirée, dont les paramètres sont listés sur la page d’aide de la fonction strptime(). Toutefois, le plus petit incrément de temps accepté par as.Date() est le jour: as.Date() exclut les heures, minutes et secondes. Le module lubridate, issu du tidyverse, permet quant à lui de manipuler avec plus de grâce les formats de date standards, incluant les dates et les heures: lubridate sera préféré dans ce chapitre. ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:data.table&#39;: ## ## hour, isoweek, mday, minute, month, quarter, second, wday, ## week, yday, year ## The following object is masked from &#39;package:plyr&#39;: ## ## here ## The following object is masked from &#39;package:base&#39;: ## ## date ## [1] &quot;2011-02-19 09:14:00 UTC&quot; Plusieurs autres formats standards sont présentés sur un aide-ménoire de lubridate. Si vos données comprennent des formats de date non standard, vous pourrez utiliser la fonction as.POSIXlt(), mais il pourrait être préférable de standardiser les dates a priori. Figure 10.2: Aide-mémoire du module lubridate. Le module lubridate rend possible l’extraction de la date (date()), l’année (year()), le mois (month()), le jour de la semaine (wday()), le jour julien (yday()), etc. pour plus d’options, voir [l’aide-mémoire de lubridate])(https://rawgit.com/rstudio/cheatsheets/master/lubridate.pdf). date_1 &lt;- ymd_hms(&quot;2019-03-14 09:14:00&quot;) date_1 %&gt;% date() ## [1] &quot;2019-03-14&quot; date_1 %&gt;% month() ## [1] 3 date_1 %&gt;% yday() ## [1] 73 date_1 %&gt;% wday() # ## [1] 5 date_1 %&gt;% seconds() ## [1] &quot;1552554840S&quot; Ces extractions peuvent être utilisées dans des suites d’opération (pipelines). Par exemple, si nous désirons obtenir le débit mensuel moyen de la rivière Chaudière depuis 1990, nous pouvons créer une nouvelle colonne Year et une autre Month avec la fonction mutate(), effectuer un filtre sur l’année, regrouper par mois pour obtenir le sommaire en terme de moyenne, puis lancer le graphique. hydro_month &lt;- hydro %&gt;% mutate(Year = Date %&gt;% year(), Month = Date %&gt;% month()) %&gt;% filter(Year &gt;= 1990) %&gt;% group_by(Month) %&gt;% dplyr::summarise(MeanFlow = mean(`Débit`, na.rm = TRUE)) hydro_month %&gt;% ggplot(aes(x=Month, y=MeanFlow)) + geom_line() + scale_x_continuous(breaks = 1:12) + expand_limits(y = 0) On pourra aussi agréger par moyenne mensuelle en gardant l’année respective en créant une nouvelle colonne de date YearMonth qui permettra le regroupement avec group_by(), puis créer plusieurs facettes. hydro %&gt;% mutate(Year = Date %&gt;% year(), Month = Date %&gt;% month(), YearMonth = ymd(paste0(Year, &quot;-&quot;, Month, &quot;-01&quot;))) %&gt;% filter(Year &gt;= 2010 &amp; Year &lt; 2018) %&gt;% group_by(Year, YearMonth) %&gt;% dplyr::summarise(`Débit` = mean(`Débit`, na.rm = TRUE)) %&gt;% ggplot(aes(x=YearMonth, y=`Débit`)) + facet_wrap(~Year, scales = &quot;free_x&quot;, ncol = 4) + geom_line() + expand_limits(y = 0) Il est possible d’effectuer des opérations mathématiques sur des données temporelles. Par exemple, ajouter 10 jours à chaque date. hydro %&gt;% head(5) %&gt;% mutate(DateOffset = Date + days(10)) ## # A tibble: 5 x 5 ## Station Date Débit Remarque DateOffset ## &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; &lt;date&gt; ## 1 23402 1915-02-27 538 MC 1915-03-09 ## 2 23402 1915-02-28 377 MC 1915-03-10 ## 3 23402 1915-03-01 269 MC 1915-03-11 ## 4 23402 1915-03-02 345 MC 1915-03-12 ## 5 23402 1915-03-03 269 MC 1915-03-13 Pour effectuer des opérations sur des incréments inférieurs aux jours, il faut s’assurer que le type des données temporelles soit bien POSIXct, et non pas Date. hydro %&gt;% pull(Date) %&gt;% class() ## [1] &quot;Date&quot; hydro &lt;- hydro %&gt;% mutate(Date = as_datetime(Date)) hydro %&gt;% pull(Date) %&gt;% class() ## [1] &quot;POSIXct&quot; &quot;POSIXt&quot; hydro %&gt;% head(5) %&gt;% mutate(DateOffset = Date + seconds(10)) ## # A tibble: 5 x 5 ## Station Date Débit Remarque DateOffset ## &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dttm&gt; ## 1 23402 1915-02-27 00:00:00 538 MC 1915-02-27 00:00:10 ## 2 23402 1915-02-28 00:00:00 377 MC 1915-02-28 00:00:10 ## 3 23402 1915-03-01 00:00:00 269 MC 1915-03-01 00:00:10 ## 4 23402 1915-03-02 00:00:00 345 MC 1915-03-02 00:00:10 ## 5 23402 1915-03-03 00:00:00 269 MC 1915-03-03 00:00:10 10.2 Analyse de séries temporelles Tout comme c’est le cas de nombreux sujet couverts lors de ce cours, l’analyse et modélisation de séries temporelles est un domaine d’étude en soi. Nous allons nous restreindre ici aux séries temporelles consignées à fréquence régulière. Les exemples d’analyses et modélisation de séries temporelles sont typiquement des données économiques, bien que les principes qui les guident sont les mêmes qu’en d’autres domaines. Cette section est vouée à l’analyse, alors que la prochaine est vouée à la modélisation. Par exemple, voici une série temporelle économique typique, qui exprime les dépenses mensuelles en restauration en Australie. library(&quot;forecast&quot;) ## ## Attaching package: &#39;forecast&#39; ## The following object is masked from &#39;package:nlme&#39;: ## ## getResponse library(&quot;fpp2&quot;) ## Loading required package: fma ## ## Attaching package: &#39;fma&#39; ## The following object is masked from &#39;package:plyr&#39;: ## ## ozone ## The following objects are masked from &#39;package:MASS&#39;: ## ## cement, housing, petrol ## The following object is masked from &#39;package:robustbase&#39;: ## ## milk ## Loading required package: expsmooth ## ## Attaching package: &#39;fpp2&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## gasoline ## The following object is masked from &#39;package:pls&#39;: ## ## gasoline data(&quot;auscafe&quot;) autoplot(auscafe) On y détecte une tendance générale, probablement propulsée par la croissance de la démographie et des revenus, ainsi que des tendances cycliques. On verra plus loin comment prédire des occurrences futures, ainsi que l’incertitude de ces prédictions, à partir des données consignées. Jusqu’à présent, nous avons travaillé avec des tableaux de données incluant une colonne en format date. Nous allons maintenant travailler avec des séries temporelles telles que représentées en R. 10.2.1 Créer et visualiser des séries temporelles L’information consignée dans une série temporelle inclut nécessairement un indice temporel associé à au moins une variable. En R, cette information est consignée dans un objet de type ts, pour time series. Prenons une mesure quelconque prise à chaque trimestre de l’année 2018. set.seed(96683) date &lt;- ymd(c(&quot;2018-01-01&quot;, &quot;2018-04-01&quot;, &quot;2018-07-01&quot;, &quot;2018-10-01&quot;)) mesure &lt;- runif(length(date), 1, 10) mesure_ts &lt;- ts(mesure, start = date[1], frequency = 4) mesure_ts ## Qtr1 Qtr2 Qtr3 Qtr4 ## 17532 7.175836 3.646285 6.631606 8.648371 L’argument start est la date de la première observation et frequency est le nombre d’observations par unité temporelle, ici l’année. J’ai auparavant recueilli des données météo avec weathercan (dispobibles seulement depuis 1998) et fusionné avec le tableau hydro. Pour accélérer la procédure, j’ai enregistré les données dans un fichier RData. De facto, ne gardons que les données disponibles entre 1998 et 2008, ainsi que les colonnes désignant la date, le débit, les précipitations totales et la température. ## Observations: 3,653 ## Variables: 4 ## $ Date &lt;date&gt; 1998-01-01, 1998-01-02, 1998-01-03, 1998-01-04, 19… ## $ Débit &lt;dbl&gt; 15.70, 16.00, 17.40, 19.30, 23.20, 29.00, 58.85, 65… ## $ total_precip &lt;dbl&gt; 1.6, 2.8, 2.2, 0.0, 5.8, 11.8, 2.4, 19.2, 11.6, 2.6… ## $ mean_temp &lt;dbl&gt; -21.1, -8.9, 1.9, -3.2, -8.7, -8.0, -7.4, -6.3, -5.… Pour créer une série temporelle de type ts, j’enlève la date, je démarre au premier événement de 1998, et chaque incrément a une fréquence de 1/365.25 unités depuis 1998 (il y a en moyenne 365.25 jours par an). hydrometeo_ts &lt;- ts(hydrometeo %&gt;% select(-Date), start = c(hydrometeo$Date[1] %&gt;% year(), 1), frequency = 365.25) Le module ggplot2 comprend la fonction autoplot(), pratique pour visualiser les séries temporelles. autoplot(hydrometeo_ts, facets = TRUE) + scale_x_continuous(breaks = 1998:2008) Il est possible de filtrer des séries temporelles en mode tidyverse. Toutefois, il est plus simple d’utiliser la fonction de base windows(). Disons, les 10 premiers jours de l’an 2000. ## Time Series: ## Start = 2000.00136892539 ## End = 2000.02600958248 ## Frequency = 365.25 ## Débit total_precip mean_temp ## 2000.001 42.40 9.4 -5.6 ## 2000.004 40.70 0.0 -5.5 ## 2000.007 43.60 23.5 -0.9 ## 2000.010 49.04 0.0 -8.8 ## 2000.012 58.90 0.0 -12.9 ## 2000.015 49.10 1.2 -4.6 ## 2000.018 44.40 3.8 -10.5 ## 2000.021 40.60 6.8 -4.9 ## 2000.023 38.10 7.0 -2.3 ## 2000.026 36.50 12.9 -0.3 Voyons l’évolution des débits mensuelles. hydrometeo_monthly &lt;- hydrometeo %&gt;% mutate(Year = Date %&gt;% year(), Month = Date %&gt;% month(), YearMonth = ymd(paste0(Year, &quot;-&quot;, Month, &quot;-01&quot;))) %&gt;% group_by(Year, YearMonth) %&gt;% dplyr::summarise(`Débit` = mean(`Débit`, na.rm = TRUE), total_precip = sum(total_precip, na.rm = TRUE), # somme mean_temp = mean(mean_temp, na.rm = TRUE)) # moyenne hydrometeo_monthly_ts &lt;- ts(hydrometeo_monthly %&gt;% ungroup() %&gt;% select(`Débit`, total_precip, mean_temp), start = c(1998, 1), frequency = 12) Contraignons la période grâce à window(), puis visualisons les tendances cycliques avec forecast::ggseasonplot() et forecast::ggsubseriesplot(). Notez que j’utilise la fonction cowplot::plot_grid() pour arranger différents graphiques ggplot2 en une grille. library(&quot;cowplot&quot;) ## ## Attaching package: &#39;cowplot&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## ggsave theme_set(theme_grey()) # cowplot change le theme ggA &lt;- ggseasonplot(window(hydrometeo_monthly_ts[, 1], 1998, 2004-1/365.25)) + ggtitle(&quot;&quot;) ggB &lt;- ggseasonplot(window(hydrometeo_monthly_ts[, 1], 1998, 2004-1/365.25), polar = TRUE) + ggtitle(&quot;&quot;) ggC &lt;- ggsubseriesplot(window(hydrometeo_monthly_ts[, 1], 1998, 2004-1/365.25), polar = TRUE) + ggtitle(&quot;&quot;) + labs(y=&quot;Flow&quot;) plot_grid(ggA, ggB, ggC, ncol = 3, labels = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;)) 10.2.2 Structures dans les séries temporelles Les séries temporelles sont susceptibles d’être caractérisées par des structures communément observées. La tendance est une structure décrivant la hausse ou la baisse à long terme d’une variable numérique. La fluctuation saisonnière est une structure périodique, qui oscille autour de la tendance générale de manière régulière selon le calendrier. La fluctuation cyclique est aussi une structure périodique, mais irrégulière (par exemple, les oscillations peuvent durer parfois 2 ans, parfois 3). Les fluctuations cycliques sont souvent de plus longue fréquence que les fluctuations saisonnières, et leur irrégularité rend les prédictions plus difficiles. Note. Une tendance détectée sur une période de temps trop courte peut s’avérer être une fluctuation. La figure 10.3 montre différentes structures. La figure 10.3A montre une tendance croissante des dépenses mensuelles en restauration en Australie, ainsi que des fluctuations saisonnières. La figure 10.3B montre des fluctuations saisonnières des températures quotidiennes moyennes à l’Université Laval, sans présenter de tendance claire. La figure 10.3C montre des fluctuations cycliques du nombre de lynx trappés par année au Canada de 1821 à 1934, sans non plus présenter de tendance claire. Les cycles sont conséquents des mécanismes de dynamique des populations (plus de proie entraîne plus de prédateur, plus de prédateur entraîne moins de proie, moins de proie entraîne moins de prédateur, moins de prédateur entraîne plus de proie, etc.), que nous couvrirons au chapitre 14. data(&quot;lynx&quot;) plot_grid(autoplot(auscafe), autoplot(hydrometeo_ts[, 3]) + labs(y=&quot;Mean temperature&quot;), autoplot(lynx), ncol = 3, labels = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;)) Figure 10.3: Identification des tendances et fluctuations dans des séries temporelles Il est possible que l’on retrouve une hiérarchie dans les fluctuations, c’est-à-dire que de grandes fluctuations (saisonnières ou cycliques) peuvent contenir des fluctuations sur des incréments de temps plus petits. 10.2.3 L’autocorrélation Lorsque les données présentes des fluctuations (saisonnières ou cycliques), le graphique d’autocorrélation montrera un sommet aux étapes des cycles ou des saisons. Le graphique d’autocorrélation de données aléatoires (aussi appelées bruit blanc) montera des sommets sans signification. Un graphique de retardement (lag plot) met successivement en relation \\(y_t\\) avec \\(y_{t-p}\\). Un graphique d’autocorrélation est la corrélation entre \\(y_t\\), \\(y_{t-1}\\), \\(y_{t-2}\\), etc. Une graphique de retardement donne un aperçu de la dépendance d’une variable selon ses valeurs passées. Les graphiques de retardement de données ayant une forte tendance présenteront des points près de la diagonale, tandis que ceux montrant des données fluctuantes de type sinusoïdal présenteront des points disposés de manière circulaire. Des données aléatoires, quant à elles, ne présenteront pas de structure de retardement facilement identifiable. set.seed(64301) bruit_blanc &lt;- ts(runif(114, 0, 6000), start = c(1821, 1), frequency = 1) plot_grid(autoplot(lynx) + ggtitle(&quot;Lynx: Série temporelle&quot;), ggAcf(lynx) + ggtitle(&quot;Lynx: Autocorrélation&quot;), gglagplot(lynx) + ggtitle(&quot;Lynx: Lag plot&quot;), autoplot(bruit_blanc) + ggtitle(&quot;Bruit blanc: Série temporelle&quot;), ggAcf(bruit_blanc) + ggtitle(&quot;Bruit blanc: Autocorrélation&quot;), gglagplot(bruit_blanc) + ggtitle(&quot;Bruit blanc: Lag plot&quot;), ncol = 3) Exercice. Créez, puis interprétez des graphiques autoplot(), ggAcf() et gglagplot() pour les données auscafe. Exercice. Trouvez le graphique d’autocorrélation et le graphique de retardement correspondant à chaque série temporelle. Figure 10.4: Exercice: Trouvez le graphique d’autocorrélation et le graphique de retardement correspondant à chaque série temporelle. Réponse, voir source(&quot;lib/09_exercice-hydrometeo.R&quot;): - Débit: A-B-C - total_precip: B-A-A - mean_temp: C-C-B 10.2.4 Signification statistique d’une série temporelle J’ai précédemment introduit la notion de bruit blanc, qui est un signal ne contenant pas de structure, comme le grésillement d’une radio mal syntonisée. Nous avons vu au chapitre 5 que les tests d’hypothèse en statistiques fréquentielles visent entre autre à détecter la probabilité que les données soient générées par une distribution dont la tendance centrale est nulle. De même, pour les séries temporelles, il est possible de calculer la probabilité qu’un signal soit un bruit blanc. Deux outils peuvent nous aider à effectuer ce test: l’un visuel, l’autre sous forme de calcul. Le graphique d’autocorrélation est à même d’inclure des seuils pour lesquels la corrélation est significative (lignes pointillées bleues). ggAcf(lynx, ci = 0.95) + ggtitle(&quot;Lynx: Autocorrélation&quot;) L’analyse des seuils de signification de l’autocorrélation indique sur la possibilité de conduire la série temporelle vers un processus de modélisation prédictive. Dans l’exemple ci-dessus, on remarque qu’il existe des corrélations significatives pour un décalage de 4 à 6 données, mais que les données situées près les unes des autres pourraient être plus difficiles à modéliser. Le test de Ljung-Box permet quant à lui de tester si la série temporelle entière peut être différenciée d’un bruit blanc. Box.test(lynx, lag = 20, type = &quot;Ljung-Box&quot;) ## ## Box-Ljung test ## ## data: lynx ## X-squared = 365.54, df = 20, p-value &lt; 2.2e-16 La probabilité que la série soit un bruit blanc est presque nulle. Notons que les tests statistiques sont aussi valides sur les dérivées des séries temporelles. En outre, une dérivée première de la série temporelle sur les dépenses devient une série temporelle de la variation des dépenses en restauration. plot_grid(autoplot(diff(auscafe)) + ggtitle(&quot;Restauration: Série temporelle&quot;), ggAcf(diff(auscafe)) + ggtitle(&quot;Restauration: Autocorrélation&quot;), gglagplot(diff(auscafe)) + ggtitle(&quot;Restauration: Lag plot&quot;), ncol = 3) Box.test(diff(auscafe), lag = 16, type = &quot;Ljung-Box&quot;) ## ## Box-Ljung test ## ## data: diff(auscafe) ## X-squared = 647.11, df = 16, p-value &lt; 2.2e-16 Jusqu’à présent, nous nous sommes contentés d’observer des séries temporelles. Lançons-nous maintenant dans un domaine plus excitant. Source: Scène de Back to the future, Robert Zemeckis et and Bob Gale, 1985 10.3 Modélisation de séries temporelles L’objectif général de la modélisation de série temporelle est la prévision (forecast). La majorité des modèles se base sur des simulations de futurs possibles, desquels on pourra déduire une tendance centrale (point forecast) ainsi que des intervalles prévisionnels. Il est important d’insister sur le fait que la tendance centrale ne signifie pas que les données futures suivront cette tendance, mais que, selon les données et le modèle, la moitié des données devrait se retrouver sous la ligne, et l’autre moitié au-dessus. De plus, la région de confiance définie par les intervalles prévisionnels signifient que par exemple 95% des points devraient se situer dans cette région. Une manière d’évaluer la performance d’une prévision est de prévoir des données auparavant observées à partir des données qui les précèdent. Ces valeurs sont dites lissées. Tout comme c’est le cas en régression statistique, il est possible de déduire les résidus du modèle. Pour les régressions couvertes au chapitre 5, nous vérifions la validité du modèle en vérifiant si les résidus étaient distribuées normalement. Pour une série temporelle, on tend plutôt à vérifier si les résidus forment un bruit blanc, c’est-à-dire qu’ils ne sont pas corrélés. De plus, pour éviter d’être biaisées, leur moyenne doit être de 0. De manière complémentaire pour la validité des intervalles prévisionnels, mais non essentielle à la validité du modèle, les résidus devraient être distribués normalement et leur variance devrait être constante (Hyndman et Athanasopoulos, 2018). Il est possible qu’un modèle remplisse toutes ces conditions, mais que sa prévision soit médiocre. Comme nous le verrons également au chapitre 12, une prédiction ou une prévision issue d’un modèle ne peut pas être évaluée sur des données qui ont servies à lisser le modèle. Pour vérifier une prévision temporelle, il faut séparer les données en deux séries: une série d’entraînement et une série de test (figure 10.5). Figure 10.5: Les points bleus désigne la série d’entraînement et les points rouges, la série de test. Source de l’image: Hyndman et Athanasopoulos, 1998. La séparation dans le temps entre la série d’entraînement et la série de test se fait à votre convenance, selon la disponibilité des données. Vous aurez toutefois avantage à conserver davantage de données en entraînement (typiquement, 70%), et à tout le moins, séparer au moins une fluctuation saisonnière ou cyclique. La série d’entraînement servira à lisser le modèle pour en découvrir les possibles structures. La série de test servira à évaluer sa performance sur des données obtenues, mais inconnues du modèle pour vérifier les structures découvertes par le modèle. L’erreur prévisionnelle est la différence entre une donnée observée en test et sa prévision (l’équivalent des résidus, mais appliqués sur des données indépendantes du modèle). La performance d’une prévision peut être évaluée de différentes manières, mais l’erreur moyenne absolue échelonnée (mean absolute scaled error, MASE) est conseillée puisqu’elle ne dépend pas de la métrique de la quantité produite: plus la MASE se rapproche de zéro, meilleure est la prévision. Plusieurs méthodes de prévision sont possibles. Nous en couvrirons 3 dans ce chapitre: la méthode naïve, la méthode SES et la méthode ARIMA. Nous allons couvrir les différents aspects de la modélisation des séries temporelles à travers l’utilisation de ces méthodes. 10.3.1 Méthode naïve La méthode naïve définit la valeur suivante selon la valeur précédente (fonction forecast::naive()), ou la valeur de la saison précédente (fonction forecast::snaive()). Ces fonctions du module forecast incluent un composante aléatoire pour simuler des occurrences futures selon des marches aléatoires (random walks), où chaque valeur suivante est simulée aléatoirement, considérant la valeur précédente. Nous tenterons de prévoir les débits de la rivière Chaudière. Ceux-ci étant caractérisé par des fluctuations saisonnières, mieux vaut utiliser snaive(). Mais auparavant, séparons la série en série d’entraînement et série de test. flow_ts &lt;- hydrometeo_monthly_ts[, 1] flow_ts_train &lt;- window(flow_ts, start = 1998, end = 2005.999) flow_ts_test &lt;- window(flow_ts, start = 2006) Lançons la modélisation sur les données d’entraînement. hm_naive &lt;- snaive(flow_ts_train, h = 24) autoplot(hm_naive) + autolayer(fitted(hm_naive)) + autolayer(flow_ts_test, color = rgb(0, 0, 0, 0.6)) + labs(x = &quot;Année&quot;, y = &quot;Débit&quot;) ## Warning: Removed 12 rows containing missing values (geom_path). Le graphique précédent montre que la prévision naïve (en rose) prend bien la valeur observée au cycle précédent (en noir). Les données de test sont en gris transparent. Notons que la présence de débit négatifs pourrait être évitée en utilisant une transformation logarithmique du débit préalablement à la modélisation. Voyons maintenant l’analyse des résidus avec la fonction forecast::checkresiduals(). checkresiduals(hm_naive) ## ## Ljung-Box test ## ## data: Residuals from Seasonal naive method ## Q* = 34.903, df = 19.2, p-value = 0.01546 ## ## Model df: 0. Total lags used: 19.2 La p-value étant de 0.01546, il est peu probable que les résidus forment un bruit blanc. Les résidus contiennent de l’autocorrélation, ce qui devrait être évité. Ceci est toutefois dû à un seul point allant au-delà du seuil de 0.05, que l’on peut observer sur le graphique d’autocorrélation. Le graphique de la distribution des résidus montre des valeurs aberrantes, ainsi qu’une distribution plutôt pointue, qui donnerait un test de Kurtosis probablement élevé. shapiro.test(residuals(hm_naive)) # non-normal si p-value &lt; seuil (0.05) ## ## Shapiro-Wilk normality test ## ## data: residuals(hm_naive) ## W = 0.93698, p-value = 0.0004733 library(&quot;e1071&quot;) kurtosis(residuals(hm_naive), na.rm = TRUE) # le résultat d&#39;un test de kurtosis sur une distribution normale devrait être de 0. ## [1] 2.909277 Pas de panique, les prédictions peuvent néanmoins être valides: seulement, les intervalles prévisionnels pourraient être trop vagues ou trop restreintes: à prendre avec des pincettes. L’évaluation du modèle peut être effectuée avec la fonction forecast::accuracy(), qui détecte automatiquement la série d’entraînement et la série de test si on lui fournit la série entière (ici l’objet flow_ts). accuracy(hm_naive, flow_ts) ## ME RMSE MAE MPE MAPE MASE ## Training set 0.5952819 80.24256 54.53431 -57.045415 95.60936 1.000000 ## Test set -1.0165543 75.18877 57.10499 -2.645333 59.10844 1.047139 ## ACF1 Theil&#39;s U ## Training set 0.1669526053 NA ## Test set -0.0006850245 0.3877041 La méthode naïve est rarement utilisée en pratique autrement que comme standard par rapport auquel la performance d’autres modèles est évaluée. 10.3.2 Méthode SES Alors que la méthode naïve donne une crédibilité complète à la valeur précédente (ou au cycle précédent), la méthode SES (simple exponential smoothing) donne aux valeurs précédentes des poids exponentiellement décroissants selon leur ancienneté. La prévision par SES sera une moyenne pondérée des dernières observations, en donnant plus de poids sur les observations plus rapprochées. Mathématiquement, la méthode SES est décrite ainsi. \\[\\hat{y}_{t + h|t} = \\alpha y_t + \\alpha\\left( 1-\\alpha \\right) y_{t-1} + \\alpha\\left( 1-\\alpha \\right)^2 y_{t-2} + ...\\] où \\(\\hat{y}_{t + h|t}\\) est la prévision de \\(y\\) au temps \\(t + h|t\\), qui est le décalage de \\(h\\) à partir de la dernière mesure au temps \\(t\\). Le paramètre \\(\\alpha\\) prend une valeur de 0 à 1, et décrit la distribution des poids. Une valeur de \\(\\alpha\\) élevé donnera davantage de poids aux événements récents. La somme de tous poids \\(\\alpha\\) tend vers 1 lorsque les pas de temps précédents tendent vers l’\\(\\infty\\). Une autre manière d’exprimer l’équation est de la segmenter en deux: une pour la prévision en fonction du niveau (level, le modèle), une autre pour décrire comment le niveau change au fil du temps. Description Équation Prévision \\(\\hat{y}_{t + h|t} = l_t\\) Niveau \\(l_t = \\alpha y_t + \\alpha\\left( 1-\\alpha \\right) l_{t-1}\\) Exprimée ainsi, la prévision n’exprimera aucune tendance ni fluctuation. Il s’agira d’une projection jusqu’à l’infini de la moyenne des observations précédentes pondérée par leur décalage. 10.3.2.1 SES de base Prenons les données de la NASA sur l’indice de température terre-océan, qui décrit un décalage par rapport à la moyenne des températures globales observées entre de 1951 à 1980. La méthode SES est appelée par la fonction forecast::ses(), de la même manière qu’on l’a fait précédemment avec la méthode naïve. loti_ts &lt;- read_csv(&quot;data/09_nasa.csv&quot;) %&gt;% pull(LOTI) %&gt;% ts(., start = 1880, frequency = 1) #loti_ts &lt;- window(loti_ts, start = 1950) loti_ts_tr &lt;- window(loti_ts, end = 2004) loti_ses &lt;- ses(loti_ts_tr, h = 20, alpha = 0.5) autoplot(loti_ses) + autolayer(fitted(loti_ses)) Note. Les prévisions climatiques sont effectuées par des modèles bien plus complexes que ce que nous voyons ici. Les prévisions du GIEC agrègent des tendances localisées et incluent une batterie de covariables, dont la plus évidente est la concentration en CO2 dans l’atmosphère. Il s’agit seulement d’un exemple d’application. 10.3.2.2 SES avec tendance La prévision a peu d’intérêt, étant donnée qu’elle n’inclut pas de tendance. Or, nous pouvons en ajouter une à l’équation. Ainsi exprimée, la tendance changera aussi au fil du temps. Description Équation Prévision \\(\\hat{y}_{t + h|t} = l_t + \\left( \\phi + \\phi^2 + ... + \\phi^h \\right) \\times b_t\\) Niveau $l_t = y_t + ( 1-) ( l_{t-1} + b_{t-1} ) $ Tendance \\(b_t = \\beta^* \\left( l_t - l_{t-1} \\right) + (1-\\beta^*) \\phi b_{t-1}\\) Le paramètre \\(\\beta^*\\) décrit la vitesse à laquelle la tendance peut changer, de 0 où la pente ne change pas à 1 où la pente change rapidement. Le paramètre \\(\\phi\\) adouci la pente en s’éloignant de la dernière mesure. Un tendant vers 0 générera un fort adoucissement, alors qu’un tendant vers 1 ne générera pas d’adoucissement. Il peut être difficile de déterminer les paramètres de lissage \\(\\alpha\\), \\(\\beta^*\\) et \\(\\phi\\), ainsi que les paramètres d’état \\(l_0\\) et \\(b_0\\). La fonction de forecast::holt() permet de les estimer automatiquement. loti_holt_dF &lt;- holt(loti_ts_tr, damped = FALSE, h = 100) loti_holt_dT &lt;- holt(loti_ts_tr, damped = TRUE, h = 100) plot_grid(autoplot(loti_holt_dF), autoplot(loti_holt_dT)) loti_holt_dF$model$par ## alpha beta l b ## 0.4052862918 0.0001000051 -0.2170967540 0.0052550677 loti_holt_dT$model$par ## alpha beta phi l b ## 0.4843654438 0.0001000061 0.8286096396 -0.1096315698 -0.0332523695 Dans ce cas, l’optimisation de \\(\\phi\\) lui donne une valeur de 0.8, une valeur suffisamment faible pour que l’adoucissement soit fort. Vous obtiendrez une valeur de \\(\\phi\\) plus élevée en ne considérant que les données obtenues depuis 1950 (en décommentant loti_ts &lt;- window(loti_ts, start = 1950), plus haut). 10.3.2.3 SES avec fluctuation saisonnière D’autres paramètres peuvent être ajoutés pour de tenir compte des fluctuations saisonnières (les fluctuations cycliques sont plus difficiles à modéliser) de manière additive ou multiplicative. Voici la modification apportée pour la modélisation additive, en laissant tomber l’adoucissement. Description Équation Prévision \\(\\hat{y}_{t + h|t} = l_t + h \\times b_t + s_{t-m+h_m^+}\\) Niveau $l_t = (y_t - s_{t-m} ) + ( 1-) ( l_{t-1} + b_{t-1} ) $ Tendance \\(b_t = \\beta^* \\left( l_t - l_{t-1} \\right) + (1-\\beta^*) b_{t-1}\\) Saison \\(s_t = \\gamma \\left( y_t - l_{t-1} - b_{t-1} \\right) + (1-\\gamma) s_{t-m}\\) où \\(m\\) est la périodicité des fluctuations saisonnière, par exemple 4 pour quatre saisons annuelles et \\(\\gamma\\) est un paramètre de la portion saisonnière, qui, tout comme un effet aléatoire en biostatistiques, fluctue autour de zéro. La variante multiplicative multiplie la prévision par un facteur plutôt que d’imposer un décalage. La mathématique n’est pas présentée ici pour plus de simplicité (consulter Hyndman et Athanasopoulos (2018), chapitre 7.3 pour plus de détails). Dans le cas multiplicatif, l’effet saisonnier fluctue autour de 1. Si l’amplitude de la fluctuation s’accroît au fil de la série temporelle, la méthode multiplicative donnera probablement de meilleurs résultats. La fonction que nous utiliserons pour les SES-saisonniers est forecast::hw(). Les données de la NASA ne sont pas saisonnières (frequency(loti_ts) donne 1). flow_hw &lt;- hw(flow_ts_train, damped = TRUE, h = 12*3, seasonal = &quot;additive&quot;) autoplot(flow_hw) + autolayer(fitted(flow_hw)) 10.3.2.4 Automatiser la prévision avec les SES L’erreur du modèle peut aussi être calculée de sorte qu’elle soit constante ou augmente selon le niveau (ou décalage). Nous avons donc plusieurs types de modèles de la famille SES. Tendance: [sans tendance, tendance additive, tendance adoucie] Saison: [sans saison, saison additive, saison multiplicative] Erreur: [erreur additive, erreur multiplicative] Lequel choisir? Encore une fois, on peut laisser R optimiser notre choix avec un modèle ETS (error, tend and seasonnal). L’optimisation est lancée avec la fonction forecast::ets(). flow_model &lt;- ets(flow_ts_train) flow_model ## ETS(M,N,M) ## ## Call: ## ets(y = flow_ts_train) ## ## Smoothing parameters: ## alpha = 0.0104 ## gamma = 1e-04 ## ## Initial states: ## l = 127.508 ## s = 0.6511 0.9399 0.7981 0.4272 0.6253 0.8872 ## 0.6594 1.2741 3.7317 1.5211 0.1878 0.2971 ## ## sigma: 0.6065 ## ## AIC AICc BIC ## 1222.469 1228.469 1260.935 Le modèle retenu est un ETS(M,N,M), définissant dans l’ordre le type d’erreur, de tendance et de saison selon A pour additif, M pour multiplicatif et N pour l’absence. Nous avons une erreur de type M (multiplicative), une tendance de type N (sans tendance) et une saison de type M (multiplicative). L’absence de valeur pour phi indique que l’adoucissement n’est probablement pas nécessaire. Nous pouvons visualiser l’évolution des différentes composantes. autoplot(flow_model) Dans un modèle sans tendance, avec saisonnalité multiplicative, les données levels sont multipliées par les données season pour obtenir la prévision. Malgré l’absence de tendance dans le modèle, il semble que le débit a diminué de 2000 à 2003 entre deux états stables de 1998 à 2000 et de 2003 à 2006. La fonction forecast::ets() génère un modèle, mais pas de prédiction. Pour obtenir une prédiction, nous devons utiliser la fonction forecast::forecast(), que j’utiliserai ici en mode tidyverse. flow_ets &lt;- flow_ts_train %&gt;% ets() flow_fc &lt;- flow_ets %&gt;% forecast() flow_fc %&gt;% autoplot() L’analyse d’exactitude et celle des résidus sont toutes aussi pertinentes. La première est effectuée sur la prévision, et la seconde sur le modèle. accuracy(flow_fc, flow_ts) ## ME RMSE MAE MPE MAPE MASE ## Training set -13.346098 59.33195 44.55719 -90.26010 110.68741 0.8170487 ## Test set 3.768712 69.95479 55.55668 -24.65164 62.94846 1.0187472 ## ACF1 Theil&#39;s U ## Training set 0.1612394 NA ## Test set 0.2452020 0.6990499 checkresiduals(flow_ets) ## ## Ljung-Box test ## ## data: Residuals from ETS(M,N,M) ## Q* = 27.068, df = 5.2, p-value = 6.727e-05 ## ## Model df: 14. Total lags used: 19.2 Il est peu probable que les résidus aient été générés par un bruit blanc, indiquant qu’il existe une structure dans les données qui n’a pas été capturée par le modèle. Exercice. Modéliser la série temporelle lynx avec forecast::ets(). Que se passe-t-il? 10.3.2.5 Prétraitement des données J’ai spécifié plus haut que les données de débit pourraient avantageusement être transformées avec un logarithme pour éviter les prédictions de débits négatifs. D’autres types de transformation peuvent être utilisées, comme la racine carrée ou cubique, l’opposée de l’inverse (\\(-1/x\\)) ou les transformations compositionnelles (chapitre 7). La transformation Box-Cox est aussi largement utilisée pour sa polyvalence. \\[ w = \\begin{cases} ln(y_t) &amp;\\text{if } \\lambda = 0 \\\\ \\frac{y_t - 1}{\\lambda} &amp;\\text{if } \\lambda \\neq 0 \\end{cases} \\] \\(\\lambda = 1\\): pas de transformation \\(\\lambda = 1/2\\): ressemble à \\(\\sqrt{y_t}\\) \\(\\lambda = 1/3\\): ressemble à \\(\\sqrt[3]{y_t}\\) \\(\\lambda = 0\\): log naturel \\(\\lambda = -1\\): ressemble à \\(1/y_t\\) La fonction forecast::BoxCox.lambda() estime la valeur optimale de \\(\\lambda\\). BoxCox.lambda(flow_ts_train) ## [1] 0.784101 Cette valeur peut être imputée à l’argument lambda de la fonction forecast::ets(). Dans notre cas, nous désirions plutôt une transformation logarithmique. Conséquemment, nous utilisons lambda = 0. flow_ts_train %&gt;% ets(lambda = 0) %&gt;% forecast() %&gt;% autoplot() Les erreurs ne franchissent pas le 0, mais sont vraisemblablement surestimées lors des sommets. Notez que R s’occupe de la transformation retour. La différenciation est aussi une forme de prétraitement. La différenciation (fonction base::diff()) consiste en la soustraction de la valeur précédente à la valeur suivante. La valeur précédente peut être décalée à la valeur de la période de l’unité temporelle précédente, par exemple le mois de mars de l’année précédente. Un objectif de la différenciation est de rendre la série temporelle stationnaire en termes de tendance et de fluctuation saisonnière, de sorte que la série différenciée se comporte comme un bruit blanc. plot_grid(flow_ts_train %&gt;% autoplot() + ggtitle(&quot;Débit&quot;), loti_ts_tr %&gt;% autoplot() + ggtitle(&quot;LOTI&quot;), flow_ts_train %&gt;% diff(., lag = 12) %&gt;% autoplot() + ggtitle(&quot;Débit avec différenciation saisonnière&quot;), loti_ts_tr %&gt;% diff(., lag = 1) %&gt;% autoplot() + ggtitle(&quot;LOTI avec différenciation d&#39;ordre 1&quot;)) 10.3.3 La méthode ARIMA Un modèle ARIMA, l’acronyme de l’anglais auto-regressive integrated moving average, est une combinaison de trois parties: AR-I-MA. L’autorégression consiste en une régression linéaire dont la variable réponse \\(y_t\\) est la variable à l’instant \\(t\\) et les variables explicatives sont les variables aux instants précédents. Pour un nombre \\(p\\) de périodes précédentes, nous obtenons une régression linéaire typique. \\[y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + ... + \\phi_p y_{t-p} + \\epsilon_t\\] où \\(\\epsilon_t\\) est l’erreur sur la prédiction. La partie concernant la moyenne mobile est une régression non pas sur les observations, mais sur les erreurs. Considérant les \\(q\\) erreurs précédentes, nous obtenons \\[y_t = c + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + ... + \\theta_q \\epsilon_{t-q} + \\epsilon_t\\] La somme de l’autorégression et de la moyenne mobile donne un modèle ARMA. Le I de ARIMA, mis pour integrated, est le contraire de la différenciation, que j’ai présenté à la fin de la section sur les SES. Puisque la série temporelle doit être stationnaire pour effectuer l’ARMA, nous devons différencier la série un nom \\(d\\) de fois avant de procéder à l’autorégression et au calcul de la moyenne mobile. Nous obtenons ainsi une ARIMA d’ordres \\(p\\), \\(d\\) et \\(q\\), notée \\(ARIMA(p,d,q)\\). Nous devons aussi statuer si \\(c\\) (l’intercept ou le drift) doit être ou non considéré come nul. Ces ordres peuvent être spécifiés dans la fonction telle que forecast::Arima(order = c(0, 1, 1), include.constant = TRUE). Toutefois, il est possible de les optimiser grâce à la fonction forecast::auto.arima(). Tout comme les sorties de forecast::ets(), forecast::auto.arima() fourni le modèle, mais pas les prédictions: la fonction forecast::forecast() doit être lancée pour obteir la prédiction. loti_arima &lt;- loti_ts_tr %&gt;% auto.arima() loti_arima %&gt;% forecast(h = 30) %&gt;% autoplot() summary(loti_arima) ## Series: . ## ARIMA(1,1,3) with drift ## ## Coefficients: ## ar1 ma1 ma2 ma3 drift ## -0.9405 0.6260 -0.6019 -0.3716 0.0060 ## s.e. 0.0522 0.0976 0.0847 0.0846 0.0031 ## ## sigma^2 estimated as 0.01036: log likelihood=109.53 ## AIC=-207.06 AICc=-206.35 BIC=-190.14 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE ## Training set -0.0007946749 0.09932376 0.0824145 22.62973 65.41129 ## MASE ACF1 ## Training set 0.8925238 -0.0221428 Le sommaire du modèle spécifie une \\(ARIMA(1,1,3)\\) en utilisant l’intercept \\(c\\) (with drift). Lorsque l’on compte prédire des séries saisonnières, nous devons ajouter un nouveau jeu d’ordres \\((P,D,Q)m\\), où \\(P\\), \\(D\\) et \\(Q\\) sont équivalents à leurs minuscules, mais portent sur des décalages saisonniers et non pas des décalages d’unités de temps. L’ordre \\(m\\) est le nombre de périodes à considérer par unité temporelle, par exemple 12 mois par an. Bonne nouvelle: forecast::auto.arima() automatise le tout. Nous pouvons utiliser lambda = 0 pour effectuer une transformation logarithmique. flow_arima &lt;- flow_ts_train %&gt;% auto.arima(lambda = 0) flow_arima %&gt;% forecast(h = 36) %&gt;% autoplot() summary(flow_arima) ## Series: . ## ARIMA(1,0,0)(2,1,0)[12] ## Box Cox transformation: lambda= 0 ## ## Coefficients: ## ar1 sar1 sar2 ## 0.3029 -0.4913 -0.2687 ## s.e. 0.1074 0.1196 0.1186 ## ## sigma^2 estimated as 0.6561: log likelihood=-101.88 ## AIC=211.77 AICc=212.27 BIC=221.49 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ## Training set 9.038874 71.47442 40.46424 -34.75026 64.84494 0.7419961 ## ACF1 ## Training set -0.1823408 Le sommaire du modèle, ARIMA(1,0,0)(2,1,0)[12] sous le format \\(ARIMA(p,d,q)(P,D,Q)m\\), retourne automatiquement une période de 12 mois avec une différenciation saisonnière mais sans différenciation ordinaire, excluant la moyenne mobile dans les deux cas. Exercice. Il est toujours pertinent d’effectuer l’analyse des résidus… 10.3.4 Les modèles dynamiques J’ai noté précédemment que l’évolution du climat tient compte d’une série de covariables explicatives. De même, le débit dans la rivière Chaudière n’est pas un effet de la saison, mais de son environnement (climat, changements dans la morphologie du paysage, utilisation de l’eau, etc.). La prévision du débit aura avantage à considérer ces covariables. L’ARIMA peut accueillir des covariables en modélisant le terme d’erreur, \\(\\epsilon_t\\) en fonction de séries temporelles conjointes. Le débit mensuel de la rivière Chaudière peut être modélisé en fonction de la température moyenne mensuelle et des précipitations totales mensuelles en ajoutant l’argument xreg à la fonction forecast::auto.arima(). L’argument consiste en la matrice temporelle des variables explicatives. Notez que forecast::auto.arima() ne fonctionne pas (encore?) avec l’interface-formule de R, mais que l’on peut se débrouiller en transformant en série temporelle la sortie de la fonction base::model.matrix(), qui elle peut accueillir une formule. hm_tr &lt;- window(hydrometeo_monthly_ts, end = c(2004, 12)) hm_te &lt;- window(hydrometeo_monthly_ts, start = c(2005, 1)) flow_darima &lt;- auto.arima(y = hm_tr[, &quot;Débit&quot;], xreg = hm_tr[, c(&quot;total_precip&quot;, &quot;mean_temp&quot;)], lambda = 0) summary(flow_darima) ## Series: hm_tr[, &quot;Débit&quot;] ## Regression with ARIMA(1,0,0)(0,1,1)[12] errors ## Box Cox transformation: lambda= 0 ## ## Coefficients: ## ar1 sma1 total_precip mean_temp ## 0.3213 -0.6236 0.0028 -0.0215 ## s.e. 0.1148 0.1828 0.0017 0.0451 ## ## sigma^2 estimated as 0.5392: log likelihood=-80.87 ## AIC=171.74 AICc=172.65 BIC=183.12 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ## Training set 3.046006 64.76333 37.60995 -31.06861 57.3032 0.7149168 ## ACF1 ## Training set -0.3068227 Note. Pour obtenir des résultats plus précis, mais dont les résultats seront plus longs à venir, spécifiez l’argument stepwise = FALSE dans la fonction forecast::auto.arima(). Les coefficients sur les covariables sont interprétables dans l’échelle de la prévision transformée. Ainsi, 1 mm de précipitation par mois augmentera le logarithme naturel du débit augmente de 0.0028. De même, 1 °C de température moyenne diminuera le logarithme naturel du débit augmente de 0.0215: notez que l’erreur standard sur ce coefficient étant très élevée, le coefficient n’est à première vue pas différent de 0. La température moyenne aurait avantage à être remplacée par un meilleur indicateur incluant les périodes d’accumulation de neige et de leur fonte. Pas mal doc? Source: Scène de Back to the future, Robert Zemeckis et and Bob Gale, 1985 La prévision d’un modèle dynamique demandera les séries temporelles des covariables, qui peuvent elles-mêmes être modélisées ou être issues de simulations. Dans notre cas, nous pouvons utiliser la série de test. flow_darima %&gt;% forecast(xreg = hm_te[, c(&quot;total_precip&quot;, &quot;mean_temp&quot;)]) %&gt;% autoplot() checkresiduals(flow_darima) ## ## Ljung-Box test ## ## data: Residuals from Regression with ARIMA(1,0,0)(0,1,1)[12] errors ## Q* = 16.849, df = 12.8, p-value = 0.1955 ## ## Model df: 4. Total lags used: 16.8 10.3.5 Les modèles TBATS Les modèles TBATS (Hyndman et Athanasopoulos, 2018) combinent tout ce que l’on a vu jusqu’à présent, à l’exception notable des covariables, dans une interface automatisée. L’automatisation a l’avantage d’une utilisation rapide, mais donne parfois des prédictions erronées. lynx_tbats &lt;- lynx %&gt;% tbats() lynx_tbats_f &lt;- lynx_tbats %&gt;% forecast() lynx_tbats_f %&gt;% autoplot() summary(lynx_tbats_f) ## ## Forecast method: BATS(0.159, {5,1}, 0.833, -) ## ## Model Information: ## BATS(0.159, {5,1}, 0.833, -) ## ## Call: tbats(y = .) ## ## Parameters ## Lambda: 0.159431 ## Alpha: 0.4096962 ## Beta: 0.06308072 ## Damping Parameter: 0.833291 ## AR coefficients: 1.166918 -0.79967 0.178396 -0.165326 -0.174982 ## MA coefficients: -0.391172 ## ## Seed States: ## [,1] ## [1,] 15.0399753 ## [2,] 0.4123534 ## [3,] 0.0000000 ## [4,] 0.0000000 ## [5,] 0.0000000 ## [6,] 0.0000000 ## [7,] 0.0000000 ## [8,] 0.0000000 ## attr(,&quot;lambda&quot;) ## [1] 0.1594313 ## ## Sigma: 1.542751 ## AIC: 1956.137 ## ## Error measures: ## ME RMSE MAE MPE MAPE MASE ## Training set 52.88873 827.169 496.3222 -21.21278 50.42654 0.5973607 ## ACF1 ## Training set -0.04569254 ## ## Forecasts: ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 1935 3144.2286 1772.9365 5314.924 1279.84707 6900.473 ## 1936 2272.7187 842.6749 5351.691 464.08249 8064.647 ## 1937 1466.3925 410.3827 4224.271 185.16011 6932.120 ## 1938 981.1883 227.2540 3210.040 88.86649 5547.989 ## 1939 807.6476 170.9933 2800.598 62.30711 4954.037 ## 1940 902.9068 196.5571 3075.407 73.22121 5401.634 ## 1941 1286.8205 307.5223 4124.040 123.20099 7068.774 ## 1942 1964.6976 520.9678 5872.628 225.83614 9784.659 ## 1943 2722.5747 765.2794 7819.328 346.39977 12815.672 ## 1944 3100.2748 837.7743 9148.331 368.32463 15162.079 Le sommaire du modèle le type de modèle sélectionné ainsi que ses paramètres. Le titre du graphique en donne aussi un aperçu: BATS(0.159, {5,1}, 0.833, -): Le coefficient lambda utilisé est 0.159. Le {5, 1} signifie que p = 5 et et q = 1. L’adoucissement est de 0.833 Aucune période n’est incluse L’approche TBATS performe bien pour les données à fluctuations cycliques. Toutefois, les intervalles prévisionnels son souvent trop larges et l’optimisation peut être longue. 10.4 Pour terminer… Nous avons vu comment manipuler des séries temporelles avec dplyr et le format de base ts. Le nouveau format de série temporelle du module tsibble permet des manipulations dans un flux de travail plus conforme au tidyverse. De même, la nouvelle mouture du module forecast nommée fable, réécrite vers l’approche tidyverse, offrira des fonctions permettant notamment une hiérarchisation dans les fluctuations saisonnières, par exemple des cycles journaliers enchâssés dans des cycles hebdomadaires, enchâssés dans des cycles trimestriels. Le module prophet, distribué par Facebook en mode open source, gagne en popularité. Bien qu’il soit réputé pour offrir des prévisions fiables, ses bases mathématiques me semblent insuffisamment documentées. Fait intéressant: prophet est en mesure d’effectuer des prévisions en mode dynamique. Quoi qu’il en soit, j’ai favorisé des modules plus matures et mieux documentés, qui pourront vous servir de tremplin vers les nouveaux modules. Maintenant, le futur vous appartient. "],
["chapitre-git.html", "11 Science ouverte et reproductibilité 11.1 Un code reproductible 11.2 Introduction à GitHub 11.3 Introduction à Pakrat 📦🐀", " 11 Science ouverte et reproductibilité ️ Objectifs spécifiques: À la fin de ce chapitre, vous saurez exprimer l’importance et les enjeux de la science ouverte saurez arranger vos données (format csv) et votre code (format notebook) afin de rendre vos recherches reproductibles saurez comment créer un dépôt sur GitHub, puis administrer son développement La science ouverte favorise la diffusion des connaissances à travers plusieurs aspects. Méthodologie ouverte. Ce n’est pas pour rien que les revues scientifiques demandent de la minutie dans la description de la méthodologie: c’est pour s’assurer de bien comprendre la signification des données collectées et faire en sorte que vos données puissent être échantillonnées de la même manière dans une potentielle expérience subséquente. À ce titre, la revue Nature a créé le site de publication de protocoles expérimentaux Protocol exchange, “où la communauté scientifique met en commun son savoir-faire expérimental pour accélérer la recherche” (ma traduction). Données ouvertes. En rendant nos données publiques, on permet à la postérité de les utiliser pour améliorer les connaissances, découvrir des structures qui nous avaient échappées, etc. Dans certains cas, l’ouverture des données peut être contrainte par des enjeux légaux (données privées) ou éthiques (données pouvant être utilisées à mauvais escient). Dans la plupart des cas, les avantages surpassent largement les risques encourus par la publication des données, et les informations personnelles peuvent être retirées. Des journaux comme Plos exigent que les données minimales à la reproduction de l’expérience soient fournies en tant que matériel supplémentaire. Code source ouvert. Les logiciels open source, comme R, sont gratuits pour la plupart. Cela permet à quiconque de les utiliser, pourvu que l’on possède le support matériel (un ordinateur) et une connection internet. De la même manière, le code R qui vous a permis de générer des résultats à partir de vos données peut être rendu public sous toutes sortes de licenses open source peu restrictive (GPL, BSD, MIT, etc.). Avec les données et le code, vos travaux pourront être reproduits. Révision ouverte. La révision est un travail essentiel en science. Traditionnellement, les publications scientifiques sont révisés de manière anonyme, le but étant d’éviter les conflits. Récemment, des revues comme Frontiers ont déployé des modes de révision ouverts, permettant (1) des échanges plus constructifs entre auteurs et réviseurs et (2) de remercier ouvertement la contribution des réviseurs à l’article final. Accès ouvert. Les éditeurs scientifiques sont largement critiqués pour demander des frais usuraires aux bibliothèques et pour la consultation à la pièce, ainsi que des frais de publication démesurés. En réaction à cela, le site Sci-Hub débloque gratuitement des millions d’articles scientifiques. Aussi, des journaux sérieux comme Plos et Frontiers publient de facto les articles sur leur site internet, de sorte qu’ils peuvent être librement téléchargés. Le manque d’ouverture dans la science a mené plusieurs scientifiques à parler d’une crise de la reproductibilité (Baker, 2016). Dans ce chapitre, nous verrons quelques astuces pour que R devienne un outil favorisant la science ouverte. À la fin de ce chapitre, vous devriez être en mesure de déployer votre code sur une archive en ligne, comme ceci. Figure 11.1: Exemple d’un dossier de code et de données ouvertes, (Jeanne et al. 2019) 11.1 Un code reproductible Figure 11.2: A Guide to Reproducible Code in Ecology and Evolution, BES 2017 La British ecological society offre des lignes guide pour créer un flux de travail reproductible (BES, 2017). En outre, les principes suivants doivent être respectés (ma traduction, avec ajouts). Commencez votre analyse à partir d’une copie des données brutes. Les données doivent être fournies dans un format ouvert (csv, json, sqlite, etc.). Évitez de démarrer une analyse par un chiffrier électronique ou un logiciel propriétaire (qui n’est pas open source). En ce sens, démarrer avec Excel (xls ou xlsx) est à éviter, tout comme les sont les données encodées pour SPSS ou SAS. Toute opération sur les données, que ce soit du nettoyage, des fusions, des transformations, etc. devrait être effectuée avec du code, non pas manuellement. S’il s’agit d’une erreur de frappe dans un tableau, on peut déroger à la règle. Mais s’il s’agit par exemple d’élimier des outliers, ne supprimez pas des entrées de vos données brutes. De même, n’effectuez pas de transformation de vos données brutes à l’extérieur du code. En somme, vos calculs devraient être en mesure d’être lancés d’un seul coup, sans opérations manuelles intermédiaires. Séparez vos opérations en unités logiques thématiques. Par exemple, vous pourriez séparer votre code en parties: (i) charger, fusionner et nettoyer les données, (ii) analyser les données, (iii) créer des fichiers comme des tableaux et des figures. Éliminez la duplication du code en créant des fonctions personnalisées. Assurez-vous de commenter vos fonctions en détails, expliquez ce qui est attendu comme entrées et comme sorties, ce qu’elles font et pourquoi. Documentez votre code et vos données à même les feuilles de calcul ou dans un fichier de documentation séparé. Tout fichier intermédiaire devrait être séparé de vos données brutes. 11.1.1 Structure d’un projet Un projet de calcul devrait être contenu en un seul dossier. Si vous n’avez que quelques projets, il est assez facile de garder l’info en mémoire. Toutefois, en particulier en milieu d’entreprise, il se pourrait fort bien que vous ayez à mener plusieurs projets de front. Certaines entreprises créent des numéros de projet: vous aurez avantage à nommer vos dossiers avec ces numéros, incluant une brève description. Pour ma part, j’ordonne mes projets chronologiquement par année, avec un descriptif. 📁 2019_abeille-canneberge Notez que je n’utilise ni espace, ni caractère spécial dans le nom du fichier, pour éviter les erreurs potentielles avec des logiciels capricieux. À l’intérieur du dossier racine du projet, j’inclus l’information générale: données source (souvent des fichiers Excel), manuscrit (mémoire, thèse, article, etc.) documentation particulière (pour les articles, j’utilise Zotero, un gestionnaire de référence), photos et, évidemment, mon dossier de code (par exemple rstats). 📁 2019_abeille-canneberge |-📁 documentation |-📁 manuscrit |-📁 photos |-📁 rstats |-📁 source Si vous rédigez votre manuscrit à même votre code (en Latex, Lyx, markdown ou R markdown que nous verrons cela plus loin), vous pouvez très bien l’inclure dans votre fichier de calcul. À l’intérieur du fichier de calcul, vous aurez votre projet RStudio et vos feuilles de calcul séquencées. J’utilise 01-, et non pas 1- pour éviter que le 10- suive le 1- dans le classement en ordre alpha-numérique au cas où j’aurais plus de 10 feuilles de calcul. J’inclus un fichier README.md (extension md pour markdown), qui contient les informations générales de mes calculs. Les données brutes (csv) sont placées dans un dossier data, mes graphiques sont exportés dans un dossier image, mes tableaux sont exportés dans un dossier tables et mes fonctions externes sont exportées dans un dossier lib. 📁 rstats |-📁 data |-📁 images |-📁 lib |-📁 tables 📄 bees.Rproj 📄 01_clean-data.R 📄 02_data-mining.R 📄 03_data-analysis.R 📄 04_data-modeling.R 📄 README.md Je décris les noms de fichiers dans la langue de communication utile pour le rendu final du projet, souvent en anglais lors de publications académiques. J’évite les noms de fichier qui ne sont pas informatifs, par exemple 01.R ou Rplot1.png, ainsi que les majuscules, les caractères spéciaux et les espaces comme dans Deuxième essai.R (le README.md est une exception). Pour partager un dossier de projet sur R, on n’a qu’à le compresser (zip), puis l’envoyer. Pour que le code fonctionne sur un autre ordinateur, les liens vers les fichiers de données à importer ou les graphiques exportés doivent être relatifs au fichier R ouvert dans votre projet, non pas le chemin complet sur votre ordinateur. Figure 11.3: Retrouvez votre chemin, dessin de Allison Horst Tout comme la BSE, l’organisme sans but lucratif rOpenSci offre un guide sur la reproductibilité. 11.1.2 Le format R markdown Un code reproductible est un code bien décrit. La structure de projet présentée précédemment propose de segmenter le code en plusieurs fichiers R. Cette manière de procéder est optionnelle. Si le fichier de calcul n’est pas trop encombrant, on pourra n’en utiliser qu’un seul, par exemple stats.R. À l’intérieur même des feuilles de calcul R, vous devrez commenter votre code pour en expliquer les étapes, par exemple: ############# ## Titre 1 ## ############# # Titre 2 ## Titre 3 data &lt;- read_csv(&quot;data/abeilles.csv&quot;) # commentaire particulier RStudio a développé une approche plus conviviale avec son format R markdown. Le langage markdown permet de formater un texte avec un minimum de décorations, et R markdown permet d’intégrer du texte et des codes. Ces notes de cours sont par ailleurs entièrement écrites en R markdown. 11.1.2.1 Le langage markdown Un fichier portant l’extension .md ou .markdown est un fichier texte clair (que vous pouvez ouvrir et éditer dans n’importe votre éditeur texte préféré), tout comme un fichier .R. Il existe néanmoins de nombreux éditeurs de texte spécialisés en édition markdown - mon préféré est Zettlr. Les décorations principales en markdown sont les suivantes (les citations utilisées ci-après sont tirées du roman Dune, de Frank Herbert). Italique. Pour emphaser en italique, balisez le texte avec des astérisques. Par exemple, “Pourrais-je porter parmi vous le nom de *Paul-Muad'dib*?” devient “Pourrais-je porter parmi vous le nom de Paul-Muad’dib?” Gras. Pour emphaser en gras, balisez le texte avec des doubles astérisques. Par exemple, “L'espérance **ternit** l'observation.” devient “L’espérance ternit l’observation”. Largeur fixe. Pour un texte à largeur fixe (signifiant du code), balisez le texte avec des accents graves. Par exemple, “Quel nom donnez-vous à la petite `souris`, celle qui saute ?” devient “Quel nom donnez-vous à la petite souris, celle qui saute?” Listes. Pour effectuer une liste numérotée, utilisez le chiffre 1. Par exemple, 1. Paul 1. Leto 1. Alia devient Paul Jessica Alia De même, pour une liste à puces, changez le 1. par le - ou le *. Entêtes. Les titres sont précédés par des #. Un # pour un titre 1, deux ## pour un titre 2, etc. Par exemple, # Imperium ## Landsraad ### Maison des Atréides ### Maison des Harkonnen ## CHOAM # Guilde des navigateurs Insérera les titres appropriés (que je n’insère pas pour ne pas bousiller la structure de ce texte). Liens. Pour insérer des liens, le texte est entre crochet directement suivi du lien entre parenthèses. Par exemple, “Longue vie aux [combattants](https://youtu.be/Cv87NJ2xX0k?t=59)” devient “Longue vie aux combattants”. Équations. Les équations suivent la syntaxe Latex entre deux $$ pour les équations sur une ligne et entre des doubles $$$$ pour les équations sur un paragraphe. Par exemple, $c = \\sqrt{a^2 + b^2}$ devient \\(c = \\sqrt{a^2 + b^2}\\). Images. Pour insérer une image, ![nom de l'image](images/spice-must-flow.png). Une liste exhaustive des balises markdown est disponible sous forme d’aide-mémoire. 11.1.2.2 R markdown Dans RStudio, ouvrez un R markdown par File &gt; New file &gt; R Markdown. Si le module rmarkdown n’est pas installé, RStudio vous demandera de l’installer. Une fenêtre apparaîtra. Figure 11.4: Nouveau fichier R markdown Les options d’exportation pourront être modifiées par la suite. Un fichier d’exemple sera créé, et vous pourrez le modifier. Les parties de texte sont écrits en markdown, et le code R est enchâssé entre les balises ```{r} et ```. Je nommerai ces parties de code des cellules de code. Des options de code l’intérieur peuvent être utilisées à l’intérieur des accolades {r}. Par exemple {r, filtre-outliers} donne le nom filtre-outliers au bloc de code, qui permet nommément de nommer les images créer dans le bloc de code. {r, eval = FALSE} permet d’activer (TRUE, valeur par défaut) ou de désactiver (FALSE) le calcul de la cellule. {r, echo = FALSE} permet de n’afficher que la sortie de la cellule de code en n’affichant pas le code, par exemple un graphique ou le sommaire d’une régression. {r, results = FALSE} permet de n’afficher que le code, mais pas la sortie. {r, warning = FALSE, message = FALSE, error = FALSE} n’affichera pas les avertissements, les messages automatiques et les messages d’erreur. {r, fig.width = 10, fig.height = 5, fig.align = &quot;center&quot;} affichera les graphiques dans les dimensions voulues, alignée au centre (&quot;center&quot;), à gauche (&quot;left&quot;) ou à droite (&quot;right&quot;). Notez que vous pouvez exécuter rapidement du code sur une ligne avec la formulation `r `, par exemple la moyenne des nombres `\\r a&lt;-round(runif(4, 0, 10)); a` est de `\\r mean(a)`, en enlevant les \\ devant les r (ajoutées artificiellement pour éviter que le code soit calculé) sera la moyenne des nombres 7, 2, 2, 8 est de 4.75 Une fois que vous serez satisfait de votre document, cliquer sur Knit et le fichier de sortie sera généré. Le guide qui permet de générer le fichier de sortie est tout en haut du fichier. Nous l’appelons le YAML (acronyme récursif de YAML Ain’t Markup Language). Prenez le YAML suivant. --- title: &quot;Dune&quot; author: &quot;Frank Herbert&quot; date: &quot;1965-08-01&quot; output: github_document --- Le titre, l’auteur et la date sont spécifiées. Pour indiquer la date courante, on peut simplement la générer avec R en remplaçant &quot;1965-08-01&quot; par 2019-03-21. La spécification output indique le type de document à générer, par exemple html_document pour une page web, pdf_document pour un pdf, ou word_document pour un docx. Dans ce cas-ci, j’indique github_document pour créer un fichier markdown comprenant nommément des liens relatifs vers les images des graphiques générés. Pourquoi un github_document? C’est le sujet de la prochaine sous-section. Mais avant cela, je vous réfère à un autre aide-mémoire. Figure 11.5: Aide-mémoire pour R Markdown, Source: RStudio 11.2 Introduction à GitHub Le system de suivi de version git (open source) a été créé par Linus Torvalds, aussi connu pour avoir créé Linux. git prend une photo de votre répertoire de projet à chaque fois que vous commettez un changement. Vous pourrez revenir sans problème sur d’anciennes versions si quelque chose tourne mal, et vous pourrez publier le résultat final sur un service d’hébergement utilisant git. Il existe plusieurs services pour rendre git utilisable en ligne, mais GitHub est définitivement le plus utilisé d’entre tous. La plateforme GitHub est presque devenue un réseau social de développement. GitHub, maintenant la propriété de Microsoft, n’est en soi pas open source. Si cela vous pose problème, je vous redirige vers la plateforme open source GitLab, qui fonctionne à peu près de la même manière que GitHub (alors que la plateforme GitHub sera fort probablement toujours vivante dans plusieurs années, on en est moins sûr pour GitLab, c’est pourquoi j’utilise GitHub à des fins professionnelles mais j’utilise GitLab à des fins personnelles). Pour suivre cette partie, je vous invite à créer un compte sur GitHub ou GitLab, à votre choix. Créez un nouveau dépôt (New repository). Figure 11.6: Nouveau dépôt avec GitHub Figure 11.7: Nouveau dépôt avec GitLab Pour utiliser git, vous pourrez toujours travailler en ligne de commande, mais je vous suggère d’utiliser GitHub desktop (qui fonctionne aussi sur GitLab). Github desktop (ou tout auter logiciel de gestion de git) vous permettra d’abord de cloner un répertoire en ligne. Le clonage vous permet de créer une copie locale du répertoire. Figure 11.8: Cloner dépôt avec GitHub Figure 11.9: Cloner dépôt avec GitLab Une fois que le dépôt est cloné, il est sur voter ordinateur. Lorsque vous effectuez un changement, vous devez commettre (commit), puis envoyer (push) vos changements vers le dépôt en ligne. Pour que votre document markdown soit lisible par GitHub et GitLab, il doit être exporté sous forme de github_document. Un fichier .md sera créé, et inclura les détails de votre document de calculs. Figure 11.10: Commettre et déployer un dépôt avec GitHub L’interface de GitHub Desktop vous permet de revenir en arrière en éliminant des commits précédents. Figure 11.11: Revenir en arrière avec GitHub desktop Vous pourrez ajouter des collaborateurs à votre dépôt, pour que plusieurs personnes travaillent de front sur un même dépôt. Il est aussi possible de créer une branche d’un dépôt, fusionner la branche de développement avec la branche principale, commenter les codes, suggérer des changements, etc., mais cela sort du cadre d’un cours sur la reproductibilité. Enfin, pour renvoyer un article vers votre matériel supplémentaire, insérez le lien dans la section méthodologie. Il peut s’agit du lien complet, ou bien d’un lien raccourci avec git.io. Par exemple, The data and the R code used to compute the results are both available as supplementary material at https://git.io/fhHEj. Notez que RStudio offre une interface pour utiliser git via un onglet afiché en haut à droite dans l’affichage par défaut. Ne l’ayant jamais utilisé, et je ne me sens pas à l’aise d’en suggérer l’utilisation, mais libre à vous d’explorer cet outil et de vous l’approprier! Figure 11.12: L’outil Git de RStudio 11.3 Introduction à Pakrat 📦🐀 Alors que les modules sont continuellement mis à jour, on doit s’assurer que l’on sache exactement quelle version a été utilisée si l’on désire être stricte sur la reproductibilité. Lorsque je révise un article, je demande à ce que le nom des modules utilisés et leur numéro de version soient explicitement cités et référencés. Par exemple, dans un article sur l’analyse de compositions foliaires de laitues inoculées par une bactérie, j’écrivais: Computations were performed in the R statistical language version 3.4.1 (R Development Core Team, 2017). The main packages used in the data analysis workflow were the vegan package version 2.4-3 (Oksanen et al., 2017) for ordination, the compositions package version 1.40-1 (van den Boogaart and Tolosana-Delgado, 2013) for ilr transformations, the nlme version 3.1-131 (Pinheiro et al., 2017) package to compute the random experimental effect, the mvoutlier package version 2.0.8 (Filzmoser and Gschwandtner, 2017) for multivariate outlier detection, and the ggplot2 package version 2.2.1 (Wickham and Chang, 2017) for data visualization. The data and computations are publicly available at https://github.com/essicolo/Nicolas-et-al_Infected-lettuce-ionomics. Nicolas et al., 2019 De cette manière, une personne (que ce soit vos collègues, quiconque voudra auditer ou évaluer votre code ou vous-même dans le futur) pourra reproduire le code publié sur GitHub en installant les versions de R et des modules cités. Mais cela est fastidieux. C’est pourquoi l’équipe de RStudio (oui, encore ceux-là) ont développé le module packrat, qui permet d’installer les modules à même voter dossier de projet (le dossier contenant le fichier .Rproj). Pour l’utiliser à tout moment en cours de projet, Figure 11.13: L’outil Packrat de RStudio Le .gitignore contient tous les documents et les types de documents qui sont ignorés par git. L’option par défaut est d’ignorer le dossier lib, qui contient les modules installés, mais de garder le dossier src, qui contient la source des modules non installés (qui devront être installés par les autres personnes utilisant votre projet). Mieux vaut garder les options par défaut. Initialiser Packrat revient à scanner vos documents de projet pour trouver les modules utilisés et créer un paquet contenant tout cela à même votre projet, dans un dossier packrat. 📁 rstats |-📁 data |-📁 images |-📁 lib |-📁 packrat |-📁 tables 📄 sentier-d-or.Rproj 📄 stats.Rmd 📄 README.md Ce dossier contiendra tout ce qu’il faut pour utiliser les modules du projet d’une personne que l’on nommera Leto. Lorsqu’une autre personne, appellons-la Ghanima, utilisera le projet de Leto, RStudio vérifiera si le module packrat est bien installé, et l’installera s’il ne l’est pas (Leto et Ghanima sont deux personnage de la série de romans Dune). Pour utiliser les modules du projet et non pas les modules de son ordinateur, Ghanima lancera la fonction packrat::restore(). Si Leto décide de mettre à jour ses modules en cours de projet, il lancera la fonction packrat::snapshot() pour que ces nouveaux modules soit intégrés à son projet. Lorsque Leto commettra (commit) ses changements dans git et les publiera (push) sur GitHub, puis lorsque Ghanima mettra à jour (fetch) son dépôt local git lié au dépôt GitHub, elle devra à nouveau lancer packrat::restore() pour que les modules soient bel et bien ceux utilisés par Leto. "],
["chapitre-ml.html", "12 Autoapprentissage 12.1 Objectifs 12.2 Lexique 12.3 Démarche 12.4 Algorithmes 12.5 L’autoapprentissage en R 12.6 Les k plus proches voisins 12.7 Les arbres décisionnels 12.8 Les réseaux neuronaux 12.9 Les processus gaussiens", " 12 Autoapprentissage ️ Objectifs spécifiques: À la fin de ce chapitre, vous saurez établir un plan de modélisation par autoapprentissage saurez définir le sous-apprentissage et le surapprentissage serez en mesure d’effectuer un autoapprentissage avec les techniques des k-proches voisins, les arbres de décision, les forêts aléatoires, les réseaux neuronnaux et les processus gaussiens Plusieurs cas d’espèces en sciences et génies peuvent être approchés en liant un variable avec une ou plusieurs autres à l’aide de régressions linéaires, polynomiales, sinusoïdales, exponentielle, sigmoïdales, etc. Encore faut-il s’assurer que ces formes préétablies représentent le phénomène de manière fiable. Lorsque la forme de la réponse est difficile à envisager, en particulier dans des cas non-linéaires ou impliquant plusieurs variables, on pourra faire appel à des modèles dont la structure n’est pas contrôlée par une équation rigide gouvernée par des paramètres (comme la pente ou l’intercept). L’autoapprentissage, apprentissage automatique, ou machine learning, vise à détecter des structures complexes émergeant d’ensembles de données à l’aide des mathématiques et de processus automatisés afin de prédire l’émergence de futures occurrences. Comme ensemble de techniques empiriques, l’autoapprentissage est un cas particulier de l’intelligence artificielle, qui elle inclut aussi les mécanismes déterministes et des ensembles d’opérations logiques. Par exemple, les premiers ordinateurs à compétitionner aux échecs se basaient sur des règles de logique (si la reine noire est positionnée en c3 et qu’un le fou blanc est en position f6 et que … alors bouge la tour en g5 - j’écris n’importe quoi). Il s’agissait d’intelligence artificielle, mais pas d’autoapprentissage. L’autoapprentissage passera davantage par la simulation de nombreuses parties et dégagera la structure optimale pour l’emporter considérant les positions des pièces sur l’échiquier. 12.1 Objectifs Comprendre les applications possibles de l’autoapprentissage Comprendre le flux de travail d’une opération d’autoapprentissage Comprendre les principes soutenant les techniques des k plus proches voisins, des arbres décisionnels, des réseaux neuronaux et des processus gaussiens. Plus spécifiquement, vous devrez à la fin de cette section être en mesure de prédire une variable catégorie ou numérique à partir de données observées. 12.2 Lexique L’autoapprentissage possède son jargon particulier. Puisque certains termes peuvent porter à confusion, voici quelques définitions de termes que j’utiliserai dans ce chapitre. Réponse. La variable que l’on cherche à obtenir. Il peut s’agir d’une variable continue comme d’une variable catégorielle. On la nomme aussi la cible. Prédicteur. Une variable utilisée pour prédire une réponse. Les prédicteurs sont des variables continues. Les prédicteurs de type catégoriel doivent préalablement être dummifiés (voir chapitre 5). On nomme les prédicteurs les entrées. Apprentissage supervisé et non-supervisé. Si vous avez suivi le cours jusqu’ici, vous avez déjà utilisé des outils entrant dans la grande famille de l’apprentissage automatique. La régression linéaire, par exemple, vise à minimiser l’erreur sur la réponse en optimisant les coefficients de pente et l’intercept. Un apprentissage supervisé a une cible, comme c’est le cas de la régression linéaire. En revanche, un apprentissage non supervisé n’en a pas: on laisse l’algorithme le soin de détecter des structures intéressantes. Nous avons déjà utilisé cette approche. Pensez-y un peu… l’analyse en composante principale ou en coordonnées principales, ainsi que le partitionnement hiérarchique ou non sont des exemples d’apprentissage non supervisé. En revanche, l’analyse de redondance a une réponse. L’analyse discriminante aussi, bien que sa réponse soit catégorielle. L’apprentissage non supervisé ayant déjà été couvert au chapitre 7, ce chapitre ne s’intéresse qu’à l’apprentissage supervisé. Régression et Classification. Alors que la régression est un type d’apprentissage automatique pour les réponses continues, la classification vise à prédire une réponse catégorielle. Il existe des algorithmes uniquement application à la régression, uniquement applicables à la classification, et plusieurs autres adaptable aux deux situations. Données d’entraînement et données de test. Lorsque l’on génère un modèle, on désire qu’il sache comment réagir à ses prédicteurs. Cela se fait avec des données d’entraînement, sur lesquelles on calibre et valide le modèle. Les données de test servent à vérifier si le modèle est en mesure de prédire des réponses sur lesquelles il n’a pas été entraîné. Fonction de perte. Une fonction qui mesure l’erreur d’un modèle. 12.3 Démarche La première tâche est d’explorer les données, ce que nous avons couvert au chapitres 3 et 4. 12.3.1 Prétraitement Pour la plupart des techniques d’autoapprentissage, le choix de l’échelle de mesure est déterminant sur la modélisation subséquente. Par exemple, un algorithme basé sur la distance comme les k plus proches voisins ne mesurera pas les mêmes distances entre deux observations si l’on change l’unité de mesure d’une variable du mètre au kilomètre. Il est donc important d’effectuer, ou d’envisager la possibilité d’effectuer un prétraitement sur les données. Je vous réfère au chapitre 6 (en développement) pour plus de détails sur le prétraitement. 12.3.2 Entraînement et test Vous connaissez peut-être l’expression sportive “avoir l’avantage du terrain”. Il s’agit d’un principe prétendant que les athlètes performent mieux en terrain connu. Idem pour les modèles phénoménologiques. Il est possible qu’un modèle fonctionne très bien sur les données avec lesquelles il a été entraîné, mais très mal sur des données externes. De mauvaises prédictions effectuées à partir d’un modèle qui semblait bien se comporter peut mener à des décisions qui, pourtant prises de manière confiante, se révèlent fallacieuses au point d’aboutir à de graves conséquences. C’est pourquoi, en mode prédictif, on doit évaluer la précision et la justesse d’un modèle sur des données qui n’ont pas été utilisés dans son entraînement. En pratique, il convient de séparer un tableau de données en deux: un tableau d’entraînement et un tableau de test. Il n’existe pas de standards sur le ratio à utiliser. Cela dépend de la prudence de l’analyse et de l’ampleur de son tableau de données. Certaines personnes préférerons couper le tableau à 50%. D’autres préférerons réserver le deux-tiers des données pour l’entraînement, ou 70%, 75%. Rarement, réservera-t-on moins plus de 50% et moins de 20% à la phase de test. Si les données sont peu équilibrées (par exemple, on retrouve peu de données de l’espèce \\(A\\), que l’on retrouve peu de données à un pH inférieur à 5 ou que l’on a peu de données croisées de l’espèce \\(A\\) à ph inférieur à 5), il y a un danger qu’une trop grande part, voire toute les données, se retrouvent dans le tableau d’entraînement (certaines situations ne seront ainsi pas testées) ou dans le tableau de test (certaines situations ne seront pas couvertes par le modèle). L’analyste doit s’assurer de séparer le tableau au hasard, mais de manière consciencieuse. 12.3.3 Sousapprentissage et surapprentissage Une difficulté en modélisation phénoménologique est ce qui tient de la structure et ce qui tient du bruit. Lorsque l’on considère une structure comme du bruit, on est dans un cas de sousapprentissage. Lorsque, au contraire, on interprète du bruit comme une structure, on est en cas de surapprentissage. Les graphiques suivant présentent ces deux cas, avec au centre un cas d’apprentissage conforme. set.seed(35473) n &lt;- 50 x &lt;- seq(0, 20, length = n) y &lt;- 500 + 0.4 * (x-10)^3 + rnorm(n, mean=10, sd=80) # le bruit est généré par rnorm() par(mfrow = c(1, 3)) plot(x, y, main = &quot;Sousapprentissage&quot;, col = &quot;#46c19a&quot;, pch=16) lines(x, predict(lm(y~x)), col = &quot;#b94a73&quot;) plot(x, y, main = &quot;Apprentissage conforme&quot;, col = &quot;#46c19a&quot;, pch=16) lines(x, predict(lm(y~x + I(x^2) + I(x^3))), col = &quot;#b94a73&quot;) plot(x, y, main = &quot;Surapprentissage&quot;, col = &quot;#46c19a&quot;, pch=16) lines(x, predict(lm(y~x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10) + I(x^11) + I(x^12) + I(x^13) + I(x^14) + I(x^15) + I(x^16))), col = &quot;#b94a73&quot;) Afin d’éviter les cas de mésapprentissage on peut avoir recours à la validation croisée. 12.3.4 Validation croisée Souvent confondue avec le fait de séparer le tableau en phases d’entraînement et de test, la validation croisée est un principe incluant plusieurs algorithmes qui consiste à entraîner le modèle sur un échantillonnage aléatoire des données d’entraînement. La technique la plus utilisée est le k-fold, où l’on sépare aléatoirement le tableau d’entraînement en un nombre k de tableaux. À chaque étape de la validation croisée, on calibre le modèle sur tous les tableaux sauf un, puis on valide le modèle sur le tableau exclu. La performance du modèle en entraînement est jugée sur les validations. 12.3.5 Choix de l’algorithme d’apprentissage Face aux centaines d’algorithmes d’apprentissages qui vous sont offertes, choisir l’algorithme ou les algorithmes adéquats pour vos données n’est pas facile. Ce choix sera motivé par les tenants et aboutissants des algorithmes, votre expérience, l’expérience de la littérature, l’expérience de vos collègues. Une approche raisonnable est de tester plusieurs modèles et d’approfondir si ce n’est déjà fait la mathématique des options retenues. Il existe des algorithmes génétiques, qui ne sont pas couverts ici, permettent de sélectionner des modèles d’autoapprentissages optimaux. Un de ces algorithmes est offert par le module Python tpot. 12.3.6 Déploiement RData, Shiny En résumé, Explorer les données Sélectionner des algorithmes Effectuer un prétraitement Créer un ensemble d’entraînement et un ensemble de test Lisser les données sur les données d’entraînement avec validation croisée Tester le modèle Déployer le modèle 12.4 Algorithmes Il existe des centaines d’algorithmes d’apprentissage. Je n’en couvrirai que quatre, qui me semblent être appropriés pour la modélisation phénoménologique des systèmes vivants, et utilisables pour la régression et la classification. Les k plus proches voisins Les arbres de décision Les réseaux neuronaux Les processus gaussiens 12.5 L’autoapprentissage en R Plusieurs options sont disponibles. Les modules que l’on retrouve en R pour l’autoapprentissage sont nombreux, et parfois spécialisés. Il est possible de les utiliser individuellement. Chacun de ces modules fonctionne à sa façon. Le module caret de R a été conçu pour donner accès à des centaines de fonctions d’autoapprentissage via une interface commune. Le module mlr occupe sensiblement le même créneau que caret, mais utilise plutôt une approche par objets connectés. Au moment d’écrire ces lignes, mlr est peu documenté, donc a priori plus complexe à prendre en main. En Python, le module scikit-learn offre un interface unique pour l’utilisation de nombreuses techniques d’autoapprentissage. Il est possible d’appeler des fonctions de Python à partir de R grâce au module reticulate. Dans ce chapitre, nous verrons comment fonctionnent certains algorithmes sélectionnés, puis nous les appliquerons avec le module respectif qui m’a semblé le plus approprié. Vous remarquerez néanmoins des références récurrentes aux modules de Python. En ce moment, la force de R réside dans la gestion des tableaux, les tests statistiques, l’exploration heuristique et la visualisation de données. Néanmoins, Python le surpasse pour l’autoapprentissage… library(&quot;tidyverse&quot;) # évidemment library(&quot;caret&quot;) ## ## Attaching package: &#39;caret&#39; ## The following object is masked from &#39;package:compositions&#39;: ## ## R2 ## The following object is masked from &#39;package:pls&#39;: ## ## R2 ## The following object is masked from &#39;package:vegan&#39;: ## ## tolerance ## The following object is masked from &#39;package:purrr&#39;: ## ## lift 12.6 Les k plus proches voisins “Le… l’idée en arrière pour être… euh… simpliste, là c’est que c’est un peu de… euhmm… de la vitamine de vinyle.” - Georges (Les voisins, une pièce de Claude Meunier) Pour dire comme Georges, le… l’idée en arrière des KNN pour être… euh… simpliste, c’est qu’un objet va ressembler à ce qui se trouve dans son voisinage. Les KNN se basent en effet sur une métrique de distance pour rechercher un nombre k de points situés à proximité de la mesure. Les k points les plus proches sont retenus, k étant un entier non nul à optimiser. Un autre paramètre parfois utilisé est la distance maximale des voisins à considérer: un voisin trop éloigné pourra être discarté. La réponse attribuée à la mesure est calculée à partir de la réponse des k voisins retenus. Dans le cas d’une régression, on utiliser généralement la moyenne. Dans le cas de la classification, la mesure prendra la catégorie qui sera la plus présente chez les k plus proches voisins. L’algorithme des k plus proches voisins est relativement simple à comprendre. Certains pièges sont, de même, peuvent être contournés facilement. Imaginez que vous rechercher les points les plus rapprochés dans un système de coordonnées géographiques où les coordonnées \\(x\\) sont exprimées en mètres et les coordonnées \\(y\\), en centimètres. Vous y projetez trois points. data &lt;- data.frame(X = c(0, 1, 0), Y = c(0, 0, 1), row.names = c(&#39;A&#39;, &#39;B&#39;, &#39;C&#39;)) options(repr.plot.width = 4, repr.plot.height = 4) par(pty=&quot;s&quot;) plot(data, cex=3, xlab = &#39;Position X (m)&#39;, ylab = &#39;Position Y (cm)&#39;) text(data, labels = rownames(data)) Techniquement la distance A-B est 100 plus élevée que la distance A-C, mais l’algorithme ne se soucie pas de la métrique que vous utilisez. Il est primordial dans ce cas d’utiliser la même métrique. Cette stratégie est évidente lorsque les variables sont comparables. C’est rarement le cas, que ce soit lorsque l’on compare des dimensions physionomiques (la longueur d’une phalange ou celle d’un fémur) mais lorsque les variables incluent des mélanges de longueurs, des pH, des décomptes, etc., il est important de bien identifier la métrique et le type de distance qu’il convient le mieux d’utiliser. En outre, la standardisation des données à une moyenne de zéro et à un écart-type de 1 est une approche courrament utilisée. 12.6.1 Exemple d’application Pour ce premier exemple, je présenterai un cheminement d’autoapprentissage, du prétraitement au test. # ionome 12.7 Les arbres décisionnels Les Ents, tiré du film le Seigneur des anneaux Un arbre décisionnel est une collection hiérarchisée de décisions, le plus souvent binaires. Chaque embranchement est un test à vrai ou faux sur une variable. La réponse, que ce soit une catégorie ou une valeur numérique, se trouve au bout de la dernière branche. Les suites de décisions sont organisées de manière à ce que la précision de la réponse soit optimisée. Par exemple, … 12.8 Les réseaux neuronaux Après les KNN et les random forests, nous passons au domaine plus complexe des réseaux neuronaux. Le terme réseau neuronal est une métaphore liée à une perception que l’on avait du fonctionnement du cerveau humain lorsque la technique des réseaux neuronaux a été développée dans les années 1950. Un réseau neuronal comprend une série de boîtes d’entrées liée à des fonctions qui transforment et acheminent successivement l’information jusqu’à la sortie d’une ou plusieurs réponse. Il existe plusieurs formes de réseaux neuronnaux, dont la plus simple manifestation est le perceptron multicouche. Dans l’exemple suivant, on retrouve 4 variables d’entrée et trois variables de sortie entre lesquelles on retrouve 5 couches dont le nombre de neurones varient entre 3 et 6. Source: Neural designer Entre la première couche de neurones (les variables prédictives) et la dernière couche (les variables réponse), on retrouve des couches cachées. Chaque neurone est relié à tous les neurones de la couche suivante. Les liens sont des poids, qui peuvent prendre des valeurs dans l’ensemble des nombres réels. À chaque neurone suivant la première couche, on fait la somme des poids multipliés par la sortie du neurone. Le nombre obtenu entre dans chaque neurone de la couche. Le neurone est une fonction, souvent très simple, qui transforme le nombre. La fonction plus utilisée est probablement la fonction ReLU, pour rectified linear unit, qui expulse le même nombre aux neurones de la prochaine couche s’il est positif: sinon, il expulse un zéro. Exercice. Si tous les neurones sont des fonctions ReLU, calculez la sortie de ce petit réseau neuronal. Vous trouverez la réponse sur l’image images/11_nn_ex1_R.jpg. Il est aussi possible d’ajouter un biais à chaque neurone, qui est un nombre réel additionné à la somme des neurones pondérée par les poids. L’optimisation les poids pour chaque lien et les biais pour chaque neurone (grâce à des algorithmes dont le fonctionnement sort du cadre de ce cours) constitue le processus d’apprentissage. Avec l’aide de logiciels et de modules spécialisés, la construction de réseaux de centaines de neurones organisés en centaines de couches vous permettra de capter des patrons complexes dans des ensembles de données. Vous avez peut-être déjà entendu parler d’apprentissage profond (ou deep learning). Il s’agit simplement d’une appellation des réseaux neuronaux modernisé pour insister sur la présence de plusieurs couches de neurones. C’est un terme à la mode. 12.8.1 Les réseaux neuronaux sur R avec neuralnet Plusieurs modules sont disponibles sur R pour l’apprentissage profond. Certains utilisent le module H2O.ia, propulsé en Java, d’autres utilisent plutôt Keras, propulsé en Python par l’intermédiaire de tensorflow. J’ai une préférence pour Keras, puisqu’il supporte les réseaux neuronaux classiques (perceptrons multicouche) autant que convolutifs ou récurrents. Keras pourrait néanmoins être difficile à installer sur Windows, où Python ne vient pas par défaut. Sur Windows, Keras ne fonctionne qu’avec Anaconda: vous devez donc installez Anaconda ou Miniconda (Miniconda offre une installation minimaliste). Mais pour ce cours, nous utiliserons le module neuralnet. Il est possible de l’utilser grâce à l’interface de caret, mais son utilisation directe permet davantage de flexibilité. Chargeons les données. library(&quot;neuralnet&quot;) ## ## Attaching package: &#39;neuralnet&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## compute data(&quot;iris&quot;) Prenons soin de segmenter nos données en entraînement et en test. set.seed(8453668) iris_tr_index &lt;- createDataPartition(y=iris$Species, p = 0.75, list = FALSE) Nous pouvons ainsi créer nos tableaux d’entraînement et de test pour les variables prédictives. Les réseaux neuronnaux sont aptes à générer des sorties multiples. Nous désirons prédire une catégorie, et neuralnet ne s’occupe pas de les transformer de facto. Lors de la prédiction d’une catégorie, nous devons générée des sorties multiples qui permettront de décider de l’appartenance exclusive à une catégorie ou une autre. Nous avons abordé l’encodage catégoriel aux chapitres 5 et 7. C’est ce que nous ferons ici. species_oh &lt;- model.matrix(~ 0 + Species, iris) colnames(species_oh) &lt;- levels(iris$Species) iris_oh &lt;- iris %&gt;% cbind(species_oh) Lançons le réseau neuronnal avec l’interface-formule de R (neuralnet n’accepte pas le . pour indiquer prend toutes les variables à l’exeption de celles utilisées en y): il faut les lsiter. L’argument hidden est un vecteur qui indique le nombre de neuronnes pour chaque couche. L’argument linear.input indique si l’on désire travailler en régression (linear.output = TRUE) ou en classification (linear.output = FALSE). Lorsque les données sont nombreuses, patience, le calcul prend pas mal de temps. Dans ce cas-ci, nous avons un tout petit tableau. nn &lt;- neuralnet(setosa + versicolor + virginica ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data = iris_oh %&gt;% dplyr::slice(iris_tr_index), hidden = c(3, 5), linear.output = FALSE) Un réseau neuronnal peu complexe peut être lisible. plot(nn) Il n’existe pas de règle stricte sur le nombre de couche et le nombre de noeud par couche. Il est néanmoins conseillé de générer d’abord un modèle simple, puis au besoin de le complexifier graduellement en terme de nombre de noeuds, puis de nombre de couches. Si vous désirez aller plus loin et utiliser keras, le module autokeras, disponible seulement en Python, est conçu pour optimiser un modèle Keras. compute_te &lt;- compute(nn, iris_oh %&gt;% select(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width) %&gt;% dplyr::slice(-iris_tr_index)) pred_te &lt;- compute_te$net.result %&gt;% as_tibble() %&gt;% apply(., 1, which.max) %&gt;% levels(iris$Species)[.] ## Warning: `as_tibble.matrix()` requires a matrix with column names or a `.name_repair` argument. Using compatibility `.name_repair`. ## This warning is displayed once per session. confusionMatrix(iris_oh %&gt;% dplyr::slice(-iris_tr_index) %&gt;% select(Species) %&gt;% pull() %&gt;% as.factor(), pred_te %&gt;% as.factor()) ## Confusion Matrix and Statistics ## ## Reference ## Prediction setosa versicolor virginica ## setosa 12 0 0 ## versicolor 2 10 0 ## virginica 0 0 12 ## ## Overall Statistics ## ## Accuracy : 0.9444 ## 95% CI : (0.8134, 0.9932) ## No Information Rate : 0.3889 ## P-Value [Acc &gt; NIR] : 2.763e-12 ## ## Kappa : 0.9167 ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: setosa Class: versicolor Class: virginica ## Sensitivity 0.8571 1.0000 1.0000 ## Specificity 1.0000 0.9231 1.0000 ## Pos Pred Value 1.0000 0.8333 1.0000 ## Neg Pred Value 0.9167 1.0000 1.0000 ## Prevalence 0.3889 0.2778 0.3333 ## Detection Rate 0.3333 0.2778 0.3333 ## Detection Prevalence 0.3333 0.3333 0.3333 ## Balanced Accuracy 0.9286 0.9615 1.0000 12.8.1.1 Pour aller plus loin En une heure divisée en 4 vidéos, Grant Sanderson explique les réseaux neuronaux de manière intuitive. En ce qui a trait à Keras, je recommande le livre Deep learning with R, de François Allaire, auquel vous avez accès avec un IDUL de l’Université Laval. Si vous vous sentez à l’aise à utiliser Keras avec le langage Python, je vous recommande le cours gratuit en ligne Applications of deep neural networks, de Jeff Heaton. Des types de réseaux neuronaux spécialisés ont été développés. Je les présente sans aller dans les détails. Réseaux neuronaux convolutif. Ce type de réseau neuronal est surtout utilisé en reconnaissance d’image. Les couches de neurones convolutifs possèdent, en plus des fonctions des perceptrons classiques, des filtres permettant d’intégrer les variables descriptives connexes à l’observation: dans le cas d’une image, il s’agit de scanner les pixels au pourtour du pixel traité. Une brève introduction sur Youtube. Réseaux neuronaux récurrents. Prédire des occurrences futures à partir de séries temporelles implique que la réponse au temps t dépend non seulement de conditions externes, mais aussi le la réponse au temps t-1. Les réseaux neuronaux récurrents. Vous devrez ajouter des neurones particuliers pour cette tâche, qui pourra être pris en charge par Keras grâce aux couches de type Long Short-Term Memory network, ou LSTM. Réseaux neuronaux probabilistes. Les réseaux neuronaux non-probabilistes offre une estimation de la variable réponse. Mais quelle est la crédibilité de la réponse selon les variables descriptives? Question qui pourrait se révéler cruciale en médecine ou en ingénierie, à la laquelle on pourra répondre en mode probabiliste. Pour ce faire, on pose des distributions a priori sur les poids du réseau neuronal. Le module edward, programmé et distribué en Python, offre cette possibilité. Vous pourrez accéder à edward grâce au module reticulate, mais à ce stade mieux vaudra basculer en Python. Pour en savoir davantage, considérez cette conférence de Andrew Rowan. 12.9 Les processus gaussiens Les sorties des techniques que sont les KNN, les arbres ou les forêts ainsi que les réseaux neuronaux sont (classiquement) des nombres réels ou des catégories. Dans les cas où la crédibilité de la réponse est importante, il devient pertinent que la sortie soit probabiliste: les prédictions seront alors présentées sous forme de distributions de probabilité. Dans le cas d’une classification, la sortie du modèle sera un vecteur de probabilité qu’une observation appartienne à une classe ou à une autre. Dans celui d’une régression, on obtiendra une distribution continue. Les processus gaussiens tirent profit des statistiques bayésiennes pour effectuer des prédictions probabilistes. D’autres techniques peuvent être utilisées pour effectuer des prédictions probabilistes, comme les réseaux neuronaux probabilistes, que j’ai introduits précédemment. Bien que les processus gaussiens peuvent être utilisés pour la classification, son fonctionnement s’explique favorablement, de manière intuitive, pas la régression. 12.9.1 Un approche intuitive Ayant acquis de l’expérience en enseignement des processus gaussiens, John Cunningham a développé une approche intuitive permettant de saisir les mécanismes des processus gaussiens. lors de conférences disponible sur YouTube (1, 2), il aborde le sujet par la nécessité d’effectuer une régression non-linéaire. Générons d’abord une variable prédictive x, l’heure, et une variable réponse y, le rythme cardiaque d’un individu en battements par minute (bpm). x &lt;- c(7, 8, 10, 14, 17) y &lt;- c(61, 74, 69, 67, 78) plot(x, y, xlab=&quot;Heure&quot;, ylab=&quot;Rythme cardiaque (bpm)&quot;) abline(v=12, lty=3, col=&#39;gray50&#39;);text(12, 67, &#39;?&#39;, cex=2) abline(v=16, lty=3, col=&#39;gray50&#39;);text(16, 72, &#39;?&#39;, cex=2) Poser un problème par un processus gaussien, c’est se demander les valeurs crédibles qui pourraient être obtenues hors du domaine d’observations (par exemple, dans la figure ci-dessus, à x=12 et x=16)? Ou bien, de manière plus générale, quelles fonctions ont pu générer les variables réponse à partir d’une structure dans les variables prédictives? Les distributions normales, que nous appellerons gaussiennes dans cette section par concordance avec le terme processus gaussien, sont particulièrement utiles pour répondre à cette question. Nous avons vu précédemment ce que sont les distributions de probabilité: des outils mathématiques permettant d’appréhender la structure des processus aléatoires. Une distribution gaussienne représente une situation où l’on tire au hasard des valeurs continues. Une distribution gaussienne de la variable aléatoire \\(X\\) de moyenne \\(0\\) et de variance de \\(1\\) est notée ainsi: \\[ X \\sim \\mathcal{N} \\left( 0, 1\\right)\\] Par exemple, une courbe de distribution gaussienne du rythme cardiaque à 7:00 pourrait prendre la forme suivante. \\[ bpm \\sim \\mathcal{N} \\left( 65, 5\\right)\\] En R: x_sequence &lt;- seq(50, 80, length=100) plot(x_sequence, dnorm(x_sequence, mean=65, sd=5), type=&quot;l&quot;, xlab=&quot;Rythme cardiaque (bpm)&quot;, ylab=&quot;Densité&quot;) Une distribution binormale, un cas particulier de la distribution multinormale, comprendra deux vecteurs, \\(x_1\\) et \\(x_2\\). Elle aura donc deux moyennes. Puisqu’il s’agit d’une distribution binormale, et non pas deux distributions normales, les deux variables ne sont pas indépendantes et l’on utilisera une matrice de covariance au lieu de deux variances indépendantes. \\[ \\binom{x_1}{x_2} \\sim \\mathcal{N} \\Bigg( \\binom{\\mu_1}{\\mu_2}, \\left[ {\\begin{array}{cc} \\Sigma_{x_1} &amp; \\Sigma_{x_1,x_2} \\\\ \\Sigma_{x_1,x_2}^T &amp; \\Sigma_{x_2} \\\\ \\end{array} } \\right] \\Bigg) \\] La matrice \\(\\Sigma\\), dite de variance-covariance, indique sur sa diagonale les variances des variables (\\(\\Sigma_{x_1}\\) et \\(\\Sigma_{x_2}\\)). Les covariances \\(\\Sigma_{x_1,x_2}\\) et \\(\\Sigma_{x_1,x_2}^T\\) sont symétriques et indiquent le lien entre les variables. On pourrait supposer que le rythme cardiaque à 8:00 soit corrélé avec celui à 7:00. Mises ensembles, les distributions gaussiennes à 7:00 et à 8:00 formeraient une distribution gaussienne binormale. \\[ \\binom{bpm_7}{bpm_8} \\sim \\mathcal{N} \\Bigg( \\binom{65}{75}, \\left[ {\\begin{array}{cc} 10 &amp; 6 \\\\ 6 &amp; 15 \\\\ \\end{array} } \\right] \\Bigg) \\] En R: library(&quot;ellipse&quot;) ## ## Attaching package: &#39;ellipse&#39; ## The following object is masked from &#39;package:car&#39;: ## ## ellipse ## The following object is masked from &#39;package:graphics&#39;: ## ## pairs means_vec &lt;- c(65, 75) covariance_mat &lt;- matrix(c(10, 6, 6, 15), ncol=2) par(pty=&#39;s&#39;) plot(ellipse(x=covariance_mat, centre=means_vec, levels=0.95), type=&#39;l&#39;, xlab=&quot;Rythme cardiaque à 7:00 (bpm)&quot;, ylab=&quot;Rythme cardiaque à 8:00 (bpm)&quot;) #lines(ellipse(x=covariance_mat, centre=means_vec, level=0.8)) On peut se poser la question: étant donnée que \\(x_1 = 68\\), quelle serait la distribution de \\(x_2\\)? Dans ce cas bivariée, la distribution marginale serait univariée, mais dans le cas multivarié en \\(D\\) dimensions, la distribution marginale où l’on spécifie \\(m\\) variables serait de \\(D-m\\). de Une propriété fondamentale d’une distribution gaussienne est que peu importe l’endroit où l’angle selon lequel on la tranche, la distribution marginale sera aussi gaussienne. Lorsque l’on retranche une ou plusieurs variables en spécifiant la valeur qu’elles prennent, on applique un conditionnement à la distribution. library(&quot;condMVNorm&quot;) condition_x1 &lt;- 61 # changer ce chiffre pour visualiser l&#39;effet cond_parameters &lt;- condMVN(mean=means_vec, sigma=covariance_mat, dependent=2, given=1, X.given=condition_x1) cond_mean &lt;- cond_parameters$condMean cond_sd &lt;- sqrt(cond_parameters$condVar) x2_sequence &lt;- seq(50, 90, length=100) x2_dens &lt;- dnorm(x2_sequence, mean=cond_mean, sd=cond_sd) par(pty=&#39;s&#39;) plot(ellipse(x=covariance_mat, centre=means_vec, levels=0.95), type=&#39;l&#39;, xlab=&quot;Rythme cardiaque à 7:00 (bpm)&quot;, ylab=&quot;Rythme cardiaque à 8:00 (bpm)&quot;) abline(v=condition_x1, col=&#39;#f8ad00&#39;, lwd=2, lty=2) lines(x=condition_x1 + x2_dens*40, y=x2_sequence, col=&quot;#f8ad00&quot;, lwd=2) lines(x = c(condition_x1, condition_x1), y = c(cond_mean-cond_sd, cond_mean+cond_sd), lwd=3, col=&#39;#46c19a&#39;) points(condition_x1, cond_mean, col=&#39;#46c19a&#39;, pch=16, cex=2) n_sample &lt;- 20 points(x = rep(condition_x1, n_sample), y = rnorm(n_sample, cond_mean, cond_sd), pch=4, col = rgb(0, 0, 0, 0.5)) Les points sur l’axe (symbole x) conditionnés sont des échantillons tirés au hasard dans la distribution conditionnée. Une autre manière de visualiser la distribution gaussienne binormale est de placer \\(x_1\\) et \\(x_2\\) côte à côte en abscisse, avec leur valeur en ordonnée. Le bloc de code suivant peut sembler lourd au premier coup d’œil: pas de panique, il s’agit surtout d’instructions graphiques. Vous pouvez vous amuser à changer les paramètres de la distribution binormale (section 1) ainsi que la valeur de \\(x_1\\) à laquelle est conditionnée la distribution de \\(x_2\\) (section 2). source(&quot;lib/plot_matrix.R&quot;) # 1. Distribution means_vec &lt;- c(65, 65) covariance_mat &lt;- matrix(c(10, 6, 6, 15), ncol=2) # 2. Condition condition_x1 &lt;- 61 # changer ce chiffre pour visualiser l&#39;effet # 3. Densité conditionnée cond_parameters &lt;- condMVN(mean=means_vec, sigma=covariance_mat, dependent=2, given=1, X.given=condition_x1) cond_mean &lt;- cond_parameters$condMean cond_sd &lt;- sqrt(cond_parameters$condVar) x2_sequence &lt;- seq(50, 90, length=100) x2_dens &lt;- dnorm(x2_sequence, mean=cond_mean, sd=cond_sd) x2_draw &lt;- rnorm(1, cond_mean, cond_sd) # 4. Graphiques options(repr.plot.width = 8, repr.plot.height = 5) layout(matrix(c(1,2,3,3), nrow=2), widths=c(1,2)) par(mar=c(4, 4, 1, 1), pty=&#39;s&#39;) ## 4.1 Ellipse plot(ellipse(x=covariance_mat, centre=means_vec, levels=0.95), type=&#39;l&#39;, xlab=&quot;BPM à 7:00&quot;, ylab=&quot;BPM à 8:00&quot;) abline(v=condition_x1, col=&#39;#f8ad00&#39;, lwd=1) lines(x=condition_x1 + x2_dens*40, y=x2_sequence, col=&quot;#f8ad00&quot;, lwd=1) lines(x = c(condition_x1, condition_x1), y = c(cond_mean-cond_sd, cond_mean+cond_sd), lwd=2, col=&#39;#46c19a&#39;) points(condition_x1, cond_mean, col=&#39;#46c19a&#39;, pch=16, cex=1) points(condition_x1, x2_draw, pch=16, col=&quot;#b94a73&quot;) ## 4.2 Covariance plot_matrix(covariance_mat) ## 4.3 Série plot(c(1, 2), c(condition_x1, x2_draw), xlim=c(0, 6), ylim=c(55, 75), type=&#39;l&#39;, xlab=&quot;Indice de la variable&quot;, ylab=&quot;Rythme cardiaque (bpm)&quot;) points(1, condition_x1, pch=16, col=&#39;#46c19a&#39;, cex=3) points(2, x2_draw, pch=16, col=&#39;#b94a73&#39;, cex=3) Les valeurs que peuvent prendre le rythme cardiaque en \\(x_2\\) sont tirées aléatoirement d’une distribution conditionnée. Sautons maintenant au cas multinormal, incluant 6 variables (hexanormal!). Afin d’éviter de composer une matrice de covariance à la mitaine, je me permets de la générer avec une fonction. Cette fonction particulière est nommée fonction de base radiale ou exponentiel de la racine. \\[K_{RBF} \\left( x_i, x_j \\right) = \\sigma^2 exp \\left( -\\frac{\\left( x_i - x_j \\right)^2}{2 l^2} \\right) \\] RBF_kernel &lt;- function(x, sigma, l) { n &lt;- length(x) k &lt;- matrix(ncol = n, nrow = n) for (i in 1:n) { for (j in 1:n) { k[i, j] = sigma^2 * exp(-1/(2*l^2) * (x[i] - x[j])^2) } } colnames(k) &lt;- paste0(&#39;x&#39;, 1:n) rownames(k) &lt;- colnames(k) return(k) } Dans la fonction RBF_kernel, x désigne les dimensions, sigma désigne un écart-type commun à chacune des dimensions et l est la longueur désignant l’amplification de la covariance entre des dimensions éloignées (dans le sens que la première dimension est éloignée de la dernière). Pour 6 dimensions, avec un écart-type de 4 et une longueur de 2. covariance_6 &lt;- RBF_kernel(1:6, sigma=4, l=2) round(covariance_6, 2) ## x1 x2 x3 x4 x5 x6 ## x1 16.00 14.12 9.70 5.19 2.17 0.70 ## x2 14.12 16.00 14.12 9.70 5.19 2.17 ## x3 9.70 14.12 16.00 14.12 9.70 5.19 ## x4 5.19 9.70 14.12 16.00 14.12 9.70 ## x5 2.17 5.19 9.70 14.12 16.00 14.12 ## x6 0.70 2.17 5.19 9.70 14.12 16.00 Changez la valeur de l permet de bien saisir son influence sur la matrice de covariance. Avec un l de 1, la covariance entre \\(x_1\\) et \\(x_6\\) est pratiquement nulle: elle est un peut plus élevée avec l=2. Pour reprendre l’exemple du rythme cardiaque, on devrait en effet s’attendre à retrouver une plus grande corrélation entre celles mesurées aux temps 4 et 5 qu’entre les temps 1 et 6. De même que dans la situation où nous avions une distribution binormale, nous pouvons conditionner une distribution multinormale. Dans l’exemple suivant, je conditionne la distribution multinormale de 6 dimensions en spécifiant les valeurs prises par les deux premières dimensions. Le résultat du conditionnement est une distribution en 4 dimensions. Puisqu’il est difficile de présenter une distribution en 6D, le graphique en haut à gauche ne comprend que les dimensions 1 et 6. Remarquez que la corrélation entre les dimensions 1 et 6 est faible, en concordance avec la matrice de covariance générée par la fonction RBF_kernel. Lancez plusieurs fois le code et voyez ce qui advient des échantillonnages dans les dimensions 3 à 6 selon le conditionnement en 1 et 2. library(&quot;MASS&quot;) # 1. Distribution means_vec &lt;- rep(65, 6) covariance_mat &lt;- covariance_6 # 2. Condition conditions_x &lt;- c(61, 74) # changer ces chiffres pour visualiser l&#39;effet # 3. Densité conditionnée cond_parameters &lt;- condMVN(mean=means_vec, sigma=covariance_mat, dependent.ind = 3:6, given.ind=1:2, X.given=conditions_x) cond_mean &lt;- cond_parameters$condMean cond_sd &lt;- sqrt(cond_parameters$condVar) x6_sequence &lt;- seq(50, 90, length=100) x6_dens &lt;- dnorm(x2_sequence, mean=cond_mean[4], sd=cond_sd[4, 4]) x_3.6_draw &lt;- mvrnorm(n = 1, mu = cond_mean, Sigma = cond_sd^2) # 4. Graphiques layout(matrix(c(1,2,3,3), nrow=2), widths=c(1,2)) par(mar=c(4, 4, 1, 1)) ## 4.1 Ellipse plot(ellipse(x=covariance_mat[c(1, 6), c(1, 6)], centre=means_vec[c(1, 6)], levels=0.95), type=&#39;l&#39;, xlab=&quot;BPM à 7:00&quot;, ylab=&quot;BPM à 8:00&quot;) abline(v=conditions_x[1], col=&#39;#f8ad00&#39;, lwd=1) lines(x=condition_x1 + x6_dens*40, y=x2_sequence, col=&quot;#f8ad00&quot;, lwd=1) lines(x = c(conditions_x[1], conditions_x[1]), y = c(cond_mean[4]-cond_sd[4, 4], cond_mean[4]+cond_sd[4, 4]), lwd=2, col=&#39;#46c19a&#39;) points(conditions_x[1], cond_mean[4], col=&#39;#46c19a&#39;, pch=16, cex=1) points(conditions_x[1], x_3.6_draw[4], pch=16, col=&quot;#b94a73&quot;) ## 4.2 Covariance plot_matrix(covariance_mat, cex=0.8) ## 4.3 Série plot(1:6, c(conditions_x, x_3.6_draw), xlim=c(0, 6), ylim=c(60, 85), type=&#39;l&#39;, xlab=&quot;Indice de la variable&quot;, ylab=&quot;Rythme cardiaque (bpm)&quot;) points(c(1, 2), conditions_x, pch=16, col=&#39;#46c19a&#39;, cex=3) points(3:6, x_3.6_draw, pch=16, col=&#39;#b94a73&#39;, cex=3) La structure de la covariance assure que les dimensions proches prennent des valeurs similaires, assurant une courbe lisse et non en dents de scie. Pourquoi s’arrêter à 6 dimensions? Prenons-en plusieurs, puis générons plus d’un échantillon. Ensuite, utilisons ces simulations pour de calculer la moyenne et l’écart-type de chacune des dimensions. # 1. Distribution n &lt;- 20 means_vec &lt;- rep(65, n) covariance_mat &lt;- RBF_kernel(x = 1:n, sigma = 10, l = 2) # 2. Condition conditions_x &lt;- c(61, 74) # changer ces chiffres pour visualiser l&#39;effet # 3. Densité conditionnée cond_parameters &lt;- condMVN(mean=means_vec, sigma=covariance_mat, dependent.ind = 3:n, given.ind=1:2, X.given=conditions_x) cond_mean &lt;- cond_parameters$condMean cond_sd &lt;- cond_parameters$condVar # 4. Graphiques par(mar=c(4, 4, 1, 1)) ## 4.3 Série plot(0, 0, xlim=c(0, n), ylim=c(40, 95), type=&#39;l&#39;, xlab=&quot;Indice de la variable&quot;, ylab=&quot;Rythme cardiaque (bpm)&quot;) samples &lt;- 50 x_3.n_draw &lt;- mvrnorm(n = samples, mu = cond_mean, Sigma = cond_sd) for (i in 1:samples) { lines(1:n, c(conditions_x, x_3.n_draw[i, ]), col = rgb(0, 0, 0, 0.15)) } x_3.n_draw_mean &lt;- apply(x_3.n_draw, 2, mean) x_3.n_draw_sd &lt;- apply(x_3.n_draw, 2, stats::sd) lines(1:n, c(conditions_x, x_3.n_draw_mean), lwd = 2) lines(1:n, c(conditions_x, x_3.n_draw_mean + x_3.n_draw_sd), col = &quot;#b94a73&quot;, lwd = 2) lines(1:n, c(conditions_x, x_3.n_draw_mean - x_3.n_draw_sd), col = &quot;#b94a73&quot;, lwd = 2) points(c(1, 2), conditions_x, pch=16, col=&#39;#46c19a&#39;, cex=2) Revenons au rythme cardiaque. On pourra utiliser le conditionnement aux temps observés, soit 7:00, 8:00, 10:00, 14:00 et 17:00 pour estimer la distribution à 12:00 et 16:00, où à des dimensions artificielles quelconques ici fixées aux demi-heures. # 1. Distribution n &lt;- 21 means_vec &lt;- rep(65, n) covariance_mat &lt;- RBF_kernel(x = 1:n, sigma = 5, l = 2) # 2. Condition conditions_x &lt;- c(61, 74, 69, 67, 78) conditions_indices &lt;- c(1, 3, 7, 15, 21) dependent_indices &lt;- (1:20)[! 1:20 %in% conditions_indices] # 3. Densité conditionnée cond_parameters &lt;- condMVN(mean=means_vec, sigma=covariance_mat, dependent.ind = dependent_indices, given.ind=conditions_indices, X.given=conditions_x) cond_mean &lt;- cond_parameters$condMean cond_sd &lt;- cond_parameters$condVar samples &lt;- 100 x_draw &lt;- mvrnorm(n = samples, mu = cond_mean, Sigma = cond_sd) means_draw &lt;- apply(x_draw, 2, mean) sd_draw &lt;- apply(x_draw, 2, stats::sd) # 4. Graphiques par(mar=c(4, 4, 1, 1)) ## 4.1 Combiner les prédictions bpm &lt;- rep(NA, n) bpm[conditions_indices] &lt;- conditions_x bpm[dependent_indices] &lt;- means_draw bpm_sd &lt;- rep(NA, n) bpm_sd[conditions_indices] &lt;- 0 bpm_sd[dependent_indices] &lt;- sd_draw ## 4.2 Combiner les tirages et les données x_draw_all &lt;- matrix(ncol = n, nrow = samples) for (i in 1:length(conditions_x)) x_draw_all[, conditions_indices[i]] &lt;- conditions_x[i] x_draw_all[, dependent_indices] &lt;- x_draw ## 4.3 Série plot(1:n, bpm, xlim=c(0, n), ylim=c(40, 90), type=&#39;l&#39;, lwd = 2, xlab=&quot;Indice de la variable&quot;, ylab=&quot;Rythme cardiaque (bpm)&quot;) for (i in 1:samples) { lines(1:n, x_draw_all[i, ], col = rgb(0, 0, 0, 0.1)) } lines(1:n, bpm+bpm_sd, col = &quot;#b94a73&quot;, lwd = 2) lines(1:n, bpm-bpm_sd, col = &quot;#b94a73&quot;, lwd = 2) points(conditions_indices, bpm[conditions_indices], pch=16, col=&#39;#46c19a&#39;, cex=2) Comme on devrait s’y attendre, la régression résultant de la mise en indices de la distribution est précise aux mesures, et imprécise aux indices peu garnis en mesures. Nous avions utilisé 21 dimensions. Lorsque l’on généralise la procédure à une quantité infinie de dimensions, on obtient un processus gaussien. L’indice de la variable devient ainsi une valeur réelle. Un processus gaussien, \\(\\mathcal{GP}\\), est défini par une fonction de la moyenne, \\(m \\left( x \\right)\\), et une autre de la covariance que l’on nomme noyau (ou kernel), \\(K \\left( x, x&#39; \\right)\\). Un processus gaussien est noté de la manière suivante: \\[\\mathcal{GP} \\sim \\left( m \\left( x \\right), K \\left( x, x&#39; \\right) \\right)\\] La fonction définissant la moyenne peut être facilement écartée en s’assurant de centrer la variable réponse à zéro (\\(y_{centré} = y - \\hat{y}\\)). Ainsi, par convention, on spécifie une fonction de moyenne comme retournant toujours un zéro. Quant au noyau, il peut prendre différentes fonctions de covariance ou combinaisons de fonctions de covariance. Règle générale, on utilisera un noyau permettant de définir deux paramètres: la hauteur (\\(\\sigma\\)) et la longueur de l’ondulation (\\(l\\)). hyperparameters &lt;- expand.grid(l=c(1, 3, 9), sigma=1:3) # Graphique n &lt;- 100 samples_list &lt;- list() for (i in 1:nrow(hyperparameters)) { sample &lt;- mvrnorm(n = 1, mu = rep(0, n), Sigma = RBF_kernel(x=1:n, sigma = hyperparameters$sigma[i], l = hyperparameters$l[i])) samples_list[[i]] &lt;- data.frame(sigma = paste(&quot;sigma =&quot;, hyperparameters$sigma[i]), l = paste(&quot;l =&quot;, hyperparameters$l[i]), x = 1:n, sample = sample) } samples_df &lt;- bind_rows(samples_list) ## Warning in bind_rows_(x, .id): Unequal factor levels: coercing to character ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector ## Warning in bind_rows_(x, .id): Unequal factor levels: coercing to character ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector samples_df %&gt;% ggplot(mapping = aes(x = x, y = sample)) + geom_line() + facet_grid(l ~ sigma) On pourra ajouter à ce noyau un bruit blanc, c’est-à-dire une variation purement aléatoire, sans covariance (noyau générant une matrice diagonale). Le noyau devient ainsi un a priori, et le processus gaussien conditionné aux données devient un a posteriori probabiliste. Finalement, les processus gaussiens peuvent être extrapolés à plusieurs variables descriptives. 12.9.2 Les processus gaussiens en R Pas de souci, vous n’aurez pas à programmer vos propres fonctions pour lancer des processus gaussiens. Vous pourrez passer par caret. Vous pourriez, comme c’est le cas avec les réseaux neuronnaux, obtenir davantage de contrôle sur l’autoapprentissage en utilisant directement la fonction gausspr du package kernlab. library(kernlab) ## ## Attaching package: &#39;kernlab&#39; ## The following object is masked from &#39;package:permute&#39;: ## ## how ## The following object is masked from &#39;package:purrr&#39;: ## ## cross ## The following object is masked from &#39;package:ggplot2&#39;: ## ## alpha x &lt;- c(7, 8, 10, 14, 17) y &lt;- c(61, 74, 69, 67, 78) y_sc &lt;- (y - mean(y)) / sd(y) m &lt;- gausspr(x, y_sc, kernel = &#39;rbfdot&#39;, kpar = list(sigma = 4), variance.model = TRUE, scaled = TRUE, var = 0.01, cross = 2) xtest &lt;- seq(6, 18, by = 0.1) y_sc_pred_mean &lt;- predict(m, xtest, type=&quot;response&quot;) y_pred_mean &lt;- y_sc_pred_mean * sd(y) + mean(y) y_sc_pred_sd &lt;- predict(m, xtest, type=&quot;sdeviation&quot;) y_pred_sd &lt;- y_sc_pred_sd * sd(y) plot(x, y, xlim = c(6, 18), ylim = c(45, 90)) lines(xtest, y_pred_mean) lines(xtest, y_pred_mean + y_pred_sd, col=&quot;red&quot;) lines(xtest, y_pred_mean - y_pred_sd, col=&quot;red&quot;) abline(v=12, lty=3, col=&#39;gray50&#39;);text(12, 67, &#39;?&#39;, cex=2) abline(v=16, lty=3, col=&#39;gray50&#39;);text(16, 72, &#39;?&#39;, cex=2) 12.9.3 Application pratique Les processus gaussiens sont utiles pour effectuer des prédictions sur des phénomène sur lesquels on désire éviter de se commettre sur la structure. Les séries temporelles ou les signaux spectraux en sont des exemples. Aussi, j’ai utilisé les processus gaussiens pour modéliser des courbes de réponse aux fertilisants. EXEMPLE… Prédiction spatiale: - https://www.sciencedirect.com/science/article/pii/S2211675316300033 - https://stackoverflow.com/questions/43618633/multi-output-spatial-statistics-with-gaussian-processes "],
["chapitre-geo.html", "13 Les données spatiales", " 13 Les données spatiales ️ Objectifs spécifiques: À la fin de ce chapitre, vous serez familiers avec les notions de base géomatique: systèmes géodésiques, projections et données géoréférencées saurez utiliser R comme outil d’analyse spatiale (donnée associées à des points, lignes, polygones) saurez cartographier des données géoréférencées avec ggplot et Leaflet serez en mesure d’effectuer un autoapprentissage spatial avec les techniques des k-proches voisins et les processus gaussiens serez aptes à aborder une analyse géostatistique serez aptes à aborder une modélisation de distribution des espèces "],
["chapitre-ode.html", "14 Modélisation déterministe 14.1 Équations différentielles 14.2 Les équations différentielles ordinaires en modélisation écologique 14.3 Les équations différentielles partielles en modélisation écologique", " 14 Modélisation déterministe ️ Objectifs spécifiques: À la fin de ce chapitre, vous saurez définir une équation différentielle ordinaire et une équation différentielle partielle saurez aptes à détecter un problème impliquant le besoin d’utiliser des équations différentielles serez en mesure d’effectuer une modélisation impliquant un système d’EDO en contexte écologique De plus, en extra (non évalué, objectif incertain), vous - serez en mesure d’effectuer une modélisation par différences finies impliquant une EDP simple en contexte écologique On se réfère à la modélisation mécanistique lorsque des principes théoriques guident une modélisation, à l’inverse de la modélisation phénoménologique, qui est guidée par les données. Il existe de nombreuses techniques de modélisation mécanistique, mais la plupart sont guidées par les équations différentielles. 14.1 Équations différentielles Les équations différentielles permettent la résolution de problèmes impliquant des gradients dans le temps et dans l’espace. On les utilise pour modéliser la dynamique des populations, la thermodynamique, l’écoulement de l’eau dans les sols, le transport des solutés, etc. On en distingue deux grandes catégories: les équations différentielles ordinaires et partielles. Équations différentielles ordinaires (EDO). Les équations différentielles ordinaires s’appliquent sur des fonctions s’appliquant à une seule variables, qui est souvent le temps. On pourra suivre, par exemple, l’évolution de la température en un point, en fonction du temps à partir d’une condition initiale. Parfois, plusieurs EDO sont utilisées conjointement pour créer un système d’EDO que l’on pourra nommé un système dynamique. Les solutions analytiques des EDO sont parfois relativement faciles à résoudre, mais les ordinateurs permettent des résolutions numériques en quelques lignes de code. Équations différentielles partielles (EDP). Dans ce cas, ce sont plusieurs variables qui sont différenciées dans la même fonction. Il peut s’agir des coordonnées dans l’espace \\([x, y, z]\\) (régime permanent), qui peuvent aussi être appliqués à différents pas de temps (régime transitoire). Le problème sera délimité non pas seulement par des conditions initiales, mais aussi par des conditions aux frontières du modèle. Puisque que les solutions analytiques des EDP peuvent rarement être développées, on utilisera pratiquement toujours des approches numériques que sont principalement les méthodes de résolution par différences finies ou par éléments finis. 14.2 Les équations différentielles ordinaires en modélisation écologique L’évolution des populations dans le temps peut être abordée à l’aide de systèmes d’équations différentielles. Une simple équation décrivant la croissance d’une population peut être couplée à des schémas d’exploitation de cette population, que ce soit une exploitation forestière, une terre fourragère ou un territoire de chasse. On pourra aussi faire interagir des populations dans des schémas de relations biologiques. Ces processus peuvent être implémentés avec des processus aléatoires pour générer des schémas probabilistes. De plus, les biostatistiques et l’autoapprentissage peuvent être mis à contribution afin de calibrer les modèles. 14.2.1 Évolution d’une seule population en fonction du temps La croissance d’une population (ou de sa densité) isolée en fonction du temps dépend des conditions qui lui offre son environnement. Dans le cas de la biomasse d’une culture à croissance constante, le taux de croissance est toujours le même. \\[ \\frac{d 🌿 }{dt} = c \\] \\[ \\int_0^t c dt = \\int_{🌿_0}^{🌿(t)} ~d🌿 \\] \\[ ct = 🌿(t) - 🌿_0\\] \\[ 🌿(t) = 🌿_0 + ct \\] par(mar=c(4, 4, 1, 1), ps=10) y0 &lt;- 2 c &lt;- 2 # exprimé en individu / pas de temps times &lt;- seq(0, 6, 0.1) y &lt;- y0 + c * times plot(times, y, &#39;l&#39;, xlab=&quot;An&quot;, ylab=&quot;Population&quot;, ylim=c(0, max(y))) text(max(times), max(y), round(max(y))) Dans le cas d’une population qui se reproduit, une formulation simple modélise une évolution linéaire associée à un taux de natalité \\(n\\) et un taux de mortalité \\(m\\), où \\(r = n-m\\) est le taux de croissance de la population d’une population de lapins 🐰 en fonction du temps \\(t\\). \\[ \\frac{d🐰}{dt} = n🐰 - m🐰 = r🐰 \\] \\[ \\int_0^t dt = \\int_{🐰_0}^{🐰(t)} \\frac{1}{r🐰} ~d🐰 \\] \\[ t = \\frac{1}{r} ln(🐰) \\bigg\\rvert_{🐰_0}^{🐰(t)} \\] \\[ rt = ln \\left( \\frac{🐰(t)}{🐰_0} \\right) \\] \\[ 🐰(t) = 🐰_0 exp(rt) \\] La vitesse de croissance est constante pour une population constante, mais la croissance de la population est exponentielle étant donnée que chaque nouvel individu se reproduit. par(mar=c(4, 4, 1, 1), ps=10) y0 &lt;- 10 r &lt;- 0.2 # exprimé en individu / pas de temps times &lt;- seq(0, 10, 0.1) y &lt;- y0 * exp(r*times) plot(times, y, &#39;l&#39;, xlab=&quot;An&quot;, ylab=&quot;Lapin&quot;, ylim=c(0, max(y))) text(max(times), max(y), round(max(y))) De 10 lapins au départ, nous en avons un peu plus de 75 après 10 ans… et près de 5 milliards après 100 ans! En fait, la capacité de support d’une population étant généralement limitée, on peut supposer que le taux de natalité décroit et que le taux de mortalité croit linéairement avec l’effectif. \\[ n(🐰) = \\alpha - \\beta 🐰 \\] \\[ m(🐰) = \\gamma + \\delta 🐰 \\] On aura donc \\[ \\frac{d🐰}{dt} = 🐰 \\left( \\alpha - \\beta 🐰 \\right) - 🐰 \\left( \\gamma + \\delta 🐰 \\right) = r🐰 \\left( 1 - \\frac{🐰}{K} \\right) \\] où \\(r = \\alpha - \\gamma\\) est l’ordonnée à l’origine du taux de croissance (théorique, lorsque la population est nulle) et \\(K = \\frac{\\alpha-\\gamma}{\\beta + \\delta}\\) est la capacité limite du milieu de subsistance. On pourra s’aider d’un logiciel de calcul symbolique comme sympy ou maxima pour en tirer une solution analytique. Mais à ce point, nous utiliserons une approximation numérique. Nous utiliserons le module deSolve. library(&quot;deSolve&quot;) deSolve demande de définir les paramètres de l’EDO ou du système d’EDO. Nous devons d’abord spécifier à quels pas de temps notre EDO doit être approximée. J’étends la plage de temps à 30 ans pour bien visualiser la courbe de croissance. times &lt;- seq(0, 30, by = 0.5) Les conditions initiales du système d’EDO sont aussi définies dans un vecteur. La seule condition initiale de notre EDO est le nombre initial de lapin. y0 &lt;- c(lapin = 10) On définira les paramètres dans un vecteur p. Dans notre cas, nous avons \\(r\\), le taux de croissance à l’origine et \\(K\\), la capacité de support de l’écosystème. Il est préférable de nommer les paramètres du vecteur pour éviter les erreurs. p &lt;- c(r = 0.2, K = 40) Enfin, une fonction définit l’EDO avec, comme entrées, les pas de temps, les conditions initiales et les paramètres. La sortie de la fonction est un vecteur des dérivées emboîtés dans une liste (lisez le fichier d’aide de la fonction ode pour les détails en lançant ?ode). model_logistic &lt;- function(t, y, p) { lapin &lt;- y[1] dlapin_dt &lt;- p[1] * lapin * (1 - lapin/p[2]) return(list(c(dlapin_dt))) } Une fois que les pas de temps, les conditions initiales, les paramètres et le modèle sont définis, on les spécifie comme arguments dans la fonction ode. La sortie de la fonction ode est une matrice dont la première colonne comprend les pas de temps imposés, et les autres colonnes sont les dérivées spécifiées à la sortie de la fonction ode. lapin_t &lt;- ode(y = y0, times = times, model_logistic, p) head(lapin_t) ## time lapin ## [1,] 0.0 10.00000 ## [2,] 0.5 10.76856 ## [3,] 1.0 11.57342 ## [4,] 1.5 12.41288 ## [5,] 2.0 13.28478 ## [6,] 2.5 14.18643 par(mar=c(4, 4, 1, 1), ps=10) plot(lapin_t[, 1], lapin_t[, 2], type=&#39;l&#39;, xlab=&quot;An&quot;, ylab=&quot;Lapin&quot;, ylim=c(0, max(lapin_t[, 2]))) Exercice. Que ce passerait-il si le taux de croissance était négatif? Profitez-en pour changer les paramètres r et K. Exercice. D’autres formulations existent pour exprimer des taux de croissance (Gompertz, Allee, etc.). En outre la formulation de Gompertz s’écrit comme suit. \\[ \\frac{d🐰}{dt} = r🐰 \\left( ln \\frac{K}{🐰} \\right) \\] Entrer cet EDO dans R avec deSolve. 14.2.2 Population exploitée L’exploitation d’une population peut être effectuée de différentes manières. D’abord, le prélèvement peut être effectué de manière constante, par exemple dans un élevage ou par la chasse ou la cueillette. Ajoutons un prélèvement constant dans une courbe de croissance logistique. \\[ \\frac{d🐰}{dt} = r🐰 \\left( 1 - \\frac{🐰}{K} \\right) - Q \\] où \\(Q\\) est le quota, ou le prélèvement constant. On pourra aussi effectuer un prélèvement proportionnel à la population. \\[ \\frac{d🐰}{dt} = r🐰 \\left( 1 - \\frac{🐰}{K} \\right) - E🐰 \\] où \\(E\\) est l’effort d’exploitation. Ou bien effectuer une série de prélèvement ponctuels, comme la récolte de plantes fourragères. \\[ \\frac{d🌿}{dt} = c - \\left[ 🌿 - \\gamma \\right] \\bigg\\rvert_{t=a, b, c, d, e, ...} \\] où \\(\\gamma\\) est le reste de la biomasse après la récolte et \\(t=a, b, c, d, e, ...\\) sont les pas de temps où le bloc entre les crochets est actif, c’est-à-dire la période de récolte. La solution analytique d’une culture à croissance constante est plutôt facile à déduire. Les fonctions de prélèvement peuvent être modulées à votre guise. Prenons pour l’exemple un prélèvement constant et une croissance logistique. p &lt;- c(r = 0.2, K = 40, Q = 1) model_logistic_expl &lt;- function(t, y, p) { lapin &lt;- y[1] dlapin_dt &lt;- p[1] * lapin * (1 - lapin/p[2]) - p[3] return(list(c(dlapin_dt))) } lapin_t &lt;- ode(y = y0, times = times, model_logistic_expl, p) par(mar=c(4, 4, 1, 1), ps=10) plot(lapin_t[, 1], lapin_t[, 2], type=&#39;l&#39;, xlab=&quot;An&quot;, ylab=&quot;Lapin&quot;, ylim=c(0, max(lapin_t[, 2]))) Exercice. Modéliser avec un prélèvement proportionnel. L’exploitation ponctuelle, comme la récolte ou l’administration d’une série de traitements, implique l’utilisation d’approches intermittentes. deSolve ignore les changements dans les variables d’état (y) tels que définis dans les dérivés. Pour ce faire, nous devons avoir recours à des évènements dans le jargon de deSolve. Ces évènements doivent être spécifiés dans un data.frame ou une liste. Il est difficile de trouver un exemple générique pour modéliser des évènements. Pour en savoir davantage, je vous invite donc à consulter la fiche d’aide ?events. Dans notre cas, nous allons modéliser une récolte de plantes fourragères. La récolte est déclenchée lorsque le rendement atteint 2 t/ha, et laisser 0.3 t/ha au sol pour assurer le renouvellement pour les coupes subséquentes. Définissons d’abord les entrées du modèles. times &lt;- seq(0, 120, 0.1) p &lt;- c(r = 0.1, K = 2.5) y0 &lt;- c(champ = 0.1) Nous devons définir une fonction root, dont la sortie est une valeur qui déclenchera un évènement lorsque la valeur sera nulle. Dans notre cas, la valeur correspond simplement au rendement moins 2, la quantité au champ y[1]. Notez que d’autres stratégies peuvent être utilisées pour déclencher une récolte, par exemple le pourcentage de floraison qui demanderait des simulations plus poussées. recolte_root &lt;- function(t, y, p) y[1]-2 Puis, lorsque la fonction root est déclenchée, l’évènement ramène la quantité au champs à 1 t/ha, une quantité qui permet de relancer la croissance. recolte_event &lt;- function(t, y, p) { y[1] &lt;- 0.3 return(y) } La fonction du modèle est telle qu’utilisée auparavant: une fonction logistique. recolte &lt;- function(t, y, p) { champ &lt;- y[1] dchamp_dt &lt;- p[1] * champ * (1 - champ/p[2]) return(list(c(dchamp_dt))) } La fonction ode est lancée en entrant les fonction root et events. out &lt;- ode(times = times, y = y0, func = recolte, parms = p, rootfun = recolte_root, events = list(func = recolte_event, root = TRUE), method=&quot;impAdams&quot;) plot(out) Nous pourrons organiser deux récoltes de 1.7 t/ha et une de 2 t/ha pour terminer la saison. Exercice. Qu’adviendrait-il si vous laissiez 0.15 t/ha au champ au lieu de 0.3? Ou si vous laissiez 1 t/ha? Ou si vous déclenchiez une récolte à 2.3 t/ha? Défi. Pouvez-vous modéliser l’ensilage? 14.2.3 Interactions biologiques Les interactions biologiques entre deux espèces à un stade de croissance défini peuvent prendre différentes formes, du mutualisme (les deux espèces bénéficient de la relation) à la compétition (les deux espèces se nuisent) en passant par la prédation ou le parasitisme (une espèce bénéficie de l’autre en lui nuisant) ou le neutralisme (aucun effet). Ces effets sont décrits dans Pringle (2016) en un tableau synthèse. Source: Pringle, E.G. 2016. Orienting the Interaction Compass: Resource Availability as a Major Driver of Context Dependence. Plos Biology. https://doi.org/10.1371/journal.pbio.2000891 Ces interactions peuvent être décrite mathématiquement dans des systèmes d’EDO, ou EDO couplées. Le cas d’étude le plus courant reprend le système d’équation prédateur-proie de Lotka-Volterra, deux auteurs ayant développé de manière indépendante des équations similaires respectivement en 1925 et 1926. Les équations de Lotka-Volterra supposent une croissance illimitée des deux espèces: les proies 🐰 se reproduisent par elles-mêmes (\\(\\alpha 🐰\\)), tandis que les prédateurs 🦊 croissent selon la disponibilité des proies (\\(\\delta 🐰🦊\\)). À l’inverse, la mortalité des proies dépend du nombre de prédateurs (\\(- \\beta 🐰🦊\\)), mais la mortalité des prédateurs est indépendante des proies (\\(- \\gamma 🦊\\)). On obtient ainsi un système d’équation. \\[\\frac{d🐰}{dt} = \\alpha 🐰 - \\beta 🐰🦊 = 🐰 \\left( \\alpha - \\beta 🦊 \\right)\\] \\[\\frac{d🦊}{dt} = \\delta 🐰🦊 - \\gamma 🦊 = 🦊 \\left( \\delta 🐰 - \\gamma \\right) \\] À l’équilibre de 🐰, c’est-à-dire où \\(\\frac{d🐰}{dt} = 0\\), on retrouve \\(🐰=0\\) ou \\(🦊 = \\frac{\\alpha}{\\beta}\\). De même, à l’équilibre de 🦊, on retrouve \\(🦊=0\\) ou \\(🐰 = \\frac{\\gamma}{\\delta}\\). En termes mathématiques, ces équilibre sont des isoclines, des points d’inflexion dans le système d’EDO. Nous allons résoudre les équations de Lotka-Volterra avec deSolve. Rappelons-nous que nous devons définir des pas de temps où approximer les populations (times), des conditions initiales (y0) et des paramètres (p). times &lt;- seq(0, 30, by = 0.1) y0 &lt;- c(lapin = 3, renard = 1) p &lt;- c(alpha = 2, # taux de croissance des lapins (naissance - mortalité, 1/an) beta = 0.8, # taux de prédation des lapins (renard / an) delta = 0.1, # taux de conversion lors de la prédation (lapin / renard) gamma = 0.2) # mortalité naturelle des renards (1/an) On peut calculer d’emblée les isoclines. lapin_iso &lt;- p[4]/p[3] renard_iso &lt;- p[1]/p[2] Nous devons ensuite créer notre modèle. modele_LV &lt;- function(t, y, p) { lapin = y[1] renard = y[2] dlapin_dt = p[1] * lapin - p[2] * lapin * renard drenard_dt = p[3] * lapin * renard - p[4] * renard return(list(c(dlapin_dt, drenard_dt))) } Lançons l’approximation. effectifs_t = ode(y = y0, times = times, modele_LV, p) head(effectifs_t) ## time lapin renard ## [1,] 0.0 3.000000 1.000000 ## [2,] 0.1 3.380961 1.011940 ## [3,] 0.2 3.806028 1.028156 ## [4,] 0.3 4.278154 1.049326 ## [5,] 0.4 4.799633 1.076263 ## [6,] 0.5 5.371673 1.109943 par(mar=c(4, 4, 1, 1), ps=10) plot(effectifs_t[, 1], effectifs_t[, 2], type = &#39;l&#39;, ylim = c(0, max(effectifs_t[, 2])), xlab = &#39;Temps&#39;, ylab = &quot;Nombre d&#39;individus&quot;) # lapins lines(effectifs_t[, 1], effectifs_t[, 3], col = &#39;red&#39;) legend(x=4, y=12, legend=c(&quot;Lapins&quot;, &quot;Renards&quot;), col=c(&quot;black&quot;, &quot;red&quot;), lty=c(1, 1), cex=1.2) Lorsque la population de lapins croit, celle des renards croit à retardement jusqu’à ce que la population de lapin diminue jusqu’à être presque éteinte. Dans ces conditions, la population de renard ne peut plus être soutenue, et décroit, ce qui en retour donne l’opportunité de la population de lapins de resurgir. par(mar=c(4, 4, 1, 1), ps=10) plot(effectifs_t[, 2], effectifs_t[, 3], type = &#39;l&#39;, xlab = &quot;Nombre lapins&quot;, ylab= &quot;Nombre de renards&quot;, xlim = c(0, max(effectifs_t[, 2])), ylim = c(0, max(effectifs_t[, 3]))) # isoclines abline(v=lapin_iso, lty=2, col=&quot;black&quot;) abline(h=renard_iso, lty=2, col=&quot;red&quot;) points(lapin_iso, renard_iso) # condition initiale points(y0[1], y0[2], pch = 16) Les conditions initiales sont responsables de l’amplitude des cycles. En faisant les faisant varier et en portant graphiquement les vecteurs de flux, on peut mieux apprécier l’importance des isoclines, qui séparent la direction que prend la relation entre deux espèces. effectifs_i &lt;- list() lapin_0 &lt;- 1:30 for (i in 1:length(lapin_0)) { y0[1] &lt;- lapin_0[i] effectifs_i[[i]] &lt;- ode(y = y0, times = times, modele_LV, p) offsets &lt;- effectifs_i[[i]][-1, -1] - effectifs_i[[i]][-nrow(effectifs_i[[i]]), -1] colnames(offsets) &lt;- c(&quot;d_lapin&quot;, &quot;d_renard&quot;) effectifs_i[[i]] &lt;- cbind(effectifs_i[[i]][-1, ], offsets) } effectifs_df &lt;- do.call(rbind.data.frame, effectifs_i) library(&quot;plotrix&quot;) plot(effectifs_df[, 2], effectifs_df[, 3], type = &#39;n&#39;, xlab = &quot;Nombre lapins&quot;, ylab= &quot;Nombre de renards&quot;, xlim = c(0, max(effectifs_df[, 2])), ylim = c(0, max(effectifs_df[, 3]))) # isoclines abline(v=lapin_iso, lty=2, col=&quot;black&quot;) abline(h=renard_iso, lty=2, col=&quot;red&quot;) points(lapin_iso, renard_iso) vectorField(u=effectifs_df[, 4], v=effectifs_df[, 5], xpos=effectifs_df[, 2], ypos=effectifs_df[, 3], scale=0.1, headspan=0.05, vecspec=&quot;lonlat&quot;) Nous avons modélisé une relation biologique de prédation. Il existe dans la littérature une panoplie de modèles d’EDO pour décrire les relations biologiques, qui peuvent être modélisés entre plusieurs espèces pour créer des réseaux trophiques complexes. Toutefois, la difficulté de collecter des données en quantité et en qualité suffisante rendent ces modèles difficiles à appréhender. Exercice. Qu’adviendrait-il des populations si l’on prenait plutôt un profil de croissance logistique chez les lapins? \\[\\frac{d🐰}{dt} = r🐰 \\left( 1-\\frac{x}{K} \\right) - \\beta 🐰🦊 \\] \\[\\frac{d🦊}{dt} = \\delta 🐰🦊 - \\gamma 🦊 \\] Exercice. Modéliser une compétition interspécifique où chaque population croit de manière logistique. \\[\\frac{d🐁}{dt} = r_1 🐁 \\left( 1-\\frac{🐁}{K_1} -\\alpha \\frac{🐀}{K_1} \\right) \\] \\[\\frac{d🐁}{dt} = r_2 🐀 \\left( 1-\\frac{🐀}{K_2} -\\beta \\frac{🐁}{K_2} \\right) \\] où \\(r_1\\) et \\(r_2\\) sont les taux de croissances respectifs des 🐁 et des 🐀, ainsi que \\(K_1\\) et que \\(K_2\\) sont les capacités de support des 🐁 et des 🐀. Le coefficient \\(\\alpha\\) décrit l’ampleur de la compétition de 🐀 sur 🐁 et le coefficient \\(\\beta\\) décrit l’ampleur de la compétition de 🐁 sur 🐀 (\\(\\alpha\\) et \\(\\beta\\) sont &gt;= 0). Exercice. Les interactions biologiques forment une bonne introduction aux systèmes d’équations différentielles ordinaires. On fait néanmoins souvent référence aux équations de Lorenz (1963), qui a développé un système d’EDO chaotique depuis trois équations, \\[ X&#39; = aX + YZ, \\] \\[ Y&#39; = b \\left(Y-Z\\right), \\] \\[ Z&#39; = -XY + cY - Z, \\] où \\(X\\) est la température horizontale, \\(Y\\) est la température verticale, \\(Z\\) est le flux de chaleur convectif, et où l’on retrouve les paramètres \\(a = -8/3\\), \\(b=-10\\) et \\(c=28\\). Résoudre les équations de Lorents avec deSolve. Porter graphiquement les relations entre X, Y et Z. 14.3 Les équations différentielles partielles en modélisation écologique Contrairement aux EDO, la solution des équations différentielle partielles (EDP) dépend de plus d’une variable indépendante. Typiquement, elles dépendent de coordonnées spatiales. Elles peuvent aussi dépendre du temps. Dans cette section, nous allons explorer les régimes permanents, c’est-à-dire indépendants du temps, en utilisant la méthode des différences finies. Nous allons aussi explorer les problèmes transitoires, qui eux dépendent du temps, en utilisant la méthode des lignes. À venir… "]
]
